{"posts":[{"title":"‘OVN实践’","content":"部署：https://www.jianshu.com/p/9619751f757a 源码：https://gobomb.github.io/post/learning-k8s-networking-reading-ovn-kubernetes-source/ ","link":"https://rexrock.github.io/post/ovn1/"},{"title":"DELL服务器升级82599网卡固件版本","content":"最近在一批比较老的服务器上部署了vhostuser类型的虚拟机，虚拟机创建后发现无法对外通信，经排查发现物理网卡未能成功执行offload操作，发出报文的inner_tcp_csum是错的，导致被对端或网关丢包。初步怀疑是网卡固件版本太低导致。 网卡固件跟网卡驱动不太一样，OEM厂商通常都会自己定制固件，而且一种网卡对应多个型号，每个型号的固件版本通常都不一样。 首先通过lspci -vvv发现网卡是DELL的板载卡： 查看服务器具体型号： 登录官网下载： https://www.dell.com/support/home/zh-cn/product-support/product/poweredge-r730/drivers 下载之后是个.BIN文件，直接执行即可。 报错提示无法获取设备内存信息，需要升级驱动，那么去Intel官网下载最新驱动： https://downloadcenter.intel.com/product/32609/Intel-82599-10-Gigabit-Ethernet-Controller 然后编译、打包、安装： rpmbuild -tb ixgbe-5.11.3.tar.gz rpm -ivh ixgbe-5.11.3-1.x86_64.rpm 重新执行.bin文件升级成功，重启——问题解决。 ","link":"https://rexrock.github.io/post/82599/"},{"title":"linux系统备份恢复","content":"参考：https://help.ubuntu.com/community/BackupYourSystem/TAR 1. 备份 没什么好讲的，就是打个tar包 cd / # THIS CD IS IMPORTANT THE FOLLOWING LONG COMMAND IS RUN FROM / tar -cvpzf backup.tar.gz \\ --exclude=/backup.tar.gz \\ --exclude=/proc \\ --exclude=/tmp \\ --exclude=/mnt \\ --exclude=/dev \\ --exclude=/sys \\ --exclude=/run \\ --exclude=/media \\ --exclude=/var/log \\ --exclude=/var/cache/apt/archives \\ --exclude=/usr/src/linux-headers* \\ --exclude=/home/*/.gvfs \\ --exclude=/home/*/.cache \\ 2. 恢复 一开始想的是完全恢复，即找一块独立的磁盘，分区+格式化，然后将tar包解压，重启后BIOS选择从该磁盘启动，先看一下步骤： 分区+格式化+mount： 具体步骤略过，可以完全模仿原系统的分区，也可以自定义分区（需要改grub.conf中vmlinux和initrd的加载路路径） 三步完成后，需要为新磁盘安装grub grub2-install /dev/sdx 解压： tar -xvpzf backup.tar.gz -C /media/ --numeric-owner 解压后需要根据分区调整文件位置，之后需要创建一些没有打包过来的临时目录 mkdir /proc /sys /mnt /media # 不全，自己看缺啥目录就创建啥目录 grub恢复 for f in dev dev/pts proc ; do mount --bind /$f /media/whatever/$f ; done chroot /media/whatever grub-mkconfig -o /boot/grub/grub.cfg # ubuntu/debian下的命令是 dpkg-reconfigure grub-pc 结果 从新磁盘启动后，发现从老的initrd启动，无法加载系统分区，即/dev/目录下看不到sda sdb这些设备； 然后重新启动，chroot到新磁盘，重新安装内核，然后再重新拉起，分区倒是都加载了，但是initrd启动还是有问题，过去太久具体记不起来了；注意chroot到新磁盘安装内核，需要mount一些目录，具体那些请根据错误提示操作： mount -t proc proc /media/proc/ 最终解决 最后的解决方法是，安装一个同版本系统，然后只将我们打包的内容作为根文件系统分区才解决。 ","link":"https://rexrock.github.io/post/linux1/"},{"title":"制作RPM包","content":"参考：https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html-single/rpm_packaging_guide/index 1. 安装必要的工具 yum install @'Development Tools' rpm-build yum-utils 2. 创建编译目录 rpmdev-setuptree 编译目录路径为${home}/rpmbuild/，目录结构如下： . ├── BUILD，编译目录 ├── RPMS，rpm包存放目录 ├── SOURCES，源码包存放目录 ├── SPECS，spec文件存放目录 └── SRPMS，src.rpm存放目录 3. 创建spec文件 本文为shell脚本创建rpm包，没有源码编译这一步，直接看spec文件： # 指定rpmbuild工作目录，可以不指定，默认就是${home}/rpmbuild/ BuildRoot: /root/rpmrebuild # 指定架构依赖，由于是脚本，所以是noarch，还可以是x86_64/arm等，如果不是noarch可以不指定，rpmbuild会自动检测并设置 BuildArch: noarch # rpm包的名称 Name: hello-test # rpm包的版本 Version: 1.0.0 # rpm包的发行号，即当前版本第n次的发行 Release: 1_11 License: GPLv3+ Group: Unspecified Summary: Hello test %description Just test for build rpm package %install # 下面执行的其实都是shell命令了，工作目录为${home}/rpmbuild/BUILD/， # 所以说我们需要将待安装的文件放在${home}/rpmbuild/BUILD/目录下,然后 # 执行下面的命令将文件安装到%{buildroot}目录下，%{buildroot}是自动创 # 建的目录，通常这个目录是${home}/rpmbuild/BUILDROOT/${Name}-${Version}-${Release}.${BuildArch} mkdir -p %{buildroot}/etc/hello-test/ install -m 0755 test.sh %{buildroot}/etc/hello-test/test.sh install -m 0644 test.conf %{buildroot}/etc/hello-test/test.conf %files # 安装文件列表，并定义每个文件的属性 %defattr(644,root,root,755) %attr(755,root,root) /etc/hello-test/test.sh %config(noreplace) %attr(0644, root, root) &quot;/etc/hello-test/test.conf&quot; 4. 执行rpmbuild开始打包 rpmbuild -ba hello-test.spec ","link":"https://rexrock.github.io/post/rpm1/"},{"title":"AF_XDP VS DPDK","content":"目前 ovs、dpdk、cilium均对 AF_XDP 做了支持，这是否预示在高性能报文转发方面 AF_XDP未来将成为DPDK外又一重要技术分支？加之AF_XDP跟内核更好的配合，随着技术不断程序，AF_XDP是否会全面超越甚至取代DPDK成为高性能报文转发的首选？未来不得而知，但至少从目前看，AF_XDP性能上仍不及DPDK，下面通过一个简单的测试来具体看一下。 说明：本次测试，AF_XDP时NATIVE模式，而不是NATIVE_WITH_ZEROCOPY模式，在CX5网卡上 ZEROCOPY开启失败，看来需要网卡的支持，后续再调试解决吧。不开启zerocopy，根据以往测试经验性能可能会有20%-30%的下降。 1. 测试拓扑 2. 软件版本 OVS-2.12 + DPDK-20.11 + KERNEL-5.4.87，具体要求及ovs编译配置参考： Open vSwitch with AF_XDP 3. OVS配置 OVS编译： ./configure --prefix=/usr/ --enable-afxdp --with-dpdk --with-debug CFLAGS=&quot;-O3&quot; make &amp;&amp; make install OVS初始化及网络配置： /usr/bin/ovs-vsctl --no-wait set Open_vSwitch . other_config:pmd-cpu-mask=0x550 init_dpdk() { ovs-vsctl set Open_vSwitch . other_config:dpdk-init=true /usr/bin/ovs-vsctl --no-wait set Open_vSwitch . other_config:dpdk-socket-mem=2048,0 /usr/bin/ovs-vsctl --no-wait set Open_vSwitch . other_config:dpdk-extra=&quot;-a 0000:5e:00.0,txq_inline=128,txqs_min_inline=4,txq_mpw_en=0 -a 0000:5e:00.0,txq_inline=128,txqs_min_inline=4,txq_mpw_en=0&quot; } add_xdp_port () { ovs-vsctl -- add-br br0 \\ -- set Bridge br0 datapath_type=netdev ovs-vsctl add-port br0 ens2f0 \\ -- set interface ens2f0 type=&quot;afxdp&quot; options:xdp-mode=native-with-zerocopy options:n_rxq=4 other_config:pmd-rxq-affinity=&quot;0:4,1:6,2:8,3:10&quot; ovs-vsctl add-port br0 ens2f1 \\ -- set interface ens2f1 type=&quot;afxdp&quot; options:xdp-mode=native-with-zerocopy options:n_rxq=4 other_config:pmd-rxq-affinity=&quot;0:4,1:6,2:8,3:10&quot; } add_dpdk_port() { ovs-vsctl --may-exist add-br br0 \\ -- set Bridge br0 datapath_type=netdev \\ -- br-set-external-id br0 bridge-id br0 \\ -- set bridge br0 fail-mode=secure ovs-vsctl --timeout 10 add-port br0 ens2f0 \\ -- set Interface ens2f0 type=dpdk options:dpdk-devargs=0000:5e:00.0 options:n_rxq=4 other_config:pmd-rxq-affinity=&quot;0:4,1:6,2:8,3:10&quot; ovs-vsctl --timeout 10 add-port br0 ens2f1 \\ -- set Interface ens2f1 type=dpdk options:dpdk-devargs=0000:5e:00.1 options:n_rxq=4 other_config:pmd-rxq-affinity=&quot;0:4,1:6,2:8,3:10&quot; } #init_dpdk #sleep 1 #add_dpdk_port add_xdp_port OVS流表配置： ovs-ofctl del-flows br0 ovs-ofctl add-flow br0 table=0,priority=0,action=normal ovs-ofctl add-flow br0 table=1,priority=0,action=normal ovs-ofctl add-flow br0 table=0,priority=20,ip,dl_src=b8:59:9f:41:0e:d6,in_port=ens2f0,action=load:0-\\&gt;NXM_OF_IN_PORT[],goto_table:1 ovs-ofctl add-flow br0 table=0,priority=20,ip,dl_src=b8:59:9f:41:0e:d7,in_port=ens2f1,action=load:0-\\&gt;NXM_OF_IN_PORT[],goto_table:1 ovs-ofctl add-flow br0 table=1,priority=20,ip,nw_dst=172.10.1.1,action=set_field:b8:59:9f:41:11:8e-\\&gt;eth_src,set_field:b8:59:9f:41:0e:d6-\\&gt;eth_dst,output:ens2f0 ovs-ofctl add-flow br0 table=1,priority=20,ip,nw_dst=172.10.2.1,action=set_field:b8:59:9f:41:11:8f-\\&gt;eth_src,set_field:b8:59:9f:41:0e:d7-\\&gt;eth_dst,output:ens2f1 4. NODE配置 ip netns add net1 ip netns add net2 sleep 1 ip link set ens2f0 netns net1 ip link set ens2f1 netns net2 sleep 1 ip netns exec net1 ifconfig ens2f0 172.10.1.1/24 up ip netns exec net2 ifconfig ens2f1 172.10.2.1/24 up ip netns exec net1 route add -net 172.10.2.0/24 gw 172.10.1.2 dev ens2f0 ip netns exec net2 route add -net 172.10.1.0/24 gw 172.10.2.2 dev ens2f1 5. 测试结果 5.1 Throughput 单口25G网卡，带宽AF_XDP和DPDK两种场景下带宽都能够打满： 我们主要看下，相同带宽下，各自的PMD使用率： 很明显，DPDK PMD使用率更低，并且hash的更均匀； 5.2 PPS AF_XDP：327W pps DPDK：1400W pps 并且各自峰值的情况下，PMD使用率DPDK仍是全面占优： 5.3 Latency netperf -t TCP_RR -H 172.10.2.1 -l 30 -- -r 1B,1B -O &quot;MAX_LATENCY,MEAN_LATENCY,P90_LATENCY,P99_LATENCY,P999_LATENCY,P9999_LATENCY,STDDEV_LATENCY,THROUGHPUT,THROUGHPUT_UNITS&quot; netperf -t TCP_CRR -H 172.10.2.1 -l 30 -- -r 1B,1B -O &quot;MAX_LATENCY,MEAN_LATENCY,P90_LATENCY,P99_LATENCY,P999_LATENCY,P9999_LATENCY,STDDEV_LATENCY,THROUGHPUT,THROUGHPUT_UNITS&quot; ","link":"https://rexrock.github.io/post/af_xdp2/"},{"title":"AF_XDP技术详解","content":"AF_XDP是一个协议族（例如AF_NET），主要用于高性能报文处理。 前文XDP技术简介中提到过，通过XDP_REDIRECT我们可以将报文重定向到其他设备发送出去或者重定向到其他的CPU继续进行处理。而AF_XDP则利用 bpf_redirect_map()函数，实现将报文重定向到用户态一块指定的内存中，接下来我们看一下这到底是如何做到的。 我们使用普通的 socket() 系统调用创建一个AF_XDP套接字（XSK）。每个XSK都有两个ring：RX RING 和 TX RING。套接字可以在 RX RING 上接收数据包，并且可以在 TX RING 环上发送数据包。这些环分别通过 setockopts() 的 XDP_RX_RING 和 XDP_TX_RING 进行注册和调整大小。每个 socket 必须至少有一个这样的环。RX或TX描述符环指向存储区域（称为UMEM）中的数据缓冲区。RX和TX可以共享同一UMEM，因此不必在RX和TX之间复制数据包。 UMEM也有两个 ring：FILL RING 和 COMPLETION RING。应用程序使用 FILL RING 向内核发送可以承载报文的 addr (该 addr 指向UMEM中某个chunk)，以供内核填充RX数据包数据。每当收到数据包，对这些 chunks 的引用就会出现在RX环中。另一方面，COMPLETION RING包含内核已完全传输的 chunks 地址，可以由用户空间再次用于 TX 或 RX。 关于ring，熟悉dpdk的同学应该都不陌生，这里只做简单介绍。ring就是一个固定长度的数组，并且同时拥有一个生产者和一个消费者，生产者向数组中逐个填写数据，消费者从数组中逐个读取生产者填充的数据，生产者和消费者都用数组的下标表示，不断累加，像一个环一样不断重复生产然后消费的动作，因此得名ring。 此外需要注意的事，AF_XDP socket不再通过 send()/recv()等函数实现报文收发，而实通过直接操作ring来实现报文收发。 FILL RING fill_ring 的生产者是用户态程序，消费者是内核态中的XDP程序； 用户态程序通过 fill_ring 将可以用来承载报文的 UMEM frames 传到内核，然后内核消耗 fill_ring 中的元素（后文统一称为 desc），并将报文拷贝到desc中指定地址（该地址即UMEM frame的地址）； COMPLETION RING completion_ring 的生产者是XDP程序，消费者是用户态程序； 当内核完成XDP报文的发送，会通过 completion_ring 来通知用户态程序，哪些报文已经成功发送，然后用户态程序消耗 completion_ring 中 desc(只是更新consumer计数相当于确认)； RX RING rx_ring的生产者是XDP程序，消费者是用户态程序； XDP程序消耗 fill_ring，获取可以承载报文的 desc并将报文拷贝到desc中指定的地址，然后将desc填充到 rx_ring 中，并通过socket IO机制通知用户态程序从 rx_ring 中接收报文； TX RING tx_ring的生产者是用户态程序，消费者是XDP程序； 用户态程序将要发送的报文拷贝 tx_ring 中 desc指定的地址中，然后 XDP程序 消耗 tx_ring 中的desc，将报文发送出去，并通过 completion_ring 将成功发送的报文的desc告诉用户态程序； 1. 用户态程序 1.1 创建AF_XDP的socket xsk_fd = socket(AF_XDP, SOCK_RAW, 0); 这一步没什么好展开的。 1.2 为UMEM申请内存 上文提到UMEM是一块包含固定大小chunk的内存，我们可以通过malloc/mmap/hugepages申请。下文大部分代码出自kernel samples。 bufs = mmap(NULL, NUM_FRAMES * opt_xsk_frame_size, PROT_READ | PROT_WRITE, MAP_PRIVATE | MAP_ANONYMOUS | opt_mmap_flags, -1, 0); if (bufs == MAP_FAILED) { printf(&quot;ERROR: mmap failed\\n&quot;); exit(EXIT_FAILURE); } 1.3 向AF_XDP socket注册UMEM struct xdp_umem_reg mr; memset(&amp;mr, 0, sizeof(mr)); mr.addr = (uintptr_t)umem_area; // umem_area即上面通过mmap申请到内存起始地址 mr.len = size; mr.chunk_size = umem-&gt;config.frame_size; mr.headroom = umem-&gt;config.frame_headroom; mr.flags = umem-&gt;config.flags; err = setsockopt(umem-&gt;fd, SOL_XDP, XDP_UMEM_REG, &amp;mr, sizeof(mr)); if (err) { err = -errno; goto out_socket; } 其中xdp_umem_reg结构定义在 usr/include/linux/if_xdp.h中： struct xdp_umem_reg { __u64 addr; /* Start of packet data area */ __u64 len; /* Length of packet data area */ __u32 chunk_size; __u32 headroom; __u32 flags; }; 成员解析： addr就是UMEM内存的起始地址； len是整个UMEM内存的总长度； chunk_size就是每个chunk的大小； headroom，如果设置了，那么报文数据将不是从每个chunk的起始地址开始存储，而是要预留出headroom大小的内存，再开始存储报文数据，headroom在隧道网络中非常常见，方便封装外层头部； flags, UMEM还有一些更复杂的用法，通过flag设置，后面再进一步展开； 1.4 创建FILL RING 和 COMPLETION RING 我们通过 setsockopt() 设置 FILL/COMPLETION/RX/TX ring的大小（在我看来这个过程相当于创建，不设置大小的ring是没有办法使用的）。 FILL RING 和 COMPLETION RING是UMEM必须，RX和TX则是 AF_XDP socket二选一的，例如AF_XDP socket只收包那么只需要设置RX RING的大小即可。 err = setsockopt(umem-&gt;fd, SOL_XDP, XDP_UMEM_FILL_RING, &amp;umem-&gt;config.fill_size, sizeof(umem-&gt;config.fill_size)); if (err) { err = -errno; goto out_socket; } err = setsockopt(umem-&gt;fd, SOL_XDP, XDP_UMEM_COMPLETION_RING, &amp;umem-&gt;config.comp_size, sizeof(umem-&gt;config.comp_size)); if (err) { err = -errno; goto out_socket; } 上述操作相当于创建了 FILL RING 和 和 COMPLETION RING，创建ring的过程主要是初始化 producer 和 consumer 的下标，以及创建ring数组。 问题来了： 上文提到，用户态程序是 FILL RING 的生产者和 CONPLETION RING 的消费者，上面2个 ring 的创建是在内核中创建了 ring 并初始化了其相关成员。那么用户态程序如何操作这两个位于内核中的 ring 呢？所以接下来我们需要将整个 ring 映射到用户态空间。 1.5 将FILL RING 映射到用户态 第一步是获取内核中ring结构各成员的偏移，因为从5.4版本开始后，ring结构中除了 producer、consumer、desc外，又新增了一个flag成员。所以用户态程序需要先获取 ring 结构中各成员的准确便宜，才能在mmap() 之后准确识别内存中各成员位置。 err = xsk_get_mmap_offsets(umem-&gt;fd, &amp;off); if (err) { err = -errno; goto out_socket; } xsk_get_mmap_offsets() 函数主要是通过getsockopt函数实现这一功能： err = getsockopt(fd, SOL_XDP, XDP_MMAP_OFFSETS, off, &amp;optlen); if (err) return err; 一切就绪，开始将内核中的 FILL RING 映射到用户态程序中： map = mmap(NULL, off.fr.desc + umem-&gt;config.fill_size * sizeof(__u64), PROT_READ | PROT_WRITE, MAP_SHARED | MAP_POPULATE, umem-&gt;fd, XDP_UMEM_PGOFF_FILL_RING); if (map == MAP_FAILED) { err = -errno; goto out_socket; } umem-&gt;fill = fill; fill-&gt;mask = umem-&gt;config.fill_size - 1; fill-&gt;size = umem-&gt;config.fill_size; fill-&gt;producer = map + off.fr.producer; fill-&gt;consumer = map + off.fr.consumer; fill-&gt;flags = map + off.fr.flags; fill-&gt;ring = map + off.fr.desc; fill-&gt;cached_cons = umem-&gt;config.fill_size; 上面代码需要关注的一点是 mmap() 函数中指定内存的长度——off.fr.desc + umem-&gt;config.fill_size * sizeof(__u64)，umem-&gt;config.fill_size * sizeof(__u64)没什么好说的，就是ring数组的长度，而 off.fr.desc 则是ring结构体的长度，我们先看下内核中ring结构的定义： struct xdp_ring_offset { __u64 producer; __u64 consumer; __u64 desc; }; 这是没有flag的定义，无伤大雅。这里desc的地址其实就是ring数组的起始地址了。而off.fr.desc是desc相对 ring 结构体起始地址的偏移，相当于结构体长度。我们用一张图来看下ring所在内存的结构分布： 后面一堆赋值代码没什么好讲的，umem-&gt;fill 是用户态程序自定义的一个结构体，其成员 producer、consumer、flags、ring都是指针，分别指向实际ring结构中的对应成员，umem-&gt;fill中的其他成员主要在后面报文收发时用到，起辅助作用。 1.6 将COMPLETION RING 映射到用户态 跟上面 FILL RING 的映射一样，只贴代码好了： map = mmap(NULL, off.cr.desc + umem-&gt;config.comp_size * sizeof(__u64), PROT_READ | PROT_WRITE, MAP_SHARED | MAP_POPULATE, umem-&gt;fd, XDP_UMEM_PGOFF_COMPLETION_RING); if (map == MAP_FAILED) { err = -errno; goto out_mmap; } umem-&gt;comp = comp; comp-&gt;mask = umem-&gt;config.comp_size - 1; comp-&gt;size = umem-&gt;config.comp_size; comp-&gt;producer = map + off.cr.producer; comp-&gt;consumer = map + off.cr.consumer; comp-&gt;flags = map + off.cr.flags; comp-&gt;ring = map + off.cr.desc; 1.7 创建RX RING和TX RING然后mmap 这里和 FILL RING 以及 COMPLETION RING的做法基本完全一致，只贴代码： if (rx) { err = setsockopt(xsk-&gt;fd, SOL_XDP, XDP_RX_RING, &amp;xsk-&gt;config.rx_size, sizeof(xsk-&gt;config.rx_size)); if (err) { err = -errno; goto out_socket; } } if (tx) { err = setsockopt(xsk-&gt;fd, SOL_XDP, XDP_TX_RING, &amp;xsk-&gt;config.tx_size, sizeof(xsk-&gt;config.tx_size)); if (err) { err = -errno; goto out_socket; } } err = xsk_get_mmap_offsets(xsk-&gt;fd, &amp;off); if (err) { err = -errno; goto out_socket; } if (rx) { rx_map = mmap(NULL, off.rx.desc + xsk-&gt;config.rx_size * sizeof(struct xdp_desc), PROT_READ | PROT_WRITE, MAP_SHARED | MAP_POPULATE, xsk-&gt;fd, XDP_PGOFF_RX_RING); if (rx_map == MAP_FAILED) { err = -errno; goto out_socket; } rx-&gt;mask = xsk-&gt;config.rx_size - 1; rx-&gt;size = xsk-&gt;config.rx_size; rx-&gt;producer = rx_map + off.rx.producer; rx-&gt;consumer = rx_map + off.rx.consumer; rx-&gt;flags = rx_map + off.rx.flags; rx-&gt;ring = rx_map + off.rx.desc; } xsk-&gt;rx = rx; if (tx) { tx_map = mmap(NULL, off.tx.desc + xsk-&gt;config.tx_size * sizeof(struct xdp_desc), PROT_READ | PROT_WRITE, MAP_SHARED | MAP_POPULATE, xsk-&gt;fd, XDP_PGOFF_TX_RING); if (tx_map == MAP_FAILED) { err = -errno; goto out_mmap_rx; } tx-&gt;mask = xsk-&gt;config.tx_size - 1; tx-&gt;size = xsk-&gt;config.tx_size; tx-&gt;producer = tx_map + off.tx.producer; tx-&gt;consumer = tx_map + off.tx.consumer; tx-&gt;flags = tx_map + off.tx.flags; tx-&gt;ring = tx_map + off.tx.desc; tx-&gt;cached_cons = xsk-&gt;config.tx_size; } xsk-&gt;tx = tx; 1.8 调用bind()将AF_XDP socket绑定的指定设备的某一队列 sxdp.sxdp_family = PF_XDP; sxdp.sxdp_ifindex = xsk-&gt;ifindex; sxdp.sxdp_queue_id = xsk-&gt;queue_id; sxdp.sxdp_flags = xsk-&gt;config.bind_flags; err = bind(xsk-&gt;fd, (struct sockaddr *)&amp;sxdp, sizeof(sxdp)); if (err) { err = -errno; goto out_mmap_tx; } 2. 内核态程序 相比用户态程序的一堆操作，内核态XDP程序看起来要简单的多。 在XDP技术简介我们曾介绍过，XDP程序利用 bpf_reditrct() 函数可以将报文重定向到其他设备发送出去或者重定向到其他CPU继续处理，后来又发展出了bpf_redirect_map()函数，可以将重定向的目的地保存在map中。AF_XDP 正是利用了 bpf_redirect_map() 函数以及 BPF_MAP_TYPE_XSKMAP 类型的 map 实现将报文重定向到用户态程序。 2.1 创建BPF_MAP_TYPE_XSKMAP类型的map 该类型map的key是网口设备的queue_id，value则是该queue上绑定的AF_XDP socket fd，所以通常需要为每个网口设备各自创建独立的map，并在用户态将对应的queue_id-&gt;xsk_fd存储到map中。 static int xsk_create_bpf_maps(struct xsk_socket *xsk) { int max_queues; int fd; max_queues = xsk_get_max_queues(xsk); if (max_queues &lt; 0) return max_queues; fd = bpf_create_map_name(BPF_MAP_TYPE_XSKMAP, &quot;xsks_map&quot;, sizeof(int), sizeof(int), max_queues, 0); if (fd &lt; 0) return fd; xsk-&gt;xsks_map_fd = fd; return 0; } bpf_create_map_name参数详解： BPF_MAP_TYPE_XSKMAP，map类型 &quot;xsks_map&quot;，map的名字 sizeof(int)，分别指定key和vlue的size max_queues，map大小 0, map_flags 2.2 XDP程序代码 /* This is the C-program: * SEC(&quot;xdp_sock&quot;) int xdp_sock_prog(struct xdp_md *ctx) * { * int index = ctx-&gt;rx_queue_index; * * // A set entry here means that the correspnding queue_id * // has an active AF_XDP socket bound to it. * if (bpf_map_lookup_elem(&amp;xsks_map, &amp;index)) * return bpf_redirect_map(&amp;xsks_map, index, 0); * * return XDP_PASS; * } */ 是不是非常的简单，真正的redirect操作只有一行代码。 2.3 XDP程序的加载 static int xsk_load_xdp_prog(struct xsk_socket *xsk) { static const int log_buf_size = 16 * 1024; char log_buf[log_buf_size]; int err, prog_fd; /* This is the C-program: * SEC(&quot;xdp_sock&quot;) int xdp_sock_prog(struct xdp_md *ctx) * { * int index = ctx-&gt;rx_queue_index; * * // A set entry here means that the correspnding queue_id * // has an active AF_XDP socket bound to it. * if (bpf_map_lookup_elem(&amp;xsks_map, &amp;index)) * return bpf_redirect_map(&amp;xsks_map, index, 0); * * return XDP_PASS; * } */ struct bpf_insn prog[] = { /* r1 = *(u32 *)(r1 + 16) */ BPF_LDX_MEM(BPF_W, BPF_REG_1, BPF_REG_1, 16), /* *(u32 *)(r10 - 4) = r1 */ BPF_STX_MEM(BPF_W, BPF_REG_10, BPF_REG_1, -4), BPF_MOV64_REG(BPF_REG_2, BPF_REG_10), BPF_ALU64_IMM(BPF_ADD, BPF_REG_2, -4), BPF_LD_MAP_FD(BPF_REG_1, xsk-&gt;xsks_map_fd), BPF_EMIT_CALL(BPF_FUNC_map_lookup_elem), BPF_MOV64_REG(BPF_REG_1, BPF_REG_0), BPF_MOV32_IMM(BPF_REG_0, 2), /* if r1 == 0 goto +5 */ BPF_JMP_IMM(BPF_JEQ, BPF_REG_1, 0, 5), /* r2 = *(u32 *)(r10 - 4) */ BPF_LD_MAP_FD(BPF_REG_1, xsk-&gt;xsks_map_fd), BPF_LDX_MEM(BPF_W, BPF_REG_2, BPF_REG_10, -4), BPF_MOV32_IMM(BPF_REG_3, 0), BPF_EMIT_CALL(BPF_FUNC_redirect_map), /* The jumps are to this instruction */ BPF_EXIT_INSN(), }; size_t insns_cnt = sizeof(prog) / sizeof(struct bpf_insn); prog_fd = bpf_load_program(BPF_PROG_TYPE_XDP, prog, insns_cnt, &quot;LGPL-2.1 or BSD-2-Clause&quot;, 0, log_buf, log_buf_size); if (prog_fd &lt; 0) { pr_warning(&quot;BPF log buffer:\\n%s&quot;, log_buf); return prog_fd; } err = bpf_set_link_xdp_fd(xsk-&gt;ifindex, prog_fd, xsk-&gt;config.xdp_flags); if (err) { close(prog_fd); return err; } xsk-&gt;prog_fd = prog_fd; return 0; } XDP程序的load 调用函数 bpf_load_program() 之前的代码不用关心。通常 eBPF 程序使用 C 语言的一个子集（restricted C）编写，然后通过 LLVM 编译成字节码注入到内核执行。由于本例中XDP程序代码比较简单，功力深厚的作者直接将其编写为 eBPF（JIT）可识别的字节码，然后直接调用 bpf_load_program() 函数将字节码程序加载到内核中。 XDP程序的attach XDP程序加载成功会返回对应的fd（后面统称为prog_fd），但是此时XDP程序还不会被执行（所有的eBPF都需要经过load和attach两步才能被触发执行，load只是将程序加载到内核中，attach将程序添加到hook点后，程序才能真正被触发执行）。我们调用函数 bpf_set_link_xdp_fd() 函数将XDP程序attach到指定网口设备的驱动中的hook点。 注意： AF_XDP socket是跟指定网口设备的队列绑定，而XDP程序则是跟指定的网口设备绑定（attach）。 3. 回到用户态，让程序run起来 经过前面两步，AF_XDP socket、UMEM、FILL/COMPLETION/RX/TX RING 都创建设置好了，XSKMAP 和XDP PROG 也都加载好了。但是要想让XDP程序把报文传到用户态程序，我们还得再进行两补操作。 3.1 将AF_XDP socket存储到XSKMAP中 前面介绍XSKMAP的时候，大家应该都想到这一步了，所以只贴代码不说话： static int xsk_set_bpf_maps(struct xsk_socket *xsk) { return bpf_map_update_elem(xsk-&gt;xsks_map_fd, &amp;xsk-&gt;queue_id, &amp;xsk-&gt;fd, 0); } 3.2 标题先卖个关子 前面我们介绍过4种ring，分别对应收发包两个场景（收包：FILL/RX ring，发包：TX/COMPLETION RING）,我画个图分别描述一下收发包场景。 3.2.1 先看收包 收包过程是由XDP程序触发的，但是XDP程序收包，需要依赖用户态程序填充FILL RING，将可以承载报文的desc告诉XDP程序。所以在用户态程序初始化阶段，我们需要先填充FILL RING，直接看代码： ret = xsk_ring_prod__reserve(&amp;xsk-&gt;umem-&gt;fq, XSK_RING_PROD__DEFAULT_NUM_DESCS, &amp;idx); if (ret != XSK_RING_PROD__DEFAULT_NUM_DESCS) exit_with_error(-ret); for (i = 0; i &lt; XSK_RING_PROD__DEFAULT_NUM_DESCS; i++) *xsk_ring_prod__fill_addr(&amp;xsk-&gt;umem-&gt;fq, idx++) = i * opt_xsk_frame_size; xsk_ring_prod__submit(&amp;xsk-&gt;umem-&gt;fq, XSK_RING_PROD__DEFAULT_NUM_DESCS); 三个经过封装的函数，看起来不明觉厉，咱们一个一个看： 1. xsk_ring_prod__reserve static inline size_t xsk_ring_prod__reserve(struct xsk_ring_prod *prod, size_t nb, __u32 *idx) { if (xsk_prod_nb_free(prod, nb) &lt; nb) return 0; *idx = prod-&gt;cached_prod; prod-&gt;cached_prod += nb; return nb; } 这个函数前面先判断一下：我现在想生产nb个数据，ring里有没有足够的地方放啊？没有的话直接退出，等会再试试。 vhostuser里再这块有个BUG，前端程序想发包发现ring里空间不够了，而后端驱动处理又由于有有问题的判断，导致报文已发的报文一直不被处理，结果造成死锁，以后别的文章中再介绍吧。 如果有足够的空间，那么会将生产者当前下标（cached_prog）赋值给idx，因为退出函数后会根据从这个idx指向的位置开始生产desc，最后cached_prod + nb。 为什么要有个cached_prog呢？ 因为生产数据这个过程需要分几步完成，所以这个东西应该为了多线程同步吧。 2. xsk_ring_prod__fill_addr static inline __u64 *xsk_ring_prod__fill_addr(struct xsk_ring_prod *fill, __u32 idx) { __u64 *addrs = (__u64 *)fill-&gt;ring; return &amp;addrs[idx &amp; fill-&gt;mask]; } 看这段代码前，我们先看下ring中元素xdp_desc的成员结构： struct xdp_desc { __u64 addr; __u32 len; __u32 options; }; 成员解析 addr指向UMEM中某个帧的具体位置，并且不是真正的虚拟内存地址，而是相对UMEM内存起始地址的偏移。 len则是指报文的具体的长度，当XDP程序向desc填充报文的时候需要设置len，但是用户态程序向FILL RING中填充desc则不用关心len。 所以上面xsk_ring_prod__fill_addr的功能就好理解了，返回的ring中下标为idx处的desc中addr的指针；并且在函数返回后对addr进行了赋值，再看下这块代码，可以看到赋值给addr是个偏移量： for (i = 0; i &lt; XSK_RING_PROD__DEFAULT_NUM_DESCS; i++) *xsk_ring_prod__fill_addr(&amp;xsk-&gt;umem-&gt;fq, idx++) = i * opt_xsk_frame_size; xsk_ring_prod__submit static inline void xsk_ring_prod__submit(struct xsk_ring_prod *prod, size_t nb) { /* Make sure everything has been written to the ring before indicating * this to the kernel by writing the producer pointer. */ libbpf_smp_wmb(); *prod-&gt;producer += nb; } 数据填充完毕，更新生产者下标。 说明：下标永远指向下一个可填充数据位置。 3.2.2 再看发包 发包真的没啥好说的。初始化的时候不用管，想发包的时候直接就发啦。 4. 收包流程解析 AF_XDP socket毕竟也是socket，所以select/poll/epoll这些函数都能用的，怎么用这里不介绍了。 我们只看具体从一个AF_XDP socket收包的过程: static void rx_drop(struct xsk_socket_info *xsk, struct pollfd *fds) { unsigned int rcvd, i; u32 idx_rx = 0, idx_fq = 0; int ret; rcvd = xsk_ring_cons__peek(&amp;xsk-&gt;rx, BATCH_SIZE, &amp;idx_rx); if (!rcvd) { if (xsk_ring_prod__needs_wakeup(&amp;xsk-&gt;umem-&gt;fq)) ret = poll(fds, num_socks, opt_timeout); return; } ret = xsk_ring_prod__reserve(&amp;xsk-&gt;umem-&gt;fq, rcvd, &amp;idx_fq); while (ret != rcvd) { if (ret &lt; 0) exit_with_error(-ret); if (xsk_ring_prod__needs_wakeup(&amp;xsk-&gt;umem-&gt;fq)) ret = poll(fds, num_socks, opt_timeout); ret = xsk_ring_prod__reserve(&amp;xsk-&gt;umem-&gt;fq, rcvd, &amp;idx_fq); } for (i = 0; i &lt; rcvd; i++) { u64 addr = xsk_ring_cons__rx_desc(&amp;xsk-&gt;rx, idx_rx)-&gt;addr; u32 len = xsk_ring_cons__rx_desc(&amp;xsk-&gt;rx, idx_rx++)-&gt;len; u64 orig = xsk_umem__extract_addr(addr); addr = xsk_umem__add_offset_to_addr(addr); char *pkt = xsk_umem__get_data(xsk-&gt;umem-&gt;buffer, addr); hex_dump(pkt, len, addr); *xsk_ring_prod__fill_addr(&amp;xsk-&gt;umem-&gt;fq, idx_fq++) = orig; } xsk_ring_prod__submit(&amp;xsk-&gt;umem-&gt;fq, rcvd); xsk_ring_cons__release(&amp;xsk-&gt;rx, rcvd); xsk-&gt;rx_npkts += rcvd; } 该函数并没有对报文做什么复杂处理，只是hex_dump了一下，整个收发包分五个步骤： 1. xsk_ring_cons__peek() 开始对RX RING进行消费，返回消费者下标和消费个数，并累加cached_cons； 2. xsk_ring_prod__reserve 开始对FILL RING进行生产，返回生产者下标和生产个数，并累加cached_prod; 3. 报文处理 处理从RX RING中收到的报文，并回填到FILL RING中； for (i = 0; i &lt; rcvd; i++) { u64 addr = xsk_ring_cons__rx_desc(&amp;xsk-&gt;rx, idx_rx)-&gt;addr; u32 len = xsk_ring_cons__rx_desc(&amp;xsk-&gt;rx, idx_rx++)-&gt;len; u64 orig = xsk_umem__extract_addr(addr); addr = xsk_umem__add_offset_to_addr(addr); char *pkt = xsk_umem__get_data(xsk-&gt;umem-&gt;buffer, addr); hex_dump(pkt, len, addr); *xsk_ring_prod__fill_addr(&amp;xsk-&gt;umem-&gt;fq, idx_fq++) = orig; } 从desc中读取addr，并通过 xsk_umem__get_data() 函数得到报文真正的虚拟地址，然后 hex_dump()下。 static inline void *xsk_umem__get_data(void *umem_area, __u64 addr) { return &amp;((char *)umem_area)[addr]; } 然后将处理完报文所在的 UMEM 帧回填到FILL RING中： *xsk_ring_prod__fill_addr(&amp;xsk-&gt;umem-&gt;fq, idx_fq++) = orig; 4. xsk_ring_prod__submit(&amp;xsk-&gt;umem-&gt;fq, rcvd) 完成对RX RING的消费，更新消费者下标； 5. xsk_ring_cons__release(&amp;xsk-&gt;rx, rcvd) 完成对FILL RING的生产，更新生产者下标； 5. 结语 关于AF_XDP的使用及背后原理暂且分析到这，目前AF_XDP已经在ovs、dpdk、cilium中应用，相应的文档下面有链接。如有错误纰漏，欢迎大家拍砖。 相关代码均出自kernel： samples/bpf/xdpsock_user.c tools/lib/bpf/xsk.c tools/lib/bpf/xsk.h net/xdp/xsk.c net/xdp/xsk.h usr/include/linux/if_xdp.h 相关参考文档如下： Kernel document for AF_XDP Man for bpf Openvswitch and XDP DPDK and XDP 性能对比 编译内核源码中的示例代码 ","link":"https://rexrock.github.io/post/af_xdp1/"},{"title":"使用istio + servicemesh搭建服务网格","content":"官方文档： https://istio.io/latest/zh/docs/setup/getting-started/ 1. 下载istio安装包 https://github.com/istio/istio/releases/tag/1.7.0 2. 通过istioctl安装 Profile说明 istioctl install 3. Sidecar注入 官方文档 https://istio.io/latest/docs/setup/additional-setup/sidecar-injection/ 3.1 手动注入 istioctl kube-inject -f samples/sleep/sleep.yaml | kubectl apply -f - 命令istioctl kube-inject会将创建sidecar需要的配置插入到sleep.yaml得配置中 3.2 自动注入 kubectl label namespaces &lt;nsName&gt; istio-injection=enabled 这样在namespace nsName 中创建得pod会被自动注入sidecar 3.3 自动注入不生效怎么办 原因一：准入控制器相关问题 参考： https://istio.io/latest/zh/docs/ops/configuration/mesh/webhook/ Istio 使用 ValidatingAdmissionWebhooks 验证 Istio 配置，使用 MutatingAdmissionWebhooks 自动将 Sidecar 代理注入至用户 Pod。 # a) 首先判断当前使用的apiserver默认会开启哪些准入控制器（admission controllers） kube-apiserver -h | grep enable-admission-plugins # b) 如果没有ValidatingAdmissionWebhooks和MutatingAdmissionWebhooks，则需修改apiserver启动参数开启 # 编辑/etc/kubernetes/manifests/kube-apiserver.yaml修改如下，然后重建 # --enable-admission-plugins=NodeRestriction,ValidatingAdmissionWebhooks,MutatingAdmissionWebhooks # c) 如果已开启，那么需要确认名为istio-sidecar-injector的webhook是否存在 kubectl get mutatingwebhookconfigurations.admissionregistration.k8s.io # d) 如果不存在，说明istio部署有问题，如果存在还不生效，需要看一下istio-sidecar-injector的配置 kubectl get mutatingwebhookconfigurations.admissionregistration.k8s.io istio-sidecar-injector -o yaml # e) 主要查看matchLabels字段，可以自动注入的labek并不是istio-injection=enabled 4. 开启both sidecar 为接收端配置containerPort即可开启both sidecar，可以对比下开启both sidecar前后iptables的差别： --- iptables.1 2020-09-09 17:56:26.452129512 +0800 +++ iptables.2 2020-09-09 17:56:18.308026728 +0800 @@ -1,5 +1,6 @@ Chain PREROUTING (policy ACCEPT) target prot opt source destination # 首先在PREROUTING链这里将所有的流量导向ISTIO_INBOUND +ISTIO_INBOUND tcp -- 0.0.0.0/0 0.0.0.0/0 Chain INPUT (policy ACCEPT) target prot opt source destination @@ -11,7 +12,11 @@ Chain POSTROUTING (policy ACCEPT) target prot opt source destination -Chain ISTIO_IN_REDIRECT (1 references) +Chain ISTIO_INBOUND (1 references) +target prot opt source destination # 然后在ISTIO_INBOUND中将访问containerPort端口的流量导向ISTIO_IN_REDIRECT # ISTIO_IN_REDIRECT和ISTIO_REDIRECT规则一样，都是将流量重定向到本地15001端口 +ISTIO_IN_REDIRECT tcp -- 0.0.0.0/0 0.0.0.0/0 tcp dpt:8088 + +Chain ISTIO_IN_REDIRECT (2 references) target prot opt source destination REDIRECT tcp -- 0.0.0.0/0 0.0.0.0/0 redir ports 15001 ","link":"https://rexrock.github.io/post/k8s3/"},{"title":"使用kubebuilder创建CRD及Controller","content":"1. 安装kubebuilder 建议源码安装更容易些： git clone https://github.com/kubernetes-sigs/kubebuilder.git cd kubebuilder/ make install ./bin/kubebuilder version 将./bin/kubebuilde拷贝到可执行路径 2. 使用kubebuilder创建CRD及Controller 官方参考文档： https://book-v1.book.kubebuilder.io/quick_start.html 其他参考文档： https://www.cnblogs.com/alisystemsoftware/p/11580202.html https://blog.csdn.net/qianggezhishen/article/details/106995181 2.1 准备工作 mkdir -p testProject/src/testController cd testProject/ export PATH=$PATH:/root/go/bin/ export GOPATH=$PWD # 强制启用go module export GO111MODULE=on # 配置默认从goproxy.io拉去go mod的依赖包，goproxy.io是国内七牛云维护的一个golang包代理库 export GOPROXY=https://goproxy.io cd src/testController 2.2 创建PROJECT kubebuilder init --domain test1.test2 --license apache2 --owner &quot;The Kubernetes Authors&quot; cat PROJECT 2.3 创建API kubebuilder create api --group apps --version v1 --kind Test cat config/samples/apps_v1_test.yaml 2.4 Install the CRDs into the cluster make install 2.5 在本地运行Controller make run 2.6 创建Resource of CRD kubectl create -f config/samples/apps_v1_test.yaml 可以看到controller的日志打印如下 2.7 编译打包 首先修改Dockerfile： 1）在RUN go mod download前插入一行ENV GOPROXY=https://goproxy.cn,direct 2）将FROM gcr.io/distroless/static:nonroot改为FROM golang:1.13 3）删除USER nonroot:nonroot 然后运行： docker build -t test-controller . 成功会看到 可以通过docker images命令查看镜像，或者通过docker save和docker load导出和导入。 3. 创建Core Resource的controller 参考： https://book-v1.book.kubebuilder.io/beyond_basics/controllers_for_core_resources.html 创建后需要修改controllers/deployment_controller.go文件： import添加：corev1 &quot;[k8s.io/api/core/v1](http://k8s.io/api/core/v1)&quot; import删除：appsv1 &quot;testController/api/v1&quot; 修改函数SetupWithManager： 将For(&amp;appsv1.Test{})改为For(&amp;corev1.Pod{}) 然后重新make run，则会收到pod创建、删除的event消息 ","link":"https://rexrock.github.io/post/k8s2/"},{"title":"使用kubeadm搭建一个k8s集群","content":"官方文档： Installing kubeadm Creating a cluster with kubeadm 参考： 使用kubeadm安装Kubernetes 1.11 Kubernetes kubectl run 命令详解 k8s的API手册 1. 安装docker curl -sSL https://get.docker.com | sh cat &gt; /etc/docker/daemon.json &lt; { &quot;registry-mirrors&quot;: [&quot;https://dic5s40p.mirror.aliyuncs.com&quot;] } EOF 使用国内的镜像仓库： # vi /etc/docker/daemon.json { &quot;registry-mirrors&quot;: [ &quot;https://hub-mirror.c.163.com&quot;, &quot;https://mirror.baidubce.com&quot;, &quot;https://dic5s40p.mirror.aliyuncs.com&quot; ] } # systemctl restart docker.service 然后查看修改是否生效： # docker info | tail -10 Debug Mode: false Experimental: false Insecure Registries: 127.0.0.0/8 Registry Mirrors: https://hub-mirror.c.163.com/ https://mirror.baidubce.com/ https://dic5s40p.mirror.aliyuncs.com/ Live Restore Enabled: false 废弃方法：然后在/etc/default/docker中添加: DOCKER\\_OPTS=&quot;--registry-mirror=https://dic5s40p.mirror.aliyuncs.com&quot; 然后执行： systemctl restart docker 2. 安装kubeadm、kubelet、kubectl https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/ #!/bin/bash set -e apt-get -y install apt-transport-https ca-certificates curl software-properties-common curl -s https://mirrors.aliyun.com/kubernetes/apt/doc/apt-key.gpg | apt-key add - add-apt-repository \\ &quot;deb http://mirrors.ustc.edu.cn/kubernetes/apt \\ kubernetes-xenial \\ main&quot; apt-get update apt-get install -y kubelet=1.17.17-00 kubeadm=1.17.17-00 kubectl=1.17.17-00 systemctl enable kubelet &amp;&amp; systemctl start kubelet 3. Cgroup设置 Cgroup驱动设置 Debian11默认使用cgroupv2，但是没通过systemd管理，然后kubeadm要求cgroupv2必须使用systemd管理。所以切回cgroup1。kubelet默认也是通过cgroup驱动管理，如果用systemd还需要修改配置文件，麻烦： # 在内核启动参数中加入 systemd.unified_cgroup_hierarchy=false systemd.legacy_systemd_cgroup_controller=false 4. 无网环境部署k8s集群 提前拉取所需镜像 kubeadm config images list kubeadm config images pull --image-repository registry.cn-hangzhou.aliyuncs.com/google\\_containers 5. 初始化master节点 kubeadm init --pod-network-cidr=10.17.0.0/16 --service-cidr=10.18.200.0/24 --kubernetes-version=v1.18.5 --image-repository registry.cn-hangzhou.aliyuncs.com/google\\_containers 成功会显示 kubeadm join 192.168.100.12:6443 --token yskexa.twu83wmh7n64oczk \\ --discovery-token-ca-cert-hash sha256:d6dcfecc04d8452875155de28dc229eb4f7842eb55e8f998cade89cc625a679e 6. 为了可以执行kubectl rm -rf $HOME/.kube mkdir -p $HOME/.kube cp -i /etc/kubernetes/admin.conf $HOME/.kube/config chown $(id -u):$(id -g) $HOME/.kube/config 7. 安装pod network wget https://docs.projectcalico.org/v3.14/manifests/calico.yaml # 什么都不要改，会自动检测出pod ip的范围 kubectl create -f calico.yaml 通过meta-plugin部署macvlan网络（前提也得是先部署calico，但是支持让macvlan口变为默认接口）： spidernet-io 为什么选择该plugin呢，因为道客应该用的就是该方案： Daocloud部署macvlan 在VPC环境中，macvlan可能不通，所以可以换成ipvlan，配置基本一致，只不过ipvlan的mode是l2/l3/l3s： 配置ipvlan 定义MacVlan网络（原生的ipam不支持跨节点管理ip资源分配，因此需要为每个节点创建相应的网络）： apiVersion: k8s.cni.cncf.io/v1 kind: NetworkAttachmentDefinition metadata: name: macvlan-overlay-big1 namespace: kube-system spec: config: |- { &quot;cniVersion&quot;: &quot;0.3.1&quot;, &quot;name&quot;: &quot;macvlan-overlay-big1&quot;, &quot;plugins&quot;: [ { &quot;type&quot;: &quot;macvlan&quot;, &quot;master&quot;: &quot;eth0&quot;, &quot;mode&quot;: &quot;bridge&quot;, &quot;ipam&quot;: { &quot;type&quot;: &quot;host-local&quot;, &quot;subnet&quot;: &quot;172.16.252.0/22&quot;, &quot;rangeStart&quot;: &quot;172.16.253.102&quot;, &quot;rangeEnd&quot;: &quot;172.16.253.151&quot;, &quot;routes&quot;: [ { &quot;dst&quot;: &quot;0.0.0.0/0&quot; } ], &quot;gateway&quot;: &quot;172.16.252.1&quot; } },{ &quot;type&quot;: &quot;router&quot;, &quot;service_hijack_subnet&quot;: [&quot;10.18.0.0/16&quot;], &quot;overlay_hijack_subnet&quot;: [&quot;10.17.0.0/16&quot;], &quot;additional_hijack_subnet&quot;: [], &quot;migrate_route&quot;: -1, &quot;rp_filter&quot;: { &quot;set_host&quot;: true, &quot;value&quot;: 0 }, &quot;overlay_interface&quot;: &quot;eth0&quot;, &quot;skip_call&quot;: false } ] } --- apiVersion: k8s.cni.cncf.io/v1 kind: NetworkAttachmentDefinition metadata: name: macvlan-standalone-big1 namespace: kube-system spec: config: |- { &quot;cniVersion&quot;: &quot;0.3.1&quot;, &quot;name&quot;: &quot;macvlan-standalone-big1&quot;, &quot;plugins&quot;: [ { &quot;type&quot;: &quot;macvlan&quot;, &quot;master&quot;: &quot;eth0&quot;, &quot;mode&quot;: &quot;bridge&quot;, &quot;ipam&quot;: { &quot;type&quot;: &quot;host-local&quot;, &quot;subnet&quot;: &quot;172.16.252.0/22&quot;, &quot;rangeStart&quot;: &quot;172.16.253.2&quot;, &quot;rangeEnd&quot;: &quot;172.16.253.51&quot;, &quot;routes&quot;: [ { &quot;dst&quot;: &quot;0.0.0.0/0&quot; } ], &quot;gateway&quot;: &quot;172.16.252.1&quot; } },{ &quot;type&quot;: &quot;veth&quot;, &quot;service_hijack_subnet&quot;: [&quot;10.18.0.0/16&quot;], &quot;overlay_hijack_subnet&quot;: [&quot;10.17.0.0/16&quot;], &quot;additional_hijack_subnet&quot;: [], &quot;migrate_route&quot;: -1, &quot;rp_filter&quot;: { &quot;set_host&quot;: true, &quot;value&quot;: 0 }, &quot;skip_call&quot;: false } ] } 定义IPVlan网络（原生的cni也不支持固化pod的ip，但是可以通过创建只有一个ip的网络来实现）： apiVersion: k8s.cni.cncf.io/v1 kind: NetworkAttachmentDefinition metadata: name: ipvlan-standalone-251 namespace: kube-system spec: config: |- { &quot;cniVersion&quot;: &quot;0.3.1&quot;, &quot;name&quot;: &quot;ipvlan-standalone-251&quot;, &quot;plugins&quot;: [ { &quot;type&quot;: &quot;ipvlan&quot;, &quot;master&quot;: &quot;eth0&quot;, &quot;mode&quot;: &quot;l3&quot;, &quot;ipam&quot;: { &quot;type&quot;: &quot;host-local&quot;, &quot;subnet&quot;: &quot;172.16.252.0/22&quot;, &quot;rangeStart&quot;: &quot;172.16.254.251&quot;, &quot;rangeEnd&quot;: &quot;172.16.254.251&quot;, &quot;routes&quot;: [ { &quot;dst&quot;: &quot;0.0.0.0/0&quot; } ], &quot;gateway&quot;: &quot;172.16.252.1&quot; } },{ &quot;type&quot;: &quot;veth&quot;, &quot;service_hijack_subnet&quot;: [&quot;10.18.0.0/16&quot;], &quot;overlay_hijack_subnet&quot;: [&quot;10.17.0.0/16&quot;], &quot;additional_hijack_subnet&quot;: [], &quot;migrate_route&quot;: -1, &quot;rp_filter&quot;: { &quot;set_host&quot;: true, &quot;value&quot;: 0 }, &quot;skip_call&quot;: false } ] } 8. 添加worker节点 在worker节点上执行kubeadm init成功后返回的命令，即 kubeadm join 192.168.100.12:6443 --token 7u1jah.da6w4tilh0j5097w \\ --discovery-token-ca-cert-hash sha256:bcd0ce4354f2e8b794b830d7a14389b6a06e46e225486ece8218424a1744583f 注意：token的有效期只有24小时，我们可以用如下命令查看可用的token kubeadm token list 如果为空，我们可以通过如下命令创建token kubeadm token create 如果你连cert-hash也忘了，那么可以通过如下命令查看 openssl x509 -pubkey -in /etc/kubernetes/pki/ca.crt | openssl rsa -pubin -outform der 2&gt;/dev/null | \\ openssl dgst -sha256 -hex | sed 's/^.* //' 设置worker节点role: kubectl label node deb11-vhu1-big2 kubernetes.io/role=worker 9. 删除worker节点 kubectl drain &lt;node name&gt; --delete-local-data --force --ignore-daemonsets # 下面三条命令在worker节点上运行 kubeadm reset iptables -F &amp;&amp; iptables -t nat -F &amp;&amp; iptables -t mangle -F &amp;&amp; iptables -X ipvsadm -C # 如果使用了ipvs kubectl delete node &lt;node name&gt; 10. 删除master节点 # 在master节点上运行 kubeadm reset 11 开启关闭dns kubectl -n kube-system scale --replicas=0 deployment/coredns kubectl -n kube-system scale --replicas=1 deployment/coredns 12. 让master节点也可以调度pod kubectl taint nodes --all node-role.kubernetes.io/master- ","link":"https://rexrock.github.io/post/k8s1/"},{"title":"XDP技术简介","content":"1. XDP程序的运行位置 XDP（eXpress Data Path）提供了一个内核态、高性能、可编程 BPF 包处理框架。这个框架在软件中最早可以处理包的位置（即网卡驱动收到包的 时刻）运行 BPF 程序。如下图所示： NAPI poll 机制不断调用驱动实现的 poll 方法，后者处理 RX 队列内的包，并最终将包送到正确的程序，也就是我们所说的 XDP 程序。所以很明显这需要网卡驱动的支持，如果驱动支持 XDP ，那 XDP 程序将在 poll 机制内执行。如果不支持，那 XDP 程序将只能在更后面的位置被执行，即上图中的receive_skb中。这其中经历了哪些步骤呢？ 创建skb，如果不支持XDP，poll机制会将报文送给 clean_rx()，该函数会创建一个skb，并skb进行一些硬件校验何检查，然后较给 gro_receive() 函数； 分片重组，GRO可以理解为LRO的软件实现，相比LRO只针对TCP报文，GRO可以处理更多其他类型的报文，总之在 gro_receive() 函数中，如果是分片报文则进行分片重组然后交给 receive_skb() 函数，如果不是分片报文，则直接交给 receive_skn() 函数进行处理； 2. XDP的三种工作模式 上面提到XDP程序可以运行在不同位置，每个位置即对应XDP的一种工作模式： Native XDP，即运行在网卡驱动实现的的 poll() 函数中，需要网卡驱动的支持； Generic XDP，即上面提到的如果网卡驱动不支持XDP，则可以运行在 receive_skb() 函数中； Offloaded XDP，这种模式是指将XDP程序offload到网卡中，这需要网卡硬件的支持，JIT编译器将BPF代码翻译成网卡原生指令并在网卡上运行。 3. XDP程序的返回码 XDP程序执行结束后会返回一个结果，告诉调用者接下来如何处理这个包： XDP_DROP，丢弃这个包，主要用于报文过滤的安全场景； XDP_PASS，将这个包“交给/还给”内核，继续走正常的内核处理流程； XDP_TX，从收到包的网卡上再将这个包发出去（即hairpin模式），主要用于负载均衡场景； XDP_REDIRECT，何XDP_TX类似，但是是通过另外一个网卡将包发出去。除此之外还可以实现将报文重定向到其他的CPU处理，类似于XDP_PASS继续走内核处理流程，但是由其他的CPU处理，当前CPU继续处理后续的报文接收； XDP_ABORTED，表示程序产生异常，行为类似XDP_DROP，但是会通过一个tracepoint打印日志义工追踪； 下面是 Mellanox mlx5 驱动中关于XDP的处理，如果该函数返回 true，则说明报文被XDP处理了，不用再走内核协议栈了，如果返回 false 则创建SKB然后继续走内核协议栈： /* returns true if packet was consumed by xdp */ bool mlx5e_xdp_handle(struct mlx5e_rq *rq, struct mlx5e_dma_info *di, u32 *len, struct xdp_buff *xdp) { struct bpf_prog *prog = rcu_dereference(rq-&gt;xdp_prog); u32 act; int err; if (!prog) return false; act = bpf_prog_run_xdp(prog, xdp); switch (act) { case XDP_PASS: *len = xdp-&gt;data_end - xdp-&gt;data; return false; case XDP_TX: if (unlikely(!mlx5e_xmit_xdp_buff(rq-&gt;xdpsq, rq, di, xdp))) goto xdp_abort; __set_bit(MLX5E_RQ_FLAG_XDP_XMIT, rq-&gt;flags); /* non-atomic */ return true; case XDP_REDIRECT: if (xdp-&gt;rxq-&gt;mem.type != MEM_TYPE_XSK_BUFF_POOL) { page_ref_sub(di-&gt;page, di-&gt;refcnt_bias); di-&gt;refcnt_bias = 0; } /* When XDP enabled then page-refcnt==1 here */ err = xdp_do_redirect(rq-&gt;netdev, xdp, prog); if (unlikely(err)) goto xdp_abort; __set_bit(MLX5E_RQ_FLAG_XDP_XMIT, rq-&gt;flags); __set_bit(MLX5E_RQ_FLAG_XDP_REDIRECT, rq-&gt;flags); if (xdp-&gt;rxq-&gt;mem.type != MEM_TYPE_XSK_BUFF_POOL) mlx5e_page_dma_unmap(rq, di); rq-&gt;stats-&gt;xdp_redirect++; return true; default: bpf_warn_invalid_xdp_action(act); fallthrough; case XDP_ABORTED: xdp_abort: trace_xdp_exception(rq-&gt;netdev, prog, act); fallthrough; case XDP_DROP: rq-&gt;stats-&gt;xdp_drop++; return true; } } 疑问？ 如果我们相对报文执行 redirect，那么我们在BPF程序中需要执行 bpf_redirect() / bpf_redirect_map()，但是从上面的代码中看，从我们的BPF程序返回后，驱动程序也调用了一个叫做 xdp_do_redirect() 的函数。那么问题来了，报文的 redirect 到底是在什么时候执行的呢？答案后面揭晓。 接着分析： case XDP_REDIRECT: /* When XDP enabled then page-refcnt==1 here */ err = xdp_do_redirect(rq-&gt;netdev, &amp;xdp, prog); if (unlikely(err)) goto xdp_abort; __set_bit(MLX5E_RQ_FLAG_XDP_XMIT, rq-&gt;flags); __set_bit(MLX5E_RQ_FLAG_XDP_REDIRECT, rq-&gt;flags); if (!xsk) mlx5e_page_dma_unmap(rq, di); rq-&gt;stats-&gt;xdp_redirect++; return true; XDP程序返回后，驱动会根据XDP程序的返回码去真正执行 action。我们以 XDP_REDIRECT 为例，继续跟踪 xdp_do_redirect() 函数： // &gt;&gt; net/core/filter.c xdp_do_redirect(netdev, xdp_buff, xdp_prog) =&gt; xdp_do_redirect_map(netdev, xdp_buff, xdp_prog, bpf_map, bpf_redirect_info) =&gt; __bpf_tx_xdp_map(netdev, fwd, bpf_map, xdp_buff, index) =&gt; // fwd即xdp_sock； // &gt;&gt; kernel/bpf/xskmap.c __xsk_map_redirect(bpf_map, xdp_buff, xdp_sock) =&gt; // &gt;&gt; net/xdp/xsk.c xsk_rcv(xdp_sock, xdp_buff) __xsk_rcv(xdp_sock, xdp_buff, len) 我们主要看下 xsk_rck() 和 __xsk_rcv() 两个函数： int xsk_rcv(struct xdp_sock *xs, struct xdp_buff *xdp) { u32 len; if (!xsk_is_bound(xs)) return -EINVAL; // AF_XDP技术详解中曾介绍过，AF_XDP socket是跟具体的网卡RX队列绑定的，这里再真正执行 // 收包前做了依次判断(虽然XDP程序中也有判断，但毕竟不是强制的) if (xs-&gt;dev != xdp-&gt;rxq-&gt;dev || xs-&gt;queue_id != xdp-&gt;rxq-&gt;queue_index) return -EINVAL; len = xdp-&gt;data_end - xdp-&gt;data; return (xdp-&gt;rxq-&gt;mem.type == MEM_TYPE_ZERO_COPY) ? __xsk_rcv_zc(xs, xdp, len) : __xsk_rcv(xs, xdp, len); } static int __xsk_rcv(struct xdp_sock *xs, struct xdp_buff *xdp, u32 len) { u64 offset = xs-&gt;umem-&gt;headroom; u64 addr, memcpy_addr; void *from_buf; u32 metalen; int err; // 从 FILL RING中获取可以承载报文数据的desc if (!xskq_peek_addr(xs-&gt;umem-&gt;fq, &amp;addr, xs-&gt;umem) || len &gt; xs-&gt;umem-&gt;chunk_size_nohr - XDP_PACKET_HEADROOM) { xs-&gt;rx_dropped++; return -ENOSPC; } if (unlikely(xdp_data_meta_unsupported(xdp))) { from_buf = xdp-&gt;data; metalen = 0; } else { from_buf = xdp-&gt;data_meta; metalen = xdp-&gt;data - xdp-&gt;data_meta; } // 执行报文数据的copy，该函数时非zero copy模式下的执行函数 memcpy_addr = xsk_umem_adjust_offset(xs-&gt;umem, addr, offset); __xsk_rcv_memcpy(xs-&gt;umem, memcpy_addr, from_buf, len, metalen); offset += metalen; addr = xsk_umem_adjust_offset(xs-&gt;umem, addr, offset); // 插入到 RX RING中 err = xskq_produce_batch_desc(xs-&gt;rx, addr, len); if (!err) { xskq_discard_addr(xs-&gt;umem-&gt;fq); xdp_return_buff(xdp); return 0; } xs-&gt;rx_dropped++; return err; } 结论： bpf_redirect() 和 bpf_redirect_map() 应该只是填充bpf_redirect_info结构（即redirect的target相关的数据），真正的redirect操作仍由驱动在 XDP程序返回后执行。 // &gt;&gt; include/linux/filter.h struct bpf_redirect_info { u32 flags; u32 tgt_index; void *tgt_value; struct bpf_map *map; struct bpf_map *map_to_flush; u32 kern_flags; }; // &gt;&gt; net/core/filter.c: int xdp_do_redirect(struct net_device *dev, struct xdp_buff *xdp, struct bpf_prog *xdp_prog) { struct bpf_redirect_info *ri = this_cpu_ptr(&amp;bpf_redirect_info); struct bpf_map *map = READ_ONCE(ri-&gt;map); if (likely(map)) return xdp_do_redirect_map(dev, xdp, xdp_prog, map, ri); return xdp_do_redirect_slow(dev, xdp, xdp_prog, ri); } 分析的没错，bpf_redirect_map()函数定义如下： // &gt;&gt; net/core/filter.c BPF_CALL_3(bpf_xdp_redirect_map, struct bpf_map *, map, u32, ifindex, u64, flags) { struct bpf_redirect_info *ri = this_cpu_ptr(&amp;bpf_redirect_info); /* Lower bits of the flags are used as return code on lookup failure */ if (unlikely(flags &gt; XDP_TX)) return XDP_ABORTED; ri-&gt;tgt_value = __xdp_map_lookup_elem(map, ifindex); if (unlikely(!ri-&gt;tgt_value)) { /* If the lookup fails we want to clear out the state in the * redirect_info struct completely, so that if an eBPF program * performs multiple lookups, the last one always takes * precedence. */ WRITE_ONCE(ri-&gt;map, NULL); return flags; } ri-&gt;flags = flags; ri-&gt;tgt_index = ifindex; WRITE_ONCE(ri-&gt;map, map); return XDP_REDIRECT; } ","link":"https://rexrock.github.io/post/xdp1/"},{"title":"Run ebpf with tc","content":"1. TC的几个概念 队列——qdisc(queueing discipline)，分为不可分类队列（classless qdisc）和可分类队列（classful qdisc）,一个qdisc会被分配一个主序列号，叫做句柄(handle)，然后把从序列号作为类的命名空间。句柄采用象10:一样的表达方式。习惯上，需要为有子类的QDisc显式地分配一个句柄。队列真正的实现QoS功能； 类别——class，通过分类器将流量划分为不同的class，一个class对应一个qos对象（即一个具体的可以配置qos策略的子队列），添加class的时候需要指定该class对应的qos策略（就是给多少带宽这种）； 分类器——filter，用于将流量划分为不同的class； 用一张图表示三者的关系，如下图所示： 2. 举个简单的TC例子 2.1 创建队列 有关队列的TC命令的一般形式为: tc qdisc [add|change|replace|link] dev DEV [parent qdisk-id|root][handle qdisc-id] qdisc[qdisc specific parameters] 首先，需要为网卡eth0配置一个HTB队列，使用下列命令: tc qdisc add dev eth0 root handle 1:htb default 11 参数说明： add 表示要添加 dev eth0 表示要操作的网卡为eth0 root 表示为网卡eth0添加的是一个根队列 handle 1: 表示队列的句柄为1: htb 表示要添加的队列为HTB队列 命令最后的”default 11 是htb特有的队列参数，意思是所有未分类的流量都将分配给类别1:11 2.2 创建类别 有关类别的TC 命令的一般形式为: tc class [add|change|replace] dev DEV parent qdisc-id [classid class-id] qdisc [qdisc specific parameters] 可以利用下面这三个命令为根队列1创建三个类别，分别是1:11、1:12和1:13，它们分别占用40、40和20mb[t的带宽。 tc class add dev eth0 parent 1: classid 1:11 htb rate 40mbit ceil 40mbit tc class add dev eth0 parent 1: classid 1:12 htb rate 40mbit ceil 40mbit tc class add dev eth0 parent 1: cllassid 1:13 htb rate 20mbit ceil 20mbit 参数说明： parent 1: 表示类别的父亲为根队列1: classid1:11 表示创建一个标识为1:11的类别 rate 40mbit 表示系统将为该类别确保带宽40mbit ceil 40mbit 表示该类别的最高可占用带宽为40mbit 注意， 在TC 中使用下列的缩写表示相应的带宽： Kbps kiIobytes per second， 即”千字节每秒 Mbps megabytes per second， 即”兆字节每秒 Kbit kilobits per second，即”千比特每秒 Mbit megabits per second， 即”兆比特每秒 2.3 创建分类器 有关过滤器的TC 命令的一般形式为: tc filter [add|change|replace] dev DEV [parent qdisc-id|root] protocol protocol prio priority filtertype [filtertype specific parameters] flowid flow-id 由于需要将WWW、E-mail、Telnet三种流量分配到三个类别，即上述1:11、1:12和1:13，因此，需要创建三个过滤器，如下面的三个命令: tc filter add dev eth0 protocol ip parent 1:0 prio 1 u32 match ip dport 80 0xffff flowid 1:11 tc filter add dev eth0 prtocol ip parent 1:0 prio 1 u32 match ip dport 25 0xffff flowid 1:12 tc filter add dev eth0 protocol ip parent 1:0 prio 1 u32 match ip dport 23 oxffff flowid 1:13 参数说明： protocol ip 表示该过滤器应该检查报文分组的协议字段 prio 1 表示它们对报文处理的优先级是相同的，对于不同优先级的过滤器， 系统将按照从小到大的优先级顺序来执行过滤器，对于相同的优先级，系统将按照命令的先后顺序执行。 这几个过滤器还用到了u32选择器(命令中u32后面的部分)来匹配不同的数据流。以第一个命令为例，判断的是dport字段，如果该字段与Oxffff进行与操作的结果是80，则“flowid 1:11” 表示将把该数据流分配给类别1:11 2.4 ingress qdisc ingress qdisc没有任何参数，我们可以像下面这样添加一个ingress qdisc: tc qdisc add dev eth0 ingress ingress qdisc不占用根队列，创建ingress qdisc后我们还能继续创建其他的tx的qdisc； ingress qdisc不支持任何子类别，所以我们无法为ingress qdisc创建class，但是我们可以直接为ingress qdisc创建分类器； tc qdisc add dev eth0 handle ffff: ingress tc filter add dev eth0 parent ffff: protocol all prio 49 basic police rate 10mbit burst 1mb mtu 65535 drop police参考：https://man7.org/linux/man-pages/man8/tc-police.8.html 参数说明： rate 限制的最大流量？后面的drop动作是指超过限速的流量还是命中的流量？？？ burst 每个计时器的流量峰值，应该同HTB的burst mtu 匹配的mtu drop ？ 3. 如何使用tc的ebpf功能 从内核4.1版本起，tc引入了一个特殊的qdisc，叫做clsact，它为TC提供了一个可以加载BPF程序的入口，使TC和XDP一样，成为一个可以加载BPF程序的网络钩子。 TC vs XDP 这两个钩子都可以用于相同的应用场景，如DDoS缓解、隧道、处理链路层信息等。但是，由于XDP在任何套接字缓冲区（SKB）分配之前运行，所以它可以达到比TC上的程序更高的吞吐量值。然而，后者可以从通过 struct __sk_buff 提供的额外的解析数据中受益，并且可以执行 BPF 程序，对入站流量和出站流量都可以执行 BPF 程序，是 TX 链路上的能被操控的最后一个点。 无需网卡驱动的支持 tc BPF 程序不需要驱动做任何改动，因为它们运行在网络栈通用层中的 hook 点。因此，它们可以 attach 到任何类型的网络设备上。 Ingress 这提供了很好的灵活性，但跟运行在原生 XDP 层的程序相比，性能要差一些。然而，tc BPF 程序仍然是内核的通用 data path 做完 GRO 之后、且处理任何协议之前 最早的 处理点。传统的 iptables 防火墙也是在这里处理的，例如 iptables PREROUTING 或 nftables ingress hook 或其他数据包包处理过程。 Egress 类似的，对于 egress，tc BPF 程序在将包交给驱动之前的最晚的地方（latest point）执 行，这个地方在传统 iptables 防火墙 hook 之后（例如 iptables POSTROUTING）， 但在内核 GSO 引擎之前。 **详细参考：** http://arthurchiao.art/blog/cilium-bpf-xdp-reference-guide-zh/#prog_type_tc 4. 最佳实践 参考：https://github.com/rexrock/tc-xdp-drop-tcp 注意：需使用4.20及以上版本的内核，才能使veth支持XDP ","link":"https://rexrock.github.io/post/ebpf2/"},{"title":"eBPF的使用限制","content":"1. 不能使用循环语句 eBPF程序中不能使用循环语句，如果非要使用循环，则必须通过编译选项“#pragma clang loop unroll(full)”让编译器在编译过程中将循环展开。 此外必须要注意的一点是，循环中的语句必须是单一且独立的块，如下： static __always_inline int search_service_ip(int i, __u32 ip) { ... return 0; } static __always_inline int is_service_ip(__u32 ip) { #pragma clang loop unroll(full) for(int i = 0; i &lt; 32; i++) { switch(search_service_ip(i, ip)) { case 0: continue; case 1: return 1; default: break; } } return 0; } 2. map操作的原子性 ebpf提供函数map_update_elem()对ebpf map表中的数据进行更新，该函数在对BPF_HASH表进行操作时是原子操作，对BPF_ARRAY操作时是非原子的。 即使map_update_elem(）全部是原子操作，我们执行累加的流程是“ lookup elem -&gt; elem++ -&gt; update elem”，这一串操作也没办法保证原子性。 对ebpf map进行原子更新我们分内核态和用户态两种场景： 在内核态运行的ebpf程序可直接对ebpf map中的数据进行操作，加上bpf_map_lookup_elem()返回的是map中数据的指针。我们可以借助编译器原语（__sync_fetch_and_add）在LLVM生成eBPF指令时，以原子方式对bpf_map_lookup_elem()返回的数据指针直接进行加减以此实现原子操作。（bcc的llvm编译器是内置的，所以已将该原语封装成lock_xadd()函数） 目前用户态程序对ebpf map进行累加修改，还没有办法保证原子性，所以在程序设计阶段，务必保证不要让用户态程序和内核态程序同时对ebpf map进行类似累加修改的操作。 ","link":"https://rexrock.github.io/post/ebpf1/"},{"title":"玩转sriov-network-device-plugin","content":"sriov-network-device-plugin需基于multus/danm和srioc-cni，所以我们依次安装multus、sriov-cni、sriov-network-device-plugin。 1. 安装Multus Multus项目地址：[https://github.com/intel/multus-cni.git](https://github.com/intel/multus-cni.git cd multus-cni-master kubectl create -f file://C:/Users/liyang07/Documents/Gridea/post-images/multus-daemonset.yml 部署完成后： 每个node上都会运行一个multus的守护进程； 获取当前“主cni”配置，并创建一个新的multus cni配置/etc/cni/net.d/00-multus.conf，以劫持cni 配置入口： 创建/etc/cni/net.d/multus.d，用来存储multus访问API server的验证文件； 验证安装是否成功： # kubectl get pods --all-namespaces | grep -i multus kube-system kube-multus-ds-amd64-4ncw6 1/1 Running 0 17h kube-system kube-multus-ds-amd64-jgzp4 1/1 Running 2 24h OK，让我们用macvlan口来验证一下Multus是否可以正常工作。 apiVersion: &quot;k8s.cni.cncf.io/v1&quot; kind: NetworkAttachmentDefinition metadata: name: macvlan-conf spec: config: '{ &quot;cniVersion&quot;: &quot;0.3.0&quot;, &quot;type&quot;: &quot;macvlan&quot;, &quot;master&quot;: &quot;bond2.100&quot;, &quot;mode&quot;: &quot;bridge&quot;, &quot;ipam&quot;: { &quot;type&quot;: &quot;host-local&quot;, &quot;subnet&quot;: &quot;192.168.1.0/24&quot;, &quot;rangeStart&quot;: &quot;192.168.1.200&quot;, &quot;rangeEnd&quot;: &quot;192.168.1.216&quot;, &quot;routes&quot;: [ { &quot;dst&quot;: &quot;0.0.0.0/0&quot; } ], &quot;gateway&quot;: &quot;192.168.1.1&quot; } }' Multus在部署的时候，顺便创建了一个CRD，用来让用户定义想要添加什么样的“副CNI”。上面的配置定义了我们想要给指定的pod添加基于macvlan的网口。下面让我来创建一个需要添加macvlan接口的pod： apiVersion: v1 kind: Pod metadata: name: samplepod annotations: k8s.v1.cni.cncf.io/networks: macvlan-conf spec: containers: - name: samplepod command: [&quot;/bin/ash&quot;, &quot;-c&quot;, &quot;trap : TERM INT; sleep infinity &amp; wait&quot;] image: alpine 注意： 并不是所有的pod创建都会自动添加副接口，我们需要通过annotations指定，我们想要给pod添加“哪些”副接口。pod成功Running后，我们查看pod里面的网卡配置，可以看到名为net1的我们创建的macvlan接口： OK，如果我们想要添加更多的“副接口”呢，配置如下： apiVersion: v1 kind: Pod metadata: name: samplepod annotations: k8s.v1.cni.cncf.io/networks: macvlan-conf, sriov-net1 # 新增sriov-net1类型的接口 spec: containers: - name: samplepod command: [&quot;/bin/ash&quot;, &quot;-c&quot;, &quot;trap : TERM INT; sleep infinity &amp; wait&quot;] image: alpine resources: # 这是sriov接口特有的配置，这里先忽略 requests: intel.com/mlnx_sriov: '1' limits: intel.com/mlnx_sriov: '1' 应用后，我们可以分别看到名为net1的vxlan接口和名为net2的sriov接口： 说明：上述配置是我在以完成sriov-network-device-plugin安装的情况下才可以配置sriov接口。 2. 安装sriov-cni 项目地址：https://github.com/k8snetworkplumbingwg/sriov-cni.git 没有太多好讲的，把二进制编译出来放到/opt/cni/bin/目录下即可： # git clone https://github.com/k8snetworkplumbingwg/sriov-cni.git # cd sriov-cni # make # cp build/sriov /opt/cni/bin 一般cni的安装需要cni二进制（/opt/cni/bin/）+cni配置文件（/etc/cni/net.d/），因为这里所有的cni配置已经被multus劫持，所以只需要安装二进制文件即可，而具体的每个副cni配置则通过multus的CRD NetworkAttachmentDefinition来定义。 3. 安装sriov-network-device-plugin 项目地址：https://github.com/k8snetworkplumbingwg/sriov-network-device-plugin.git # git clone https://github.com/k8snetworkplumbingwg/sriov-network-device-plugin.git # cd sriov-network-device-plugin/ 这里需要编辑下configmap文件，这个configmap使用来定义sriov资源的，我用的mellanox网卡，配置如下： 参数解析： vendors通过lspci -v -s pci_addr可以查看； devices，我的网卡pf时1017，vf是1018，包括驱动，dpdk-devbind.py都可以查看。 开始部署： # kubectl create -f configMap.yaml # kubectl create -f k8s-v1.16/sriovdp-daemonset.yaml # kubectl get pod --all-namespaces | grep sriov kube-system kube-sriov-device-plugin-amd64-5vsnr 1/1 Running 0 18h kube-system kube-sriov-device-plugin-amd64-lr8wh 1/1 Running 0 19h sriov-device-plugin成功部署，我们可以看到vf被添加到相应的资源池： # kubectl get node dell740.it.163.org -o json | jq '.status.allocatable' { &quot;cpu&quot;: &quot;48&quot;, &quot;ephemeral-storage&quot;: &quot;260988928388&quot;, &quot;hugepages-1Gi&quot;: &quot;0&quot;, &quot;hugepages-2Mi&quot;: &quot;0&quot;, &quot;intel.com/intel_sriov_dpdk&quot;: &quot;0&quot;, &quot;intel.com/intel_sriov_netdevice&quot;: &quot;0&quot;, &quot;intel.com/mlnx_sriov&quot;: &quot;24&quot;, &quot;memory&quot;: &quot;394747480Ki&quot;, &quot;pods&quot;: &quot;110&quot; } OK，sriov-device-plugin到现在算是部署成功了，接下来我们可以创建基于sriov的“副CNI”了： apiVersion: &quot;k8s.cni.cncf.io/v1&quot; kind: NetworkAttachmentDefinition metadata: name: sriov-net1 annotations: k8s.v1.cni.cncf.io/resourceName: intel.com/mlnx_sriov spec: config: '{ &quot;type&quot;: &quot;sriov&quot;, &quot;cniVersion&quot;: &quot;0.3.1&quot;, &quot;name&quot;: &quot;sriov-network&quot;, &quot;ipam&quot;: { &quot;type&quot;: &quot;host-local&quot;, &quot;subnet&quot;: &quot;172.10.1.0/24&quot;, &quot;routes&quot;: [{ &quot;dst&quot;: &quot;0.0.0.0/0&quot; }] } }' 注意：annotations里指定的resourceName，必须跟前面configmap定义的资源名称“完全一致”。 OK，创建基于sriov的pod的配置前面已经贴过了。置于configmap以及pod yaml中的配置参数，可以参考项目中的文档。 ","link":"https://rexrock.github.io/post/k8s-net-sriov/"},{"title":"Cilium datapath梳理","content":"1. Cilium datapath的组成 1.1 Cilium中的流量劫持点 所以流量劫持从prog类型上可以分为： 内核层面基于sock的流量劫持，主要用于lb（k8s-proxy）; 基于端口流量劫持，实现整个datapath的替换； 置于cilium_host和cilium_net: 1.2 Cilium中ebpf map的构成 2. Cilium datapath加载流程 2.1 公共ebpf map的初始化 cilium有很多公用的ebpf map，这些map在ebpf prog加载前被创建： runDaemon() =&gt;NewDaemon() =&gt;Daemon.initMaps() cilium_call_policy，PROG_ARRAY，用来装“to-contaner” cilium_ct4_global，CT表，for tcp cilium_ct_any4_global，CT表，for non-tcp cilium_events， cilium_ipcache，ip+mask -&gt; sec_label + VETP,如果是本地，则VETP为0 cilium_ipv4_frag_datagrams cilium_lb4_affinity cilium_lb4_backends cilium_lb4_reverse_nat cilium_lb4_reverse_sk cilium_lb4_services_v2 cilium_lb_affinity_match cilium_lxc，本地endpoint对应的netdev，ip -&gt; NETDEV-INFO cilium_metrics cilium_nodeport_neigh4 cilium_signals cilium_snat_v4_external cilium_tunnel_map，ip -&gt; VETP，只记录非本地的ip 2.2 基础网络构建(init.sh) 2.2.1 初始化参数 LIB=/var/lib/cilium/bpf，bpf源码所在目录 RUNDIR=/var/run/cilium/state，工作目录 IP4_HOST=10.17.0.7，cilium_host的ipv4地址 IP6_HOST=nil MODE=vxlan，网络模式 NATIVE_DEVS=eth0，出口网卡，可以手动指定，没指定的话就看默认路由走那个口 XDP_DEV=nil XDP_MODE=nil MTU=1500 IPSEC=false ENCRYPT_DEV=nil HOSTLB=true HOSTLB_UDP=true HOSTLB_PEER=false CGROUP_ROOT=/var/run/cilium/cgroupv2 BPFFS_ROOT=/sys/fs/bpf NODE_PORT=true NODE_PORT_BIND=true MCPU=v2 NODE_PORT_IPV4_ADDRS=eth0=0xc64a8c0 NODE_PORT_IPV6_ADDRS=nil NR_CPUS=64 2.2.2 具体工作 1）创建了cilium_host和cilium_net； 2）如果是vxlan模式，添加并设置vxlan口cilium_vxlan； 3）编译并加载cilium_vxlan相关的prog和map； 2个map： cilium_calls_overlay_2，每个endpoint都有自己独立的tail call map，2是init.sh脚本固定写死的ID_WORLD； cilium_encrypt_state 6个prog： from-container：bpf_overlay.c to-container：bpf_overlay.c cilium_calls_overlay_2【1】 = __send_drop_notify：lib/drop.h cilium_calls_overlay_2【7】 = tail_handle_ipv4：bpf_overlay.c cilium_calls_overlay_2【15】= tail_nodeport_nat_ipv4：lib/nodeport.h cilium_calls_overlay_2【17】= tail_rev_nodeport_lb4：lib/nodeport. 4）删除出口网卡已经挂载的ebpf程序（from-netdev和to-netdev） 5）加载LB相关ebpf和map； tc exec bpf pin /sys/fs/bpf/tc/globals/cilium\\_cgroups\\_connect6 obj bpf\\_sock.o type sockaddr attach\\_type connect6 sec connect6 tc exec bpf pin /sys/fs/bpf/tc/globals/cilium\\_cgroups\\_post\\_bind6 obj bpf\\_sock.o type sock attach\\_type post\\_bind6 sec post\\_bind6 tc exec bpf pin /sys/fs/bpf/tc/globals/cilium\\_cgroups\\_sendmsg6 obj bpf\\_sock.o type sockaddr attach\\_type sendmsg6 sec sendmsg6 tc exec bpf pin /sys/fs/bpf/tc/globals/cilium\\_cgroups\\_recvmsg6 obj bpf\\_sock.o type sockaddr attach\\_type recvmsg6 sec recvmsg6 tc exec bpf pin /sys/fs/bpf/tc/globals/cilium\\_cgroups\\_connect4 obj bpf\\_sock.o type sockaddr attach\\_type connect4 sec connect4 tc exec bpf pin /sys/fs/bpf/tc/globals/cilium\\_cgroups\\_post\\_bind4 obj bpf\\_sock.o type sock attach\\_type post\\_bind4 sec post\\_bind4 tc exec bpf pin /sys/fs/bpf/tc/globals/cilium\\_cgroups\\_sendmsg4 obj bpf\\_sock.o type sockaddr attach\\_type sendmsg4 sec sendmsg4 tc exec bpf pin /sys/fs/bpf/tc/globals/cilium\\_cgroups\\_recvmsg4 obj bpf\\_sock.o type sockaddr attach\\_type recvmsg4 sec recvmsg4 6）XDP、FLANNEL、IPSEC相关初始化暂未研究 2.3 剩余的初始化工作 1）cilium_host的datapath tc[filter replace dev cilium_host ingress prio 1 handle 1 bpf da obj 554_next/bpf_host.o sec to-host] tc[filter replace dev cilium_host egress prio 1 handle 1 bpf da obj 554_next/bpf_host.o sec from-host] 说明：加载了2 + 5 个prog，1个PROG_ARRAY map，1个cilium_policy_00554 map PROG： from-host、to-host PROG_ARRAY_MAP： cilium_calls_hostns_00554（554是epid） PROG IN PROG_ARRAY_MAP： cilium_calls_hostns_00554【1】= __send_drop_notify cilium_calls_hostns_00554【7】= tail_handle_ipv4_from_netdev =&gt; tail_handle_ipv4(ctx,false) cilium_calls_hostns_00554【15】= tail_nodeport_nat_ipv4 cilium_calls_hostns_00554【17】= tail_rev_nodeport_lb4 cilium_calls_hostns_00554【22】= tail_handle_ipv4_from_host =&gt; tail_handle_ipv4(ctx, true) 2）cilium_net的datapath tc[filter replace dev cilium_net ingress prio 1 handle 1 bpf da obj 554_next/bpf_host_cilium_net.o sec to-host] 说明：加载了1 + 5个prog，1个PROG_ARRAY map PROG： to-host PROG_ARRAY_MAP： cilium_calls_netdev_00004（4是ifindex，ip link命令可以查看） PROG IN PROG_ARRAY_MAP： cilium_calls_netdev_00004【1】= __send_drop_notify cilium_calls_netdev_00004【7】= tail_handle_ipv4_from_netdev =&gt; tail_handle_ipv4(ctx,false) cilium_calls_netdev_00004【15】= tail_nodeport_nat_ipv4 cilium_calls_netdev_00004【17】= tail_rev_nodeport_lb4 cilium_calls_netdev_00004【22】= tail_handle_ipv4_from_host =&gt; tail_handle_ipv4(ctx, true) 3）eth0的datapath tc[filter replace dev eth0 ingress prio 1 handle 1 bpf da obj 554_next/bpf_netdev_eth0.o sec from-netdev] tc[filter replace dev eth0 egress prio 1 handle 1 bpf da obj 554_next/bpf_netdev_eth0.o sec to-netdev] **说明：**加载了2+5个prog，1个PROG_ARRAY map PROG： from-netdev、to-netdev PROG_ARRAY_MAP： cilium_calls_netdev_00002（4是ifindex，ip link命令可以查看） PROG IN PROG_ARRAY_MAP： cilium_calls_netdev_00002【1】= __send_drop_notify cilium_calls_netdev_00002【7】= tail_handle_ipv4_from_netdev =&gt; tail_handle_ipv4(ctx,false) cilium_calls_netdev_00002【15】= tail_nodeport_nat_ipv4 cilium_calls_netdev_00002【17】= tail_rev_nodeport_lb4 cilium_calls_netdev_00002【22】= tail_handle_ipv4_from_host =&gt; tail_handle_ipv4(ctx, true) 4）lxc_health的datapath，跟增加一个pod的datapath是完全一样的 tc[filter replace dev lxc_health ingress prio 1 handle 1 bpf da obj 908_next/bpf_lxc.o sec from-container] 说明：加载了1+4+1个prog，1个PROG_ARRAY map，1个cilium_policy_00908 map PROG： from-container PROG IN PROG_ARRAY_MAP： cilium_calls_00908【1】= __send_drop_notify cilium_calls_00908【6】= tail_handle_arp cilium_calls_00908【15】= tail_nodeport_nat_ipv4 cilium_calls_00908【17】= tail_rev_nodeport_lb4 cilium_call_policy[908] = handle_policy(to-container好像已经废弃了) ","link":"https://rexrock.github.io/post/cilium2/"},{"title":"Cilium简介","content":"1. Cilium工作模式 官网： https://docs.cilium.io/en/v1.8/concepts/networking/routing/ Cilium也分为overlay和underlay两种工作模式： overlay，目前支持vxlan和geneve两种虚拟化网络协议； underlay，该模式下cilium需要能够对分配到其他node上的ip段进行路由，但遗憾得是，cilium既不能像flannel那样通过自身的守护进程下发路由配置，也不能像calico那样直接集成bird以提供BGP功能；所以要实现cilium路由模式的部署，我们需要自己提供BGP功能，有两种方式： 方式一：节点本身知道如何路由所有POD IP，但是网络中存在一个路由器，该路由器知道如何到达每个POD IP，每个节点需配置一条默认路由指向该路由器，该方式主要常见于云提供商的网络集成场景。 方式二：每个节点都知道所有的POD IP，并将路由插入到本地内核的路由表中。和flannel和calico一样，这需要所有节点二层互通。这需要我们自己部署BGP功能（可以通过kube-router来部署BGP功能）。 说明： Geneve（Generic Network Virtualization Encapsulation-统用网络虚拟化封装），参考：https://zhuanlan.zhihu.com/p/35790366 2. 路由模式 2.1 Cilium部署 git clone https://github.com/cilium/cilium.git #kubectl create -f cilium/install/kubernetes/quick-install.yaml 注意： cilium默认是采用vxlan方式部署的，所以我们需要先修改quick-install.yaml： --- cilium-vxlan.yaml 2020-10-26 14:36:13.449026862 +0800 +++ cilium-route.yaml 2020-10-26 14:28:08.458113851 +0800 @@ -95,7 +95,9 @@ # - disabled # - vxlan (default) # - geneve - tunnel: vxlan + tunnel: disabled + native-routing-cidr: 10.17.0.0/16 + #auto-direct-node-routes: true # Name of the cluster. Only relevant when building a mesh of clusters. cluster-name: default 官方文档中说，如果各节点二层互通，那么直接通过参数auto-direct-node-routes: true 即可实现各节点路由配置的同步和下发，环境所限并未验证。 我们看到，只部署cilium，那么该节点上是没有其他节点POD IP的路由的。这时候如果默认路由指向的网关可以提供其他节点POD IP的路由（对应underlay方式一的部署），那么整个集群POD已经可以直接互通。 2.2 kube-router部署 wget https://raw.githubusercontent.com/cloudnativelabs/kube-router/v0.4.0/daemonset/generic-kuberouter-only-advertise-routes.yaml 按照文档说明，需要先修改一些内容： root@k8s-99-12:~# diff -rNua kube-router.orig.yaml kube-router.yaml --- kube-router.orig.yaml 2020-10-26 14:45:03.716777078 +0800 +++ kube-router.yaml 2020-10-26 11:08:40.467744916 +0800 @@ -29,11 +29,9 @@ - &quot;--run-firewall=false&quot; - &quot;--run-service-proxy=false&quot; - &quot;--enable-cni=false&quot; - - &quot;--enable-ibgp=false&quot; - - &quot;--enable-overlay=false&quot; - - &quot;--peer-router-ips=&lt;CHANGE ME&gt;&quot; - - &quot;--peer-router-asns=&lt;CHANGE ME&gt;&quot; - - &quot;--cluster-asn=&lt;CHANGE ME&gt;&quot; + - &quot;--enable-pod-egress=false&quot; + - &quot;--enable-ibgp=true&quot; + - &quot;--enable-overlay=true&quot; - &quot;--advertise-cluster-ip=true&quot; - &quot;--advertise-external-ip=true&quot; - &quot;--advertise-loadbalancer-ip=true&quot; 测试环境使用ibgp即可，无需配置ebpf，所以peer-router-ips、peer-router-asns、cluster-asn无需配置。 kube-router启动后，可以看到其他节点的POD IP已经被加入本地路由。 3. VXLAN模式 Cilium默认就是以vxlan方式部署，但是cilium并不会自动读取以配置的pod-cidr，需要我们通过参数native-routing-cidr: 10.17.0.0/16自己指定。 可以看到vxlan部署后，跨界点访问都导向了cilium_host。由于cilium大量使用ebpf功能，穿透内核协议栈部分功能，所以目前没办法画出完整的流量走向图，后续持续更新。 ","link":"https://rexrock.github.io/post/cilium1/"},{"title":"Flannel和Calico简介","content":"1. Flannel Flannel是CoreOS维护的一个网络组件，Flannel为每个Pod提供全局唯一的IP，Flannel使用ETCD来存储Pod子网与Node IP之间的关系。flanneld守护进程在每台主机上运行，并负责维护ETCD信息和路由数据包。 https://github.com/coreos/flannel 1.1 Flannel部署 wget https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml # 修改net-conf.json配置，选择网络模式“ vxlan / host-gw &quot;，并配置pod-cidr(注意需要跟kubeadm创建集群时指定 # 的pod-cidr一致，这里不够只能，calico可以自动读取的) kubectl apply -f kube-flannel.yml 部署完成，可以在每个node上看到属于这个node子网 root@k8s-99-12:~# cat /var/run/flannel/subnet.env FLANNEL_NETWORK=10.17.0.0/16 FLANNEL_SUBNET=10.17.0.1/24 FLANNEL_MTU=1450 FLANNEL_IPMASQ=true 1.2 hos-gw工作模式 如何配置工作模式，上面已有介绍。这里主要看下flannel的host-gw模式时如何工作的 说明： flannel的host-gw模式，就是将每个节点都当成一个网关，部署中指定pod-cidr后，flannel会为每个node分配一个子网，并将这些node子网信息都存储到etcd中，然后每个节点上flannel守护进程会根据这些信息将其他所有节点都加到本地路由中以实现跨界点访问。下面我们分别看一下node节点上以及pod中的路由是什么样的。 pod路由 宿主机路由 限制： 1. Node需要二层互通，否则下一条转发不出去（以我们的云主机为例，云主机之间转发靠流表，因此在云主机上搭建的k8s集群，如果采用flannel的host-gw模式，跨节点访问不通的）。 1.3 vxlan模式 vxlan模式的转发路径如下图所示，flannel.1即linux的vxlan port： vxlan的原理这里不做展开，关于linux vxlan的配置及工作原理可参考： http://just4coding.com/2020/04/20/vxlan-fdb/ 分别看一下12节点和16节点的转发表： 可以看到每个节点被当成一个网关，只不过底层传输走了vxlan。 2. Calico 2.1 Calico部署 部署前确保宿主机的iptables为legacy模式： iptables --version update-alternatives --set iptables /usr/sbin/iptables-legacy update-alternatives --set ip6tables /usr/sbin/ip6tables-legacy update-alternatives --set arptables /usr/sbin/arptables-legacy update-alternatives --set ebtables /usr/sbin/ebtables-legacy 参考：https://docs.projectcalico.org/getting-started/kubernetes/self-managed-onprem/onpremises#install-calico-with-kubernetes-api-datastore-50-nodes-or-less 参考：https://blog.51cto.com/14143894/2463392 说明： 1. Calico可以自己读取集群的pod-cidr配置，无需像flannel一样去手动修改配置； 2. Calico网络模式的选择： BGP：CALICO_IPV4POOL_IPIP=&quot;Never&quot; 且 CALICO_IPV4POOL_VXLAN=”Never“ IP Tunnel: CALICO_IPV4POOL_IPIP=&quot;Always&quot; 且 CALICO_IPV4POOL_VXLAN=”Never“ VXLAN: CALICO_IPV4POOL_IPIP=&quot;Never&quot; 且 CALICO_IPV4POOL_VXLAN=”Always“ 2.2 BGP模式 类似flannel的host-gw模式，但是有两点不同： 1. 不在使用bridge，所有pod通信全部走路由，例如pod1和pod2的通信的路由如下： 2. 其他节点路由信息的添加由BGP负责，flannel则是通过自己的守护进程实现，可以看到calico-node的pod里跑了bird： 限制： 和flannel一样，BGP模式要求节点在二层互通； 最后看一下两个节点上的路由信息： 2.3 IP Tunnel模式 IP Tunnel方案相比BGP方案相比，唯一的区别时跨节点通信由原来的路由转发，改为IPIP隧道模式。好处时对节点网络没有二层互通的要求，只要节点三层可达，即可实现通信。 2.4 Vxlan模式 现在Calico也支持vxlan模式了，相比IP Tunnel模式，唯一的区别就是隧道类型变了，毕竟vxlan已经成为网络虚拟化的主流方案，和flannel一样都是使用的linux内核提供的vxlan功能。 直接看下12节点的路由和转发表好了： ","link":"https://rexrock.github.io/post/k8s-net1/"},{"title":"验证网卡PCIe带宽","content":"1. 测试拓扑 2. 配置脚本 1）添加network namespace ip netns add net1 ip netns add net2 ip netns add net3 2）将VF添加到对应netns ip link set ens4f0v2 netns net1 ip link set ens4f0v3 netns net2 ip link set ens4f1v3 netns net2 ip link set ens4f1v2 netns net3 3）为每个VF配置IP ip netns exec net1 ifconfig ens4f0v2 192.168.1.11/24 up ip netns exec net2 ifconfig ens4f0v3 192.168.1.12/24 up ip netns exec net2 ifconfig ens4f1v3 192.168.2.12/24 up ip netns exec net3 ifconfig ens4f1v2 192.168.2.11/24 up 4）为netns添加路由 ip netns exec net1 route add -net 192.168.2.0/24 gw 192.168.1.12 dev ens4f0v2 ip netns exec net3 route add -net 192.168.1.0/24 gw 192.168.2.12 dev ens4f1v2 注意：路由需要指定网关，否则需要手动设置arp； 5）设置VF的vlan ip link set ens4f0 vf 2 vlan 1002 ip link set ens4f0 vf 3 vlan 1002 ip link set ens4f1 vf 3 vlan 1003 ip link set ens4f1 vf 2 vlan 1003 3. 带宽测试 测试方法1： ip netns exec net2 iperf -s -i1 ip netns exec net1 iperf -c 192.168.1.12 -i1 -t6000 -P4 测试方法2： ip netns exec net3 iperf -s -i1 ip netns exec net1 iperf -c 192.168.2.11 -i1 -t6000 -P4 ","link":"https://rexrock.github.io/post/nic1/"},{"title":"RDMA编程模型","content":"1. 连接管理 通过rdmacm实现连接管理： Socket既管理连接又管理传输； RDMACM只管理连接，数据传输下面再介绍； 2. 数据传输 RDMA的三种数据传输模式： 双边操作—Send/Receive，收发端都参与才能完成； 单边操作—Write，提前明确两边的收发地址，数据传输过程，无需接收端参与； 单边操作—Read，提前明确两边的收发地址，数据传输过程，无需发送端参与； 基于socket的通信不用关心物理设备，但是RDMA通信必须要跟RDMA设备绑定！！！ 创建socket的时候，不管管最终从哪个设备出去，也不用做任何跟绑定设备相关的操作； 创建qp的时候，必须指定在哪个RDMA设备上创建qp； 跟哪个RDMA设备绑定 如果不使用rdmacm，需要用户自己根据IP地址来寻找跟哪个RDMA设备绑定，或者干脆手动指定；然后打开设备：struct ibv_context *ctx = ibv_open_device () 如果使用了rdmacm，则通过rdma_resolve_addr自动找到要绑定的设备，并返回struct ibv_context *ctx; 2.1 创建数据传输需要的QP、CQ 申请保护域PD：ibv_alloc_pd() 创建CQ: 创建cq_event_channel：ibv_create_comp_channel() 创建cq：ibv_create_cq() 指定cq的通知机制：ibv_req_notify_cq(),这样的话当一个cqe(completion queue entry )被放到cq中时，会产生一个completion event ，被放到event channel中。然后我们通过ibv_get_cq_event（阻塞）得到event，再调用ibv_poll_cq获取cqe。否则指定通过ibv_poll_cq（非阻塞）不停的轮询。 创建QP并指定其CQ、PD、ibver_ctx： rdma_create_qp() 注册将要参与数据传输的内存：ibv_reg_mr（）,返回这块内存的key，对端有了key才能访问这块内存； 2.2 创建QP的契机 图中标★处： 2.3 传输之send/receive 2.5 传输之write/read 前提，被read/write的一端(称接收端)，需要把自己要被read/write的内存，发给发起read/write的一端（称发送端） 发送哪些内容？ addr，rkey：ibv_reg_mr，返回两个key，lkey即本端网卡操作这端内存的key，rkey即对端网卡操作这段内存的key。 怎么把addr、rkey发送给对端？ 随便：可以通过上面讲的send/receive，或者干脆通过TCP/IP传输这种数据； 2.6 传输之write_with_immediate 前提，被read/write的一端(称接收端)，需要把自己要被read/write的内存，发给发起read/write的一端（称发送端） 3. 总结 前面为什么说RDMA的连接管理合数据传输时分开的？ Rdma_cm_event管理连接状态； Cq管理数据传输的状态； ","link":"https://rexrock.github.io/post/rdma1/"},{"title":"openstack环境搭建","content":"1. Mysql安装 参考：https://cloud.tencent.com/developer/article/1329000 登录mysql： # mysql -u root -p # show databases; # use mysql; # show tables; 2. Keystone安装 2.1 选取openstack版本 使用yum search查看支持如下openstack发行版本： centos-release-openstack-ocata.noarch centos-release-openstack-pike.x86_64 centos-release-openstack-queens.noarch centos-release-openstack-rocky.noarch centos-release-openstack-stein.noarch 首先处于兼容性考虑选取了最老的版本ocata，安装后总是提示： ArgsAlreadyParsedError: arguments already parsed: cannot register CLI option 解决无果，google上说是老版本的patch，所以决定换个新版本再试试； Stein、rocky、queens三者安装失败，只能选择pike版本了： # yum install centos-release-openstack-pike # yum update 2.2 配置keystone数据库 # CREATE DATABASE keystone; # GRANT ALL PRIVILEGES ON keystone.* TO 'keystone'@'localhost' IDENTIFIED BY 'keystone_Passw0rd'; # GRANT ALL PRIVILEGES ON keystone.* TO 'keystone'@'%' IDENTIFIED BY 'keystone_Passw0rd'; 2.3 安装keystone # yum install python-openstackclient openstack-keystone httpd mod_wsgi 2.4 标记keystone配置文件 # vi /etc/keystone/keystone.conf [database] connection = mysql://keystone:keystone_Passw0rd@172.24.10.2/keystone [token] provider = fernet 2.5 初始化 主要是再数据库中创建keystone服务所需要的表项，后面其他服务都是一样的道理。 # su -s /bin/sh -c &quot;keystone-manage db_sync&quot; keystone 初始化fernet密钥库 # keystone-manage fernet_setup --keystone-user keystone --keystone-group keystone # keystone-manage credential_setup --keystone-user keystone --keystone-group keystone 引导服务 # keystone-manage bootstrap --bootstrap-password rootPassw0rd## --bootstrap-admin-url http://172.24.10.2:35357/v3/ --bootstrap-internal-url http://172.24.10.2:5000/v3/ --bootstrap-public-url http://172.24.10.2:5000/v3/ --bootstrap-region-id RegionOne 2.6 配置apache http server 编辑/etc/httpd/conf/httpd.conf ServerName 172.10.24.2:80 创建链接： # ln -s /usr/share/keystone/wsgi-keystone.conf /etc/httpd/conf.d/ 2.7 启动服务并验证 # systemctl start httpd.service export OS_USERNAME=admin export OS_PASSWORD=rootPassw0rd## export OS_PROJECT_NAME=admin export OS_USER_DOMAIN_NAME=Default export OS_PROJECT_DOMAIN_NAME=Default export OS_AUTH_URL=http://172.24.10.2:35357/ export OS_IDENTITY_API_VERSION=3 export OS_TENANT_NAME=admin 运行如下命令进行验证 # openstack user list ","link":"https://rexrock.github.io/post/openstack1/"},{"title":"深入浅出vhostuser传输模型","content":"1. virtio的ring结构 Virtio设备是支持多队列，每个队列由结构vring_virtqueue定义（可以是收包队列也可以是发包队列），而每个vring_virtqueue中都定义了一个vring结构，负责具体的数据传输。 // include/uapi/linux/virtio_ring.h struct vring { unsigned int num; struct vring_desc *desc; struct vring_avail *avail; struct vring_used *used; }; 可见，ving不是一个ring环，而是包含了三个ring环，利用着三个ring环实现报文的收发。我们通过一张图来描述三个ring环的作用及关系： 1. vring_desc Struct vring_desc并没有定义一个ring环，而是定义了ring环中每个元素的结构。上图中已经对vring_desc各成员做了注解。Desc ring没有消费者和生产者，我们可以把它看作一块用来交互数据的共享内存。 说明：vring_desc结构中的addr成员，在Guest向外发包的场景中，指向的是一块承载了发包数据的内存，而在Guest从外面收包的场景中，指向的是一块预分配好的空内存，Host会将收到的包存放到这块空内存中。 2. vring_avail Struct vring_avail是定义了一个ring环的（即成员ring[]），这个ring环的生产者是Guest中的virtio-net，消费者是Host中vhostuser/vhostnet。Avail ring环中每个元素即指向desc ring的下标。 说明：Avail ring和desc ring的长度都是一样的，但是avail ring并不会指向desc ring的每一个desc。例如有些skb是由多个分片组成的（scattergather），那么这个skb实际会被转换成多个desc，并且通过vring_desc中的next将多个desc链接在一起，最后一个desc通过flag标记结束。那么这种情况下，Avail ring只会存储第一个desc的下标，同时vring_avail的idx也只累加1。 3. vring_used Struct vring_used跟vring_avail类似，不过used ring的生产者是vhostnet/vhostuser，消费者是virtio-net。 说明：used ring中的每个元素包含两个成员id和len，id指向desc ring中的下标，而len则指向desc中所存储数据的长度(通常len成员只在Guest从外面收包的场景中才有效，这个时候desc中len指的是内存中可以最大存储的数据的长度，而user ring中的len指的则是内存中实际存储的数据的长度）。 那么这三个ring在内存中是怎么分布的呢？我们通过一张图描述下： 如图，三个ring是分布在一块连续的内存中的（物理/虚拟地址都是连续的）。最前面是desc ring，接下来是avail ring，最后是used ring。 2. 将vring映射到vhostuser Virtio队列中的vring是由Guest中的virtio-net驱动申请的，那么vhostuser如何操作这些vring呢？答案是virtio-net在申请好vring后需要将vring的地址告诉vhostuser。我们通过一张图，看一下虚拟机启动时所涉及到的内存注册过程： 如上图所示，整个内存注册过程分为三个步骤： 第一步： QEMU未虚拟机申请内存，并将虚拟机的整个内存注册到vhostuser。你没看错，确实是需要将虚拟机的整个内存都注册到vhostuser驱动中。 说明：Vhostuser和QEMU通过unix socket建立了通信连接，两者通过该连接进行协商。 第二步： Guest中的virtio-net驱动申请队列（即virtqueue），并将队列中的vring地址同步给QEMU。 // 追踪从virtio-net开始初始化到创建virtqueue，函数位置：linux-kernel-src/drivers/virtio/ |virtio_pci_probe | |virtio_pci_legacy_probe / virtio_pci_modern_probe | | |setup_vq | | | |vring_create_virtqueue | | | | |vring_create_virtqueue_split | | | | | |void *queue = vring_alloc_queue // 申请vring的地址 | | | | | |vring_init(struct vring *, queue) | | | | | |__vring_create_virtqueue | | | |iowrite32(VIRTIO_PCI_QUEUE_PFN) // 将vring_addr注册到QEMU 说明：Virtio-net和QEMU之间的通信不是通过什么scoket，而是由virtio-net向一段特定的io空间写数据实现的。不单单QEMU是这样做的，包括VMWARE也是这么做的（XEN不熟悉）。同理，QEMU向GUEST发起的数据请求也都是都通过IO实现的。 第三步： QEMU在enable每个virtqueu的时候，会将virtqueue中三个vring的长度及地址注册到vhostuser。并且初始化三个vring中消费者/生产者的位置。 // vhostuser中相关协商处理函数 static vhost_message_handler_t vhost_message_handlers[VHOST_USER_MAX] = { ...... [VHOST_USER_SET_VRING_NUM] = vhost_user_set_vring_num, [VHOST_USER_SET_VRING_ADDR] = vhost_user_set_vring_addr, [VHOST_USER_SET_VRING_BASE] = vhost_user_set_vring_base, ...... }; 3. Guest向外发包 // 函数位置：linux-kernel-src/drivers/net/virtio-net.c |start_xmit | |free_old_xmit_skbs // 每次发包前，先清理上一次已成功发送的包 | |xmit_skb | | |virtqueue_add_outbuf | | | |virtqueue_add | | | | |virtqueue_add_split 这里面virtqueue_add()是一个通用的函数，不管收包还是发包，都是通过调用virtqueue_add()函数实现： static inline int virtqueue_add(struct virtqueue *_vq, struct scatterlist *sgs[], unsigned int total_sg, unsigned int out_sgs, unsigned int in_sgs, void *data, void *ctx, gfp_t gfp)； 参数解析： _vq，没什么好解释的，virtqueue被包含在vring_virtqueue中，几乎跟vring传输相关的所有内容都定义在vring_virtqueue中； sgs，元素为scatterlist的列表；这里需要额外注意，每个scatterlist本身也是一个列表；举个例子，一个skb可以由多个分片构成，多个分片内存上是不连续的，在没有scatter-gather之前或者禁用scatter-gather的情况下，驱动需要将所有分片拷贝到一块连续的内存上，而开启scatter-gather后，我们不必再重新拷贝报文分片，直接通过scattherlist将报文的多个分片串联起来，供网卡驱动使用。可以说scatterlist是skb在网卡驱动中的表示； total_sg，所有scatterlist中分片加起来的总数，每个分片都占用一个独立的desc，所以total_sg表明接下来要消耗的desc总数； out_sgs，sgs中有多少是out_sg； 【说明】：scatterlist是分为out_sg（只读）和in_sg（可读可写）两种类型的。当Guest发送报文的时候，使用out_sg，当Guest打算收包，需要先将可承载报文数据的内存通过desc ring传递到vhost的时候，就使用in_sg。此外需要注意，我们发包的时候，只会传递out_sg给virtqueue_add()，收包的时候只传递in_sg给virtqueue_add()，还有一种通过virtqueue进行前后端协商和管理的virtqueue，会同时传递out_sg和in_sg给virtqueue_add（）。 int_sgs，sgs中有多少是in_sg； data，要传输的内存起始地址； 【说明】：在发包场景中，就是要发送的skb的地址，注意是虚拟地址，而我们赋值给desc-&gt;addr是物理地址，那么这个data有啥用呢？用处就是这个报文被vhost成功处理发送后，virtio-net会通过used ring再次获取到已经被成功发送的报文，这个时候virtio-net需要释放报文，那么直接引用这个data指向的虚拟地址释放就可以了。 【说明】：在收包场景中类似，virtio-net填充预申请的空白内存给vhostuser收包，收到的报文会通过used ring再送回到virtio-net中，这个时候直接引用data即可对内存中的报文数据进行操作了。 【说明】：那么data存储再哪呢？下面代码解析里有介绍。 ctx，跟indirect相关，暂时不管； gfp，跟indirect相关，暂时不管； virtqueue_add_split函数源码分析： 说明：packed queus是virtio 1.1引入的新特性，我们暂时不管，先分析老的split模式。 static inline int virtqueue_add_split(struct virtqueue *_vq, struct scatterlist *sgs[], unsigned int total_sg, unsigned int out_sgs, unsigned int in_sgs, void *data, void *ctx, gfp_t gfp) { ...... } else { // 非indirect模式 indirect = false; desc = vq-&gt;split.vring.desc; i = head; descs_used = total_sg; } ...... // 如果desc ring没有空间了，赶紧通知vhost处理报文好腾地方 if (vq-&gt;vq.num_free &lt; descs_used) { pr_debug(&quot;Can't add buf len %i - avail = %i\\n&quot;, descs_used, vq-&gt;vq.num_free); /* FIXME: for historical reasons, we force a notify here if * there are outgoing parts to the buffer. Presumably the * host should service the ring ASAP. */ if (out_sgs) vq-&gt;notify(&amp;vq-&gt;vq); if (indirect) kfree(desc); END_USE(vq); return -ENOSPC; } ...... // ************************************************************************* // 第一步，填充desc ring // 本函数最核心的代码了，out_sg和in_sg的存放位置也是有讲究的，当同时又两种scatterlist时， // out_sg总是被放在前面，in_sg被存储在out_sg后面； for (n = 0; n &lt; out_sgs; n++) { for (sg = sgs[n]; sg; sg = sg_next(sg)) { // 这里需要注意的是，通过desc-&gt;addr传递给vhost的是Guest的物理地址 dma_addr_t addr = vring_map_one_sg(vq, sg, DMA_TO_DEVICE); if (vring_mapping_error(vq, addr)) goto unmap_release; desc[i].flags = cpu_to_virtio16(_vq-&gt;vdev, VRING_DESC_F_NEXT); desc[i].addr = cpu_to_virtio64(_vq-&gt;vdev, addr); desc[i].len = cpu_to_virtio32(_vq-&gt;vdev, sg-&gt;length); prev = i; i = virtio16_to_cpu(_vq-&gt;vdev, desc[i].next); } } for (; n &lt; (out_sgs + in_sgs); n++) { for (sg = sgs[n]; sg; sg = sg_next(sg)) { dma_addr_t addr = vring_map_one_sg(vq, sg, DMA_FROM_DEVICE); if (vring_mapping_error(vq, addr)) goto unmap_release; desc[i].flags = cpu_to_virtio16(_vq-&gt;vdev, VRING_DESC_F_NEXT | VRING_DESC_F_WRITE); desc[i].addr = cpu_to_virtio64(_vq-&gt;vdev, addr); desc[i].len = cpu_to_virtio32(_vq-&gt;vdev, sg-&gt;length); prev = i; i = virtio16_to_cpu(_vq-&gt;vdev, desc[i].next); } } /* Last one doesn't continue. */ // OK，对于发包场景，上面所有desc都是一个SKB的，现在这个SKB填充完毕，需要通过flag标记 // desc的结束，前面介绍desc ring的时候介绍过，所有desc通过next成员链在一起，并且通过flag // 标记一个报文存储的结束。 desc[prev].flags &amp;= cpu_to_virtio16(_vq-&gt;vdev, ~VRING_DESC_F_NEXT); /* We're using some buffers from the free list. */ // 用了多少，得从num_free中减掉 vq-&gt;vq.num_free -= descs_used; /* Update free pointer */ if (indirect) ...... else // 更新下一次开始填充的desc下标 vq-&gt;free_head = i; ...... // vring_virtqueue又自己维护了一个跟desc ring长度相同的数组，专门用来存储对应desc中内存 // 对应的虚拟地址 vq-&gt;split.desc_state[head].data = data; ...... /* Put entry in available array (but don't update avail-&gt;idx until they * do sync). */ // ************************************************************************* // 第二步，填充avail ring // 上面是desc ring的填充，下main开始填充avail ring了，可以看到只需要将第一个desc // 填充到avail ring即可 avail = vq-&gt;split.avail_idx_shadow &amp; (vq-&gt;split.vring.num - 1); vq-&gt;split.vring.avail-&gt;ring[avail] = cpu_to_virtio16(_vq-&gt;vdev, head); /* Descriptors and available array need to be set before we expose the * new available array entries. */ // 累加avail ring的生产者计数 virtio_wmb(vq-&gt;weak_barriers); vq-&gt;split.avail_idx_shadow++; vq-&gt;split.vring.avail-&gt;idx = cpu_to_virtio16(_vq-&gt;vdev, vq-&gt;split.avail_idx_shadow); // ************************************************************************* // num_added主要跟通知机制有关，下面章节详细介绍 vq-&gt;num_added++; pr_debug(&quot;Added buffer head %i to %p\\n&quot;, head, vq); END_USE(vq); /* This is very unlikely, but theoretically possible. Kick * just in case. */ if (unlikely(vq-&gt;num_added == (1 &lt;&lt; 16) - 1)) virtqueue_kick(_vq); ...... 4. Guest从外面收包 |virtnet_poll() | |virtnet_receive() | | |virtqueue_get_buf() | | | |detach_buf() | | |receive_buf() | | |try_fill_recv() | | | |add_recebuf_xxx() | | | | |virtqueue_add_xxx() | | | | | |virtqueue_add() | | | |virqueue_kick() 我们从virtqueue_get_buf()函数开始看。该函数执行的是收包函数的第一步，还是以split模式为例，该函数会根据模式选择最终调用到virtqueue_get_buf_ctx_split()函数： static void *virtqueue_get_buf_ctx_split(struct virtqueue *_vq, unsigned int *len, void **ctx) { // 注意：该函数每次只收一个包 ...... // 这一步先判断下used ring里有没有未处理的成员。贴一下more_used_split（）的代码： // return vq-&gt;last_used_idx != // virtio16_to_cpu(vq-&gt;vq.vdev, vq-&gt;split.vring.used-&gt;idx); // *************************************************************************** // 这里需要说明的是，vring_virtqueue中定义了一个成员叫last_used_idx，last_used_idx是 // virtio-net消费used ring的下标+1，也就是这一次将从last_used_idx这个位置开始消费used // ring。而vring_used中的idx则是由生产者（也就是vhost）填充的，表示下一次将要填充的used // ring的下标。 // *************************************************************************** // 说明：Vring_avail和vring_used中的idx都是生产者填充的，而消费者都会在各自的virtqueue的 // 结构中定义一个last_xxx_idx，表示上次消费的截至位置，以及下一次开始消费的位置。 if (!more_used_split(vq)) { pr_debug(&quot;No more buffers in queue\\n&quot;); END_USE(vq); return NULL; } /* Only get used array entries after they have been exposed by host. */ virtio_rmb(vq-&gt;weak_barriers); // 获取要消费的used ring的下标 last_used = (vq-&gt;last_used_idx &amp; (vq-&gt;split.vring.num - 1)); // 从used成员中获取指向的desc ring中的下标 i = virtio32_to_cpu(_vq-&gt;vdev, vq-&gt;split.vring.used-&gt;ring[last_used].id); // 获取这个报文的实际长度 // 注意：这个报文可能是由多个desc构成的，下面的len是指所有desc中报文的总长度，并且报文的存 // 储总是前面desc满了之后，再向下一个desc中存储数据。 *len = virtio32_to_cpu(_vq-&gt;vdev, vq-&gt;split.vring.used-&gt;ring[last_used].len); // 如果这个desc ring的下标超过数组长度，则发生错误。 // *************************************************************************** // 特别注意： // 细心的同学可能已经发现，avail ring和used ring的生产者/消费者下标是不断累加的，然后使用 // 的时候做一下“idx&amp;(vring_num-1)”的操作来保证访问不越界。但是我们使用desc ring的下标并不 // 是不断累加的，而是每次通过desc的next成员获取到的（观察上面virtqueue_add函数得分析）。所 // 以我们从avail ring和used ring中获取得desc下标是直接得下标，不存在越界。 if (unlikely(i &gt;= vq-&gt;split.vring.num)) { BAD_RING(vq, &quot;id %u out of range\\n&quot;, i); return NULL; } // *************************************************************************** // 这个data前面介绍过了 if (unlikely(!vq-&gt;split.desc_state[i].data)) { BAD_RING(vq, &quot;id %u is not a head!\\n&quot;, i); return NULL; } /* detach_buf_split clears data, so grab it now. */ ret = vq-&gt;split.desc_state[i].data; // OK，报文已成功提取，释放掉这个desc，如果占用了多个desc，会在detach_buf_split中一起 // 释放（通过flag标记结束）。 detach_buf_split(vq, i, ctx); // 累加消费者下标 vq-&gt;last_used_idx++; /* If we expect an interrupt for the next entry, tell host * by writing event index and flush out the write before * the read in the next get_buf call. */ if (!(vq-&gt;split.avail_flags_shadow &amp; VRING_AVAIL_F_NO_INTERRUPT)) virtio_store_mb(vq-&gt;weak_barriers, &amp;vring_used_event(&amp;vq-&gt;split.vring), cpu_to_virtio16(_vq-&gt;vdev, vq-&gt;last_used_idx)); LAST_ADD_TIME_INVALID(vq); END_USE(vq); // 返回指向报文的虚拟机地址 return ret; 5. Vhost从Guest收包 我们选择DPDK-20.11的代码进行分析，因为这个版本vhostuser的收包代码非场简洁。在DPDK-20.08之前，vhostuser驱动支持zerocopy功能，但是在DPDK-20.08中zerocopy被移除了。因为zerocopy虽然带来了性能的提升，却让代码变得复杂且难以维护，同时zerocopy在VPC场景存在使用限制，复杂的代码也给virtio一些新功能添加也带来的阻碍，种种因素导致zerocopy最终被社区抛弃。今后virtio性能优化的方向主要时通过硬件的方式进行，例如通过CPU的CBDMA引擎加速拷贝，或者通过支持virtio offload的网卡进行卸载加速。 |rte_vhost_dequeue_burst() | |virtio_dev_tx_split() | | |for() // 处理所有报文（最多不超过32个，可配） | | | |fill_vec_buf_split() | | | | |while() // 处理该报文下所有的desc（通过desc.next串起的list） | | | | | |map_one_desc() | | | | | | |vhost_iova_to_vva() | | | | | | | |rte_vhost_va_from_guest_pa() | | | |copy_desc_to_mbuf() 继续贯彻深入浅出原则，咱们先看rte_vhost_va_from_guest_pa()函数，该函数主要实现将desc-&gt;addr这个Guest的物理地址（后面简称GPA）转换成DPDK进程中可以直接访问虚拟地址（后面简称VVA，虽然通常大家喜欢称之为HVA，但是我们跟着DPDK里面定义的VVA叫吧，大家知道怎么回事就行了）。 特别介绍： 在分析rte_vhost_va_from_guest_pa()函数之前，有必要先介绍一下rte_vhost_memory和rte_vhost_mem_region 这2个结构，前面第2节曾提到，VM虚拟机启动的时候的QEMU会将虚拟机整个内存都注册到vhostuser驱动中，那么虚拟机的内存信息存储在哪呢？答案就是由rte_vhost_memory结构负责存储： // 每个rte_vhost_mem_region对应一个page struct rte_vhost_memory { // region个数 uint32_t nregions; // region数组 struct rte_vhost_mem_region regions[]; }; struct rte_vhost_mem_region { // 就是这个region在Guest中的物理地址 uint64_t guest_phys_addr; // 主要在QEMU把vring注册过来的时候用到，Guest中的虚拟地址？TODO uint64_t guest_user_addr; // region映射到DPDK进程后的虚拟地址 uint64_t host_user_addr; // region的长度 uint64_t size; void *mmap_addr; uint64_t mmap_size; int fd; }; 我们再来分析rte_vhost_va_from_guest_pa()函数： __rte_experimental static __rte_always_inline uint64_t rte_vhost_va_from_guest_pa(struct rte_vhost_memory *mem, uint64_t gpa, uint64_t *len) { struct rte_vhost_mem_region *r; uint32_t i; // 其实就是拿报文的gpa在vhostuser维护的mem_regions中逐个对比，看属于 // 哪个page，然后报文在vhostuser中的vva = page-&gt;vva + （gpa - page-&gt;gpa） for (i = 0; i &lt; mem-&gt;nregions; i++) { r = &amp;mem-&gt;regions[i]; if (gpa &gt;= r-&gt;guest_phys_addr &amp;&amp; gpa &lt; r-&gt;guest_phys_addr + r-&gt;size) { if (unlikely(*len &gt; r-&gt;guest_phys_addr + r-&gt;size - gpa)) *len = r-&gt;guest_phys_addr + r-&gt;size - gpa; return gpa - r-&gt;guest_phys_addr + r-&gt;host_user_addr; } } *len = 0; return 0; } vhost_iova_to_vva()是个封装函数，我们不用管。直接看map_one_desc()函数： static __rte_always_inline int map_one_desc(struct virtio_net *dev, struct vhost_virtqueue *vq, struct buf_vector *buf_vec, uint16_t *vec_idx, uint64_t desc_iova, uint64_t desc_len, uint8_t perm) { uint16_t vec_id = *vec_idx; // 这里为什么有个循环处理？要知道map_one_desc()这个函数只处理一个desc， // 也就是只处理当前的desc，不用管desc.next。答案是：因为desc-&gt;addr有可能 // 是跨page的，所以需要多次地址转换，特别是开启tso的情况下。 while (desc_len) { uint64_t desc_addr; uint64_t desc_chunck_len = desc_len; if (unlikely(vec_id &gt;= BUF_VECTOR_MAX)) return -1; // 地址转换：GPA =&gt; VVA desc_addr = vhost_iova_to_vva(dev, vq, desc_iova, &amp;desc_chunck_len, perm); if (unlikely(!desc_addr)) return -1; rte_prefetch0((void *)(uintptr_t)desc_addr); // 这个函数将desc转换后，存储在buf_vec中，然后再上层函数统一处理 buf_vec[vec_id].buf_iova = desc_iova; buf_vec[vec_id].buf_addr = desc_addr; buf_vec[vec_id].buf_len = desc_chunck_len; desc_len -= desc_chunck_len; desc_iova += desc_chunck_len; vec_id++; } *vec_idx = vec_id; return 0; } 接着看fill_vec_buf_split()函数： static __rte_always_inline int fill_vec_buf_split(struct virtio_net *dev, struct vhost_virtqueue *vq, uint32_t avail_idx, uint16_t *vec_idx, struct buf_vector *buf_vec, uint16_t *desc_chain_head, uint32_t *desc_chain_len, uint8_t perm) { // 获取desc ring中的下标 uint16_t idx = vq-&gt;avail-&gt;ring[avail_idx &amp; (vq-&gt;size - 1)]; uint16_t vec_id = *vec_idx; uint32_t len = 0; uint64_t dlen; uint32_t nr_descs = vq-&gt;size; uint32_t cnt = 0; struct vring_desc *descs = vq-&gt;desc; struct vring_desc *idesc = NULL; // 上文提到过，desc ring中下标是不会超过数组长度的，因为其值来自desc.next if (unlikely(idx &gt;= vq-&gt;size)) return -1; *desc_chain_head = idx; if (vq-&gt;desc[idx].flags &amp; VRING_DESC_F_INDIRECT) { ...... } while (1) { ...... len += descs[idx].len; // 为一个desc转换地址 if (unlikely(map_one_desc(dev, vq, buf_vec, &amp;vec_id, descs[idx].addr, descs[idx].len, perm))) { free_ind_table(idesc); return -1; } // 判断desc list是否截止 if ((descs[idx].flags &amp; VRING_DESC_F_NEXT) == 0) break; // 处理该报文的下一个desc idx = descs[idx].next; } // 报文总长度 *desc_chain_len = len; // vsec总个数 // 注意：desc是可以跨page的，但是用于接收的desc_vec是不跨page的 // 所以desc_vec中的元素的个数有可能回避desc的个数多。 *vec_idx = vec_id; if (unlikely(!!idesc)) free_ind_table(idesc); return 0; } copy_desc_to_mbuf()这个函数不想太详细的看了，改函数主要就是将buf_vec中的数据拷贝到mbuf中。并且根据virtio_hdr初始化mbuf相关参数（例如offload相关参数等）。 6. Vhost向Guest发包 7. Virtio的前后端通知机制 ","link":"https://rexrock.github.io/post/vhu1/"},{"title":"使用testpmd验证CX5网卡rte_flow功能","content":"1. 测试环境说明 网卡：Mellanox cx4/cx5 驱动：OFED 4.3/4.5/5.3 DPDK： 18.05、18.11 拓扑： 2. 初始化配置网络 由于是使用VF测试，所以不依赖外部环境，也不依赖物理网口的link状态，只要你有CX4/CX5的网卡就行： # 修改网卡名称 netdev=eth2 # 创建VF echo 4 &gt; /sys/class/net/${netdev}/device/sriov_numvfs # 获取VF-iD vf_num=`cat /sys/class/net/${netdev}/device/sriov_numvfs` vf1_idx=$((vf_num-4)) vf2_idx=$((vf_num-3)) vf3_idx=$((vf_num-2)) vf4_idx=$((vf_num-1)) # 获取每个VF对应的以太网的网口名称 vf1_dev=`ls /sys/class/net/${netdev}/device/virtfn${vf1_idx}/net/` vf2_dev=`ls /sys/class/net/${netdev}/device/virtfn${vf2_idx}/net/` vf3_dev=`ls /sys/class/net/${netdev}/device/virtfn${vf3_idx}/net/` vf4_dev=`ls /sys/class/net/${netdev}/device/virtfn${vf4_idx}/net/` # 获取每个VF对应的PCI vf1_pci=`ls /sys/class/net/${netdev}/device/virtfn${vf1_idx} -l | awk -F '/' '{print $NF}'` vf2_pci=`ls /sys/class/net/${netdev}/device/virtfn${vf2_idx} -l | awk -F '/' '{print $NF}'` vf3_pci=`ls /sys/class/net/${netdev}/device/virtfn${vf3_idx} -l | awk -F '/' '{print $NF}'` vf4_pci=`ls /sys/class/net/${netdev}/device/virtfn${vf4_idx} -l | awk -F '/' '{print $NF}'` # 列出上面获取到的数据 echo &quot; &gt;&gt; init netdev&quot; echo &quot; $netdev vf $vf1_idx : $vf1_pci : $vf1_dev&quot; echo &quot; $netdev vf $vf2_idx : $vf2_pci : $vf2_dev&quot; echo &quot; $netdev vf $vf3_idx : $vf3_pci : $vf3_dev&quot; echo &quot; $netdev vf $vf4_idx : $vf4_pci : $vf4_dev&quot; # 设置VF link state为enable，这样不管物理网口link状态，VF总是UP的 ip link set $netdev vf $vf1_idx state enable ip link set $netdev vf $vf2_idx state enable ip link set $netdev vf $vf3_idx state enable ip link set $netdev vf $vf4_idx state enable # 设置VF1的vlan、mac、限速 ip link set ${netdev} vf ${vf1_idx} vlan 3001 ip link set ${netdev} vf ${vf1_idx} mac fa:65:a1:a6:75:01 ip link set ${netdev} vf ${vf1_idx} rate 1000 echo -n &quot;${vf1_pci}&quot; &gt; /sys/bus/pci/drivers/mlx5_core/unbind echo -n &quot;${vf1_pci}&quot; &gt; /sys/bus/pci/drivers/mlx5_core/bind # 设置VF2的vlan、mac、限速 ip link set ${netdev} vf ${vf2_idx} vlan 0 ip link set ${netdev} vf ${vf2_idx} mac fa:65:a1:a6:75:02 ip link set ${netdev} vf ${vf2_idx} rate 1000 echo -n &quot;${vf2_pci}&quot; &gt; /sys/bus/pci/drivers/mlx5_core/unbind echo -n &quot;${vf2_pci}&quot; &gt; /sys/bus/pci/drivers/mlx5_core/bind # 设置VF3的vlan、mac、限速 ip link set ${netdev} vf ${vf3_idx} vlan 0 ip link set ${netdev} vf ${vf3_idx} mac fa:65:a1:a6:75:03 ip link set ${netdev} vf ${vf3_idx} rate 1000 echo -n &quot;${vf3_pci}&quot; &gt; /sys/bus/pci/drivers/mlx5_core/unbind echo -n &quot;${vf3_pci}&quot; &gt; /sys/bus/pci/drivers/mlx5_core/bind # 设置VF4的vlan、mac、限速 ip link set ${netdev} vf ${vf4_idx} vlan 3333 ip link set ${netdev} vf ${vf4_idx} mac fa:65:a1:a6:75:04 ip link set ${netdev} vf ${vf4_idx} rate 1000 echo -n &quot;${vf4_pci}&quot; &gt; /sys/bus/pci/drivers/mlx5_core/unbind echo -n &quot;${vf4_pci}&quot; &gt; /sys/bus/pci/drivers/mlx5_core/bind # 创建network namespace，并将对应的VF添加到netns中并设置ip、srp等 ip netns add net1 ip link set $vf1_dev netns net1 ip netns exec net1 ifconfig $vf1_dev 1.1.1.1/24 up ip netns exec net1 arp -s 1.1.1.2 fa:65:a1:a6:75:02 ip netns add net2 ip link set $vf4_dev netns net2 ip netns exec net2 ifconfig $vf4_dev 1.1.1.2/24 up ip netns exec net2 arp -s 1.1.1.1 fa:65:a1:a6:75:03 3. 编译testpmd 编译前先看下需要修改的地方 diff --git a/app/test-pmd/Makefile b/app/test-pmd/Makefile index d5258eae4..0e12d9189 100644 --- a/app/test-pmd/Makefile +++ b/app/test-pmd/Makefile @@ -70,6 +70,8 @@ ifeq ($(CONFIG_RTE_LIBRTE_PMD_SOFTNIC),y) LDLIBS += -lrte_pmd_softnic endif +LDLIBS += -lrte_pmd_mlx5 + endif include $(RTE_SDK)/mk/rte.app.mk diff --git a/app/test-pmd/config.c b/app/test-pmd/config.c index b9e5dd923..d8d66deac 100644 --- a/app/test-pmd/config.c +++ b/app/test-pmd/config.c @@ -2976,7 +2976,7 @@ tx_vlan_set(portid_t port_id, uint16_t vlan_id) } tx_vlan_reset(port_id); - ports[port_id].dev_conf.txmode.offloads |= DEV_TX_OFFLOAD_VLAN_INSERT; + //ports[port_id].dev_conf.txmode.offloads |= DEV_TX_OFFLOAD_VLAN_INSERT; ports[port_id].tx_vlan_id = vlan_id; } diff --git a/app/test-pmd/macfwd.c b/app/test-pmd/macfwd.c index 7cac757a0..13fd8ea6d 100644 --- a/app/test-pmd/macfwd.c +++ b/app/test-pmd/macfwd.c @@ -91,6 +91,7 @@ pkt_burst_mac_forward(struct fwd_stream *fs) rte_prefetch0(rte_pktmbuf_mtod(pkts_burst[i + 1], void *)); mb = pkts_burst[i]; + rte_vlan_strip(mb); eth_hdr = rte_pktmbuf_mtod(mb, struct ether_hdr *); ether_addr_copy(&amp;peer_eth_addrs[fs-&gt;peer_addr], &amp;eth_hdr-&gt;d_addr); @@ -102,6 +103,7 @@ pkt_burst_mac_forward(struct fwd_stream *fs) mb-&gt;l3_len = sizeof(struct ipv4_hdr); mb-&gt;vlan_tci = txp-&gt;tx_vlan_id; mb-&gt;vlan_tci_outer = txp-&gt;tx_vlan_id_outer; + rte_vlan_insert(&amp;mb); } nb_tx = rte_eth_tx_burst(fs-&gt;tx_port, fs-&gt;tx_queue, pkts_burst, nb_rx); /* 编译脚本： #export EXTRA_CFLAGS='-O3 -fno-strict-aliasing' export EXTRA_CFLAGS='-O0 -g' #taskset -c 0-32 make config T=x86_64-native-linuxapp-gcc EXTRA_CFLAGS='-O3 -fno-strict-aliasing' taskset -c 0-32 make config T=x86_64-native-linuxapp-gcc EXTRA_CFLAGS='-O0 -g' taskset -c 0-32 make -j32 4. 启动testpmd并配置转发 4.1 启动命令 ./build/app/testpmd -c f -n 4 -w 0000:06:00.3 -w 0000:06:00.4 -- --rxq=4 --txq=4 --disable-rss -i 因为要通过将指定报文重定向到指定队列，为了便于观察结果，所以禁用了rss 4.2 转发配置 vlan set filter on 0 rx_vlan add 3001 0 set promisc 0 on set allmulti 0 on set eth-peer 0 fa:65:a1:a6:75:01 port stop 0 tx_vlan set 0 3001 port start 0 vlan set filter on 1 rx_vlan add 3333 1 set promisc 1 on set allmulti 1 on set eth-peer 1 fa:65:a1:a6:75:04 port stop 1 tx_vlan set 1 3333 port start 1 set fwd mac start 这个时候可以测试net1和net2的联通性了 ip netns exec net2 ping 1.1.1.1 5. 创建RTE_FLOW规则并验证 命令如下： flow create 0 priority 0 ingress pattern eth / ipv4 / udp / vxlan / eth / ipv4 / tcp dst is 1001 / end actions queue index 2 / end flow create 0 priority 1 ingress pattern eth / ipv4 / udp / vxlan / eth / ipv4 / tcp dst is 1002 / end actions queue index 3 / end 使用scapy构造报文： p1 = Ether(src=&quot;fa:65:a1:a6:75:04&quot;,dst=&quot;fa:65:a1:a6:75:03&quot;)/IP(src=&quot;1.1.1.2&quot;,dst=&quot;1.1.1.1&quot;)/UDP()/VXLAN(vni=100)/Ether(src=&quot;fa:65:a1:a6:77:14&quot;,dst=&quot;fa:65:a1:a6:77:01&quot;)/IP(src=&quot;172.16.1.254&quot;,dst=&quot;172.16.1.210&quot;) / TCP(sport=8001, dport=1001) / &quot;netease&quot; p2 = Ether(src=&quot;fa:65:a1:a6:75:04&quot;,dst=&quot;fa:65:a1:a6:75:03&quot;)/IP(src=&quot;1.1.1.2&quot;,dst=&quot;1.1.1.1&quot;)/UDP()/VXLAN(vni=100)/Ether(src=&quot;fa:65:a1:a6:77:14&quot;,dst=&quot;fa:65:a1:a6:77:01&quot;)/IP(src=&quot;192.168.1.10&quot;,dst=&quot;192.168.1.210&quot;) / TCP(sport=8001, dport=1002) / &quot;netease&quot; 观察结果： ","link":"https://rexrock.github.io/post/dpdk1/"},{"title":" QUAGGA ZEBRA : bgpd","content":" 启动和关闭BGP bgpd(config)# router bgp asn bgpd(config)# no router bgp asn asn即自治系统的ID，asn对于bgp协议来说是必要的，因为asn要通过asn来判断两个BGP路由之间的关系式IBGP还是EBGP。 设置routerID bgpd(config-router)# bgp router-id A.B.C.D routerID是BGP路由器的标识符，用来标识一台BGP路由器。如果bgpd能够连接到zebra并获取到接口信息，那么bgpd将使用所以接口中最大的IP地址来作为routerID，如果不能连接到zebra，那么routerID将被设置为0.0.0.0。所以设置routerID是十分必要的。 设置管理距离 bgpd(config-router)# distance bgp &lt;1-255&gt; &lt;1-255&gt; &lt;1-255&gt; 设置管理距离，三个值分别为： 设置对等体peer ","link":"https://rexrock.github.io/post/bgpd1/"},{"title":" QUAGGA ZEBRA : ospfd","content":"1. 运行和终止ospfd ospfd(config)# router ospf ospfd(config)# no router ospf 2. 配置router id ospfd(config-router)# ospf router-id a.b.c.d ospfd(config-router)# no ospf router-id 3. 兼容RFC1583的开关 ospfd(config-router)# ospf rfc1583compatibility ospfd(config-router)# no ospf rfc1583compatibility RFC2328是RFC1583的继任者，建议通过改变路径优先算法来避免在老版本OSPFV2中发生的路由环路，具体来讲，就是他要求区域间路径和区域间骨干路径平等但是依然优先外部路径。 4. 配置接口处于被动模式 ospfd(config-router)# passive-interface interface ospfd(config-router)# no passive-interface interface 5. 设置管理距离 ospfd(config-router)# distance &lt;1-255&gt; ospfd(config-router)# no distance &lt;1-255&gt; 6. 创建OSPF区域 network命令和area命令有啥区别？ ospfd(config-router)# network a.b.c.d/m area a.b.c.d ospfd(config-router)# network a.b.c.d/m area &lt;0-4294967295&gt; ospfd(config-router)# no network a.b.c.d/m area a.b.c.d ospfd(config-router)# no network a.b.c.d/m area &lt;0-4294967295&gt; RouterID和AreaID是OSPF报文头中不可缺少的两个字段（都是32bit），所以当使用network指定哪些网络和端口要运行OSPF协议的同时必须指定其所属的AREA。AREA相当于OSPF端口的一个集合，按照一般人的理解，如果没有创建AREA那么何来AREAID，所以在我们的系统中首先使用area areaID命令创建区域，再指定那些网络或者端口在此区域中运行areaID。 ospfd(config-router)# area a.b.c.d range a.b.c.d/m ospfd(config-router)# area &lt;0-4294967295&gt; range a.b.c.d/m ospfd(config-router)# no area a.b.c.d range a.b.c.d/m ospfd(config-router)# no area &lt;0-4294967295&gt; range a.b.c.d/m Area命令是用来设置路由汇聚的，而不是用来定义Area的。 7. 设置虚连接 ospfd(config-router)# area a.b.c.d virtual-link a.b.c.d ospfd(config-router)# area &lt;0-4294967295&gt; virtual-link a.b.c.d ospfd(config-router)# no area a.b.c.d virtual-link a.b.c.d ospfd(config-router)# no area &lt;0-4294967295&gt; virtual-link a.b.c.d 8. 外部路由的Metric说明 ","link":"https://rexrock.github.io/post/ospfd1/"},{"title":"QUAGGA ZEBRA : ripd","content":"1. 运行和终止ripd # zebra -d # ripd -d 由于rip协议需要的接口信息都保存在守护进程zebra中，所以运行ripd之前一定要先运行zebra。 kill ‘cat /var/run/ripd.pid‘ 直接使用kill命令终止ripd 2. 信号处理 ‘SIGHUP’ Reload configuration file ripd.conf. All configurations are reseted. All routes learned so far are cleared and removed from routing table. 3. 开启关闭RIP 必须在调用其他RIP命令之前开启RIP ripd(config)# router rip ripd(config)# no router rip 4. 设置用来发送和接收RIP数据包的接口 4.1 通过网络地址设置 ripd(config-router)# network network ripd(config-router)# no network network 4.2 通过网口名称设置 ripd(config-router)# network ifname ripd(config-router)# no network ifname 5. 指定邻居 如果当前路由器的一个邻居不能处理组播，那么该命令用来在当前路由器上指定该邻居，然后当前路由器将以单播形式来发送更新给这个邻居。 ripd(config-router)# neighbor a.b.c.d ripd(config-router)# no neighbor a.b.c.d 6. 设置被动接口 设置接口处于被动模式，被动接口只接受路由更新而不发送路由更新（使用neighbor指定的邻居除外，也就是说即使接口处于被动模式，但是使用neighbor命令指定的邻居通过该接口与当前路由器连接，那么该接口仍会向这个邻居发送路由更新）。 ripd(config-router)#passive-interface (IFNAME|default) ripd(config-router)#no passive-interface IFNAME 如果为default那么则所有的接口都将处于被动模式。 7. 设置水平分割和带毒性逆转的水平分割 ripd(config-if)# ip rip split-horizon ripd(config-if)# no ip split-horizon 缺省情况下，ripd是启动了水平分割的，使用no ip split-horizon关闭水平分割 ripd(config-if)# ip rip split-horizon poisoned-reverse ripd(config-if)# no ip rip split-horizon poisoned-reverse 缺省情况下，ripd没有打开带毒性逆转的水平分割，使用ip rip split-horizon poisoned-reverse开启带毒性逆转的水平分割。 8. 全局版本控制 默认情况下，可以同时接收ripv1和ripv2的数据包并发送ripv2的数据包。 ripd(config-router)# version version version can be either ‘1” or ‘2”. 通过version命令可以指定发送数据包的版本，该版本是全局的；由于ripv1的不安全性，使用“version 2”指定发送版本是十分受鼓励的。 config# no version 设置全局版本为默认值。 9. 接口版本控制 接口版本控制的优先级高于关于版本控制 ripd(config-if)# ip rip send version version 指定改接发送packets的rip版本，如果指定为‘1 2’，那么packets will be both broadcast and multicast，默认为全局版本. ripd(config-if)# ip rip receive version version version can be ‘1’, ‘2’ or ‘1 2’. 默认为‘1 2’ 10. 设置定时器 10.1 rip协议有几个定时器 更新计时器，默认为30s，每隔一个周期就会主动发送“包含了完整路由表”的应答信息给所有的RIP邻居。 无效计时器，默认是180s，每一条路由被创建的时候，rip会为其建立一个倒计时，如果这个时间内没有收到更新，那么该路由度量将自动被设置为16（ x.x.x.x is possibly down，即不可达），但是在清楚计时器超时以前，该路由仍将保留在路由表中。 清除计时器，清除计时器。默认情况下，清除计时器设置为 240 秒，比无效计时器长 60 秒。当清除计时器超时后，该路由将从路由表中删除。（这里就意味着一个路由条目在180秒内没有收到更新报文时，无效计时器超时。路由条目中该路由被标志为x.x.x.x is possibly down，直到清除计时器也超时了(再过60秒后)该路由条目才被删除。在RIP中真正删除路由条目的是清除计时器超时。） 抑制计时器，抑制计时器。该计时器用于稳定路由信息，并有助于在拓扑结构根据新信息收敛的过程中防止路由环路。在某条路由被标记为不可达后，它处于抑制状态的时间必须足够长，以便拓扑结构中所有路由器能在此期间获知该不可达网络。默认情况下，抑制计时器设置为 180 秒。 10.2 抑制计时器介绍 抑制计时器通过以下方式工作： 路由器从邻居处接收到更新，该更新表明以前可以访问的网络现在已不可访问。 路由器将该网络标记为 possibly down 并启动抑制计时器。 如果在抑制期间从任何相邻路由器接收到含有更小度量的有关该网络的更新，则恢复该网络并删除抑制计时器。 如果在抑制期间从相邻路由器收到的更新包含的度量与之前相同或更大，则该更新将被忽略。如此一来，更改信息便可以继续在网络中传播一段时间。 路由器仍然会转发目的网络被标记为 possibly down 的数据包。通过这种方式，路由器便能克服连接断续所带来的问题。如果目的网络确实不可达，但路由器又转发了数据包，黑洞路由就会建立起来并持续到抑制计时器超时。 分析： 在quagga中，只有三种计时器“更新计时器”、“无效计时器”和“抑制计时器”，上面所描述的清除计时器=无效计时器+抑制计时器。所以ripd的timer命令只需要设置三种计时器即可。 ripd(config-router)# timers basic update timeout garbage ripd(config-router)# no timers basic 11. 设置管理距离 管理距离是一种对路由选择信息的可信度进行排序的方法。管理距离是一个0-255的证书，值越小可信度越高。尽管管理距离可以配置为1-9，但是他们被保留内部使用，不推荐使用他们。 ripd(config-router)# distance &lt;1-255&gt; [RIP command] ripd(config-router)# no distance &lt;1-255&gt; [RIP command] 12. RIPV2的认证 设置ripv2的验证模式（简单密码验证|MD5验证） ripd(config-if)# ip rip authentication mode ripd(config-if)# no ip rip authentication mode 设置简单密码验证的密码 ripd(config-if)# ip rip authentication string string ripd(config-if)# no ip rip authentication string string 设置MD5认证的秘钥链 ripd(config-if)# ip rip authentication key-chain key-chain ripd(config-if)# no ip rip authentication key-chain key-chain ","link":"https://rexrock.github.io/post/ripd1/"},{"title":"NETLINK中常用数据结构说明","content":" include/linux/uio.h 其中nlmsg_flags用于设置消息标志，包括如下： include/linux/socket.h ","link":"https://rexrock.github.io/post/netlink1/"},{"title":"memcached的key和value大小的限制","content":"memcached.h中规定key的长度要小于250byte slab（slab（value）的长度限制默认是1M，可以使用-I选项更改此限制value）的长度限制默认是1M，可以使用-I选项更改此限制 -I Override the size of each slab page. Adjusts max item size (default: 1mb, min: 1k, max: 128m) 要增加的话，启动时添加-I 10m参数就可以,会有一个警告： WARNING: Setting item max size above 1MB is not recommended! Raising this limit increases the minimum memory requirements and will decrease your memory efficiency. ","link":"https://rexrock.github.io/post/memcached4/"},{"title":"Memcached配置详细说明（包括协议说明）","content":"Memcached的key一定不能有空格 1. 常用参数 memcached -m 64 -M -u root -d -l 127.0.0.1 -p 11211 -U 11212 # -m 指定缓存所使用的最大内存容量，单位是Megabytes，默认是64MB # -u 只有以root身份运行时才指定该参数 # -d 以daemon的形式运行 # -l 指定监听的地址 # -p 指定监听的TCP端口号，默认是11211 # -t 指定线程数，默认是4个 # -h 打印帮助信息 # -c 最大同时连接数，默认是1024. # -U 指定监听的UDP端口号，默认是11211 # -M 内存耗尽时显示错误，而不是删除项 # -f 块大小增长因子，默认是1.25 # -n 最小分配空间，key+value+flags默认是48 # “-d”参数需要进行进一步的解释 # -d install 安装memcached # -d uninstall 卸载memcached # -d start 启动memcached服务 # -d restart 重启memcached服务 # -d stop 停止memcached服务 # -d shutdown 停止memc 2. Memcached的命令（memcached有三种类型的命令） 2.0 出错信息 每个命令都有可能被反馈以一个错误消息。这些错误消息有以下三个类型： ERROR 意味着客户端发送了一个在协议中不存在的命令。 CLIENT_ERROR 表示客户端输入的命令行上存在某种错误，输入不符合协议规定。是一个人工可读（human-readable）的错误注释。 SERVER_ERROR 表示服务器在执行命令时发生了某些错误，致使服务器无法执行下去。也是一个人工可读（human-readable）的错误注释。在一些情况下，错误导致服务器不能再为客户端服务（这样的情况很少发生），服务器就会在发生错误消息后主动关闭连接。这也是服务器主动关闭到客户端连接的唯一情况。 2.1 存储命令 命令格式： 3个命令：set、add和replace，要求服务器按照关键字存储数据。 客户端先发送一个命令行，然后才可以发送一个数据块；命令执行后客户端等待一行反馈，用来表示命令执行成功与否。 参数说明： &quot;command name&quot;是 set、add或者replace。set表示存储该数据；add表示如果服务器没有保存该关键字的情况下，存储该数据；replace表示在服务器已经拥有该关键字的情况下，替换原有内容。 &quot;key&quot;是客户端要求服务器存储数据的关键字。 &quot;flags&quot;是一个16位的无符号整数，服务器将它和数据一起存储并且当该数据被检索时一起返回。客户端可能使用该数值作为一个位图来存储特殊数据信息；这个字段对服务器不是透明的。 &quot;exptime&quot;是超时时间。如果值为0表示该数据项永远不超时（但有时候该数据项可能被删除以为其他数据腾出空间）；如果值不为0，可能是绝对的UNIX时间，也可能是自现在开始的偏移值，它保证客户段在这个超时时间到达后，客户端将取不到该数据项。 &quot;bytes&quot;是随后数据的字节数，不包括终结符”&quot;r&quot;n”。有可能是0，它后面将是一个空的数据块。 &quot;data block&quot;是真正要存储数据流。 返回结果： 发送命令行和数据后，客户端等待反馈，可以是如下几种情况： STORED表示存储数据成功。 NOT_STORED表示发送的数据没有存储，但这不因为错误，而是发生在add或者replace命令不能满足条件时，或者数据项正处于要删除的队列中。 错误消息 2.2 读取命令 命令格式： 返回的数据格式 发送命令后，客户端等待返回一个或多个数据项，每个数据项的格式是一个文本行，后跟着一个数据块。当所有的数据项发送完毕后，服务器发送字符串”END&quot;r&quot;n”表示服务器反馈数据的结束。 “key”是发生数据项的关键字。 &quot;flags&quot;是存储该数据项时，客户端命令中的标志字段。 &quot;bytes&quot;是紧跟文本行后数据块的长度，不包括终结符”&quot;r&quot;n”。 &quot;datablock&quot;是数据项的数据部分。 如果请求命令行中的有些关键字对应的数据项没有被返回，这意味着服务器没有该关键字标示下的数据项（有可能是从来没有被存储过，或者存储过但被删除掉以腾出内存空间，或者数据项超时了，再或者它被某个客户端删除了）。 2.3 删除命令 delete每次只能删除一个K-V对象 “key”是客户端希望服务器删除数据项的关键字 &quot;time&quot;是客户端希望服务器阻止add和replace命令使用该关键字数据项的秒数，可以是相对时间也可以是UNIX的绝对时间。在这段时间内，数据项被放入一个删除队列，它不能被get命令读取，在其上使用add和replace也会失败，但使用set命令可以成功。当这个时间过去后，数据项从服务器的内存中真正的删除。该参数是可选参数，如果不存在默认为0，这意味着立即从服务器上删除。 服务器返回信息： &quot;DELETED&quot;r&quot;n&quot; 表示数据项删除成功 &quot;NOT_FOUND&quot;r&quot;n&quot; 表示该关键字指定的数据项在服务器上没有找到 其他错误消息 2.4 flush_all命令 清除memcached中的所有数据 2.5 incr/decr命令 注意： 命令中的value不是K-V对象中的value，而是对应的“加数”或者“减数”，K-V对象中value将会加上或者减去这个值，这个值是一个32位无符号整数。 只有K-V对象的value全部为数字时（即数字之间可以有空格，但是不可有任何其他字符），才可以使用incr/decr命令，当有很多被空格隔开的数字时，只操作第一个数字。 2.6 stats命令 “stats”命令用来查询服务器的运行情况和其他内部数据。它有两种情况，以有无参数来区分： 当接收到没有带参数的“stats”命令后，服务器发送许多类似与如下格式的文本行： 在所有STAT文本行中，是该统计项目的名称，是其数据。下面是一份stats命令反馈的所有统计项目的列表，后面跟着其值的数据类型。在数据类型列中，”32u”表示一个32位无符号整数，”64u”表示一个64位无符号整数，”32u:32u”表示是两个用冒号分割的32位无符号整数。 2.7 quit命令 &quot;quit&quot;是一个没有参数的命令。其格式如下 当服务器接受到此命令后，就关闭与该客户的连接。不管怎样，客户端可以在任意不需要该连接的时刻关闭它，而不需要发送该命令。 2.8 version命令 返回版本信息 服务器发回的反馈信息如下： &quot;VERSION &quot; 是从服务器返回的版本字符串。 错误消息: 3. 关于通信 3.1 协议 memcached的客户端通过TCP连接与服务器通信（UDP协议的接口也可以使用，详细说明请参考”UDP 协议”部分）。一个给定的运行中的memcached服务器在某个（可配置的）端口上监听连接；客户端连接该端口，发送命令给服务器，读取反馈，最后关闭连接。 没有必要发送一个专门的命令去结束会话。客户端可以在不需要该连接的时候就关闭它。注意：我们鼓励客户端缓存它们与服务器的连接，而不是每次要存储或读取数据的时候再次重新建立与服务器的连接。memcache同时打开很多连接不会对性能造成到大的影响，这是因为memcache在设计之处，就被设计成即使打开了很多连接（数百或者需要时上千个连接）也可以高效的运行。缓存连接可以节省与服务器建立TCP连接的时间开销（于此相比，在服务器段为建立一个新的连接所做准备的开销可以忽略不计）。 memcache通信协议有两种类型的数据：文本行和非结构化数据。文本行用来发送从客户端到服务器的命令以及从服务器回送的反馈信息。非结构化的数据用在客户端希望存储或者读取数据时。服务器会以字符流的形式严格准确的返回相应数据在存储时存储的数据。服务器不关注字节序，它也不知道字节序的存在。memcahce对非结构化数据中的字符没有任何限制，可以是任意的字符，读取数据时，客户端可以在前次返回的文本行中确切的知道接下来的数据块的长度。 文本行通常以“&quot;r&quot;n”结束。非结构化数据通常也是以“&quot;r&quot;n”结束，尽管&quot;r、&quot;n或者其他任何8位字符可以出现在数据块中。所以当客户端从服务器读取数据时，必须使用前面提供的数据块的长度，来确定数据流的结束，二不是依据跟随在字符流尾部的“&quot;r&quot;n”来确定数据流的结束，尽管实际上数据流格式如此。 3.2 关键字 Keys memcached使用关键字来区分存储不同的数据。关键字是一个字符串，可以唯一标识一条数据。当前关键字的长度限制是250个字符（当然目前客户端似乎没有需求用这么长的关键字）；关键字一定不能包含控制字符和空格。 3.3 UDP协议 当基于TCP协议的连接数超过TCP连接的上限时，我们可以使用UDP协议来替代。但是UDP协议接口不提供可靠的传输，所以多用在不严格要求成功的操作上；典型的get请求会因为缓存的问题，引起丢失或者不完整的传输。 每个UDP数据包包含一个简单的帧头，接着就是如TCP协议描述的数据格式的数据流。在当前的实现中，请求必须包含在一个单独的UDP数据包中，但返回可能分散在多个数据包中。（唯一的可以拆分请求数据包的是大的多关键字get请求和set请求，鉴于可靠性相比而言他们更适合用TCP传输。） 帧头有8字节长，如下是其格式（所有的数字都是16位网络字节序整形，高位在前）： 0 - 1 请求ID 2 - 3 序列号 4 - 5 在当前的消息中含有的数据包的个数 6-7 保留以后使用，当前必须为0 请求ID由客户端提供。它的典型值是一个从随机种子开始递增值，实际上客户端可以使用任意的请求ID。服务器的反馈信息中包含了和请求命令中一样的请求ID。客户端凭借这个请求ID区分来自于同一服务器的反馈。每一个包含未知请求ID的数据包，可能是由于延时反馈造成，这些数据包都应该抛弃不用。 序列号从0到n-1，n是消息中总的数据包的个数。客户端按照序列号排序重组数据包；结果序列中包含了一个完整的如TCP协议一样格式的反馈信息（包含了“&quot;r&quot;n”总结字符串）。 ","link":"https://rexrock.github.io/post/memcached3/"},{"title":"Libmemcached客户端UDP/UNIX Domain Sockeet使用方法","content":"首先通过看文档： 给客户端添加 server的函数主要是下面四个 说明如下： 简单翻译一下（直说前面 2个）： Memcached_ser_add() 函数添加一个 TCP的server 给memcached_st结构体，如果设置了 MEMCACHED_BEHAVIOR_USE_UDP将会返回一个MEMCACHED_INVALID_HOST_PROTOCOL错误。 Memcached_ser_add_udp() 函数添加一个 UDP的server 给memcached_st结构体，如果不设置 MEMCACHED_BEHAVIOR_USE_UDP将会返回一个MEMCACHED_INVALID_HOST_PROTOCOL错误。 然后我天真的相信了这个文档。 Memcached服务器监听口如下： 初始化代码如下： 结果如下： 原因是没有成功添加 SERVER。我果断去看了下源码： 跟memcached_server_add()函数的代码比较了一下发现 memcached_server_add_udp()什么都没做，如果memcached_st已经定义告诉我 memcached服务器拒绝，如果没有定义告诉我无效的参数，真是会开玩笑。下面的 memcached_server_add()的代码： 意外收获，如果memcached_server_add()的参数 hostname为“绝对”路径名，则默认是使用 UNIX Domain Socket方式进行通信，这非常有用，好啦，回到正题。 我果断跟进去瞧一瞧，server_add() –&gt; __instance_create_with() : _server_init()函数只是一些赋值操作，看到这里是有些 疑问○1， 后面再说； 我惊奇的发现，居然有 UDP的操作，看了一下memcached_is_udp()函数的代码： 不好意思，原来是一个宏，相信大家都很想知道怎样设置这个 flags.use_udp为真，直接改当然没有问题，但是我想到了 memcached_behavior_set()函数（其实之前看过memcached_behavior_set()的代码，所以看到这里时，我就明白了）； memcached_behavior_set() 就是设置 flags.use_udp为真；注意第三个参数就是要付给 flags.use_udp的值，所以一定要为1，否则 flags.use_udp仍为假。 虽说明白了，但是害的亲自测试一下才放心： memcached服务器监听口： 客户端初始化代码： 结果如下： 疑问 01：type是用来指定什么的？ _Server_init()函数中 Self 为struct Instance 类型，其中 原来 每个客户端(struct memcached_st) 只能使用一种通信方式（ TCP OR UDP）； 每个server （struct Instance ）只能使用一种连接方式（ Socket OR UNIX Domain Socket）； Memcached如果指定使用 UNIX Domain Socket通信方式则不能设置TCP和 UDP端口。 所以一共有三种通信方式： TCP/UDP/UNIX Domain Socket。 ","link":"https://rexrock.github.io/post/memcached2/"},{"title":"Memcached安装及使用","content":"1. 下载 1.下载libevent 2.下载memcached 2.下载C语言客户端开发库libmemcached 2. 安装使用 1.libevent ./configure --prefix=/usr # &quot;如果没有指定安装前缀--prefix=/usr否则libevent将会被默认安装在/usr/local/lib目录下&quot; make &amp;&amp; make install 2.memcached ./configure --with-libevent=/usr make &amp;&amp; make install # ＂如果libevent没有被安装在/usr/lib目录下，在编译时会发生错误 # /usr/local/memcached/bin/memcached: error while loading shared libraries: libevent-1.4.so.2: # cannot open shared object file: No such file or directory # 解决办法即创建软链接（参照LD_DEBUG） ln –s /usr/local/lib/libevent-2.0.so.5 /usr/lib/libevent-2.0.so.5＂ 3.libmemcached #./configure #make &amp;&amp; make install 3. 使用 memcached -d -m 64 -l localhost -p 11211 -u rexrock 启动memcached，在运行使用libmemcached开发的客户端程序时要保证memcached是运行的 4. memcached的配置文件 1.自启动脚本/etc/init.d/memcached 2.自启动开关/etc/default/memcached 3.自启动参数/etc/memcached.conf 5. 清空memcached数据的方法 service memcached restart 即重启memcached服务即可 ","link":"https://rexrock.github.io/post/memcached1/"},{"title":"PAMk开发","content":"1. Pam应用程序开发 任何一个支持PAM的应用程序在进行认证时必须以pam_start( )开始进行初始化，最后以pam_end( )结束。 1.1 pam_start() #include &lt;security/pam_appl.h&gt; Int pam_start( const char *service_name, const char *user, const struct pam_conv *pam_conversation, pam_handle_t **pamh ); 1.1.1 参数讲解 service_name 应用的名字，这个名字参数非常重要。 当应用程序执行验证操作时，libpam库会在/etc/pam.d/下寻找以service_name命名的配置文件，该配置文件中指定了相关参数，其中包括将要调用哪个动态链接库进行验证。 配置文件的格式与前面讲的/etc/pam.conf的格式基本一致，只是去掉了service-name这一项，因为该配置文件的名字就是service-name。 其实linux系统中/etc/pam.conf这个配置文件在整个pam框架中已经起不到什么作用，至少在ubuntu10.04系统中是否对这个配置文件进行配置，根本什么也不影响。 前面的对/etc/pam.conf的所有介绍均适用于/etc/pam.d/目录下的配置文件。 user 用户名，即PAM框架所作用的用户的名称。说白了就是你要对哪个用户进行pam验证，该用户就是此轮验证中（pam_start——pam_end）被操作的对象。 &amp;conv 对话函数conv，用于提供PAM与用户或应用程序进程通信的通用方法。对话函数是必需的，因为PAM模块无法了解如何进行通信。通信可以采用 GUI、命令行、智能读卡器或其他设备等方式进行。这个对话函数是一个回调函数，这里只是对它的一个注册，接下来的验证过程中，应用程序和服务模块之间的所有信息交互都要通过它。对话函数需要应用程序的作者自己编写。 &amp;pamh PAM句柄 pamh，即 PAM 框架用于存储有关当前操作信息的不透明句柄。成功调用 pam_start() 后将返回此句柄。要研究这个pamh，需要对pam的内部实现机制进行了解，有点复杂，暂时先不说这个。大家把它想成类似于一个socket套接字的东西就行，反正就是一个句柄而已。 1.1.2 返回值讲解 PAM_ABORT 一般性的错误，我也不知道什么叫一般性的错误。 PAM_BUF_ERR 内存缓冲区错误，具体的我也不知道。 PAM_SUCCESS 成功建立验证xx，反正就是成功了。 PAM_SYSTEM_ERR 系统错误，参数中有无效的指针。有些参数必须提供指向了真实数据的指针变量。 1.2 pam_end() #include &lt;security/pam_appl.h&gt; int pam_end( pam_handle_t *pamh, int pam_status); 1.2.1 参数讲解 pam_end()函数是整个pam验证过程中，应用程序最后调用的函数。从此以后句柄pamh不再有效，而且所有的占用的内存将被释放。 pamh 就是pam_start()函数中创建的pamh； pam_status 是应用程序在执行pam_end()函数前所执行的最后一个pam API函数的返回值。 Pam_status通常被传递给服务模块的特有的一个回调函数cleanup()，这样服务模块就知道关闭应用程序是否成功，然后继续执行和之前一样的工作，即给其他应用程序提供验证服务。 Pam_end()释放了pam_set_item()和pam_get_item()申请的所有内存，所有对象的指针在pam_end()执行后也都不在有效。 1.2.2 返回值讲解 PAM_SUCCESS 应用程序的验证过程被成功终止。 PAM_SYSTEM_ERR 系统错误，例如pamt是个无效指针NULL，或者函数还在被服务模块调用。 在pam_start()和pam_end()之间就是应用程序要执行的验证操作。就是所谓的四个服务模块。当然，你可以只进行其中的一项认证或者几个认证，哪怕没有任何验证操作，只要您认为值得就可以。 1.3 认证管理pam_authenticate() #include &lt;security/pam_appl.h&gt; int pam_authenticate( pam_handle_t *pamh, int flags); 该函数被用来验证用户的合法性（即令牌认证），这个用户就是在pam_start()参数传递的user。User被要求提供一个密码或者一个简单的数字输入。然后服务模块中对应的pam_sm_authenticate()函数会检测user的输入并验证是否合法。 1.3.1 参数讲解 pamh 参数pamh就是pam_start()函数中创建的pamh了（所有函数的pamh参数都相同，下面的函数将不再介绍pamh参数）； flags 参数flags可以被设置为0，或者设置为如下值： PAM_SILENT 不输出任何信息 PAM_DISALLOW_NULL_AUTHTOK 如果用户没有注册，那么服务模块应该返回PAM_DISALLOW_NULL_AUTHTOK； 1.3.2 返回值讲解 PAM_ABORT 如果收到这个，应用程序应该立即调用pam_end()退出； PAM_AUTH_ERR user没有被验证； PAM_CRED_INSUFFICIENT 由于一些原因应用程序没有足够的凭证来验证用户； PAM_AUTHINFO_UNVAIL 服务模块不能获取user的验证信息，原因可能是网络或硬件配置错误； PAM_MAXTRIES 服务模块验证用户的次数达到上限，应用程序这边不要再提交验证请求了，也就是说不要再运行pam_ authenticate()了； PAM_SUCCESS 用户成功通过验证 PAM_USER_UNKNOWN 该用户不能够被服务模块识别，我估计要么是用户名格式非法，要么是用户没有注册等其他原因。 1.4 账户管理pam_acct_mgmt() #include &lt;security/pam_appl.h&gt; int pam_acct_mgmt( pam_handle_t *pamh, int flags); pam_acct_mgmt()通常被用来确认用户账户是否有效。确认的内容可以包括下面的内容： 令牌认证（就是pam_authenticate()实现的功能） 账户是否过期； 访问权限； 该函数一般在pam_authenticate()成功执行（即用户通过验证）之后被调用执行，所以上面内容中的第一项常可以被省略。 1.4.1参数讲解 与pam_authenticate完全相同。 1.4.2返回值讲解 PAM_ACCT_EXPIRED 用户已经过期失效； PAM_AUTH_ERR 用户令牌认证失败； PAM_NEW_AUTHTOK_REQD 该用户账户是有效的但是认证令牌是过期的，正确的做法是回复该值要求用户执行pam_chauthtok()来更新令牌（密码）在用户获得其他服务之前。 PAM_PERM_DENIED 不允许访问，应该就是所谓的权限控制； PAM_SUCCESS 认证令牌被成功更新，或者是用过通过认证； PAM_USER_UNKNOWN 该用户不能够被服务模块识别； 1.5 会话管理pam_open_session() #include &lt;security/pam_appl.h&gt; int pam_open_session( pam_handle_t *pamh, int flags); 该函数为已经成功通过验证的用户建立一个用户会话，该会话应该在后面被函数pam_close_session()终止。 1.5.1参数讲解 参数flags可以被设置为0或者下面的值： PAM_SILENT 不输任何信息； 1.5.2返回值讲解 PAM_ABORT 一般性的失败； PAM_BUF_ERR 内存缓冲区错误； PAM_SESSION_ERR 建立会话失败； PAM_SUCCESS 成功建立会话； 1.6会话管理pam_close_session() #include &lt;security/pam_appl.h&gt; int pam_close_session( pam_handle_t *pamh, int flags); 该函数被用来关闭pam_open_session()创建的会话。参数和返回值与pam_open_session()的参数和返回值完全相同。 1.7密码管理pam_chauthtok() #include &lt;security/pam_appl.h&gt; int pam_chauthtok( pam_handle_t *pamh, int flags); 1.7.1参数讲解 PAM_SILENT 不输任何信息； PAM_CHANGE_EXPIRED_AUTHTOK 告诉服务模块只更新过期令牌，如果不设置这个参数，应用程序要求更改所有用户的令牌； 1.7.2返回值讲解 PAM_AUTHTOK_ERR 服务模块未能获得新的用户令牌； PAM_AUTHTOK_RECOVERY_ERR 服务模块未能获得旧的用户令牌； PAM_AUTHTOK_LOCK_BUSY 又有用户令牌被锁定，服务模块不能对其进行更改；. PAM_AUTHTOK_DISABLE_AGING 用户令牌被至少一个服务模块禁用了； PAM_PERM_DENIED 没有权限； PAM_SUCCESS 用户令牌被成功更新； PAM_TRY_AGAIN 并不是所有的服务模块都能够更新用户令牌，遇到这种情况，用户令牌都不能得到更新； PAM_USER_UNKNOWN 该用户不能够被服务模块识别； 1.8认证管理pam_setcred() #include &lt;security/pam_appl.h&gt; int pam_setcred( pam_handle_t *pamh, int flags); pam_setcred()函数被用来创建、维持、或删除一个用户的证书。Pam_setcred()应该在user已经通过验证（after pam_authenticate）并且在会话建立之前(before pam_open_ session)被调用。删除user证书的操作必须在会话被关闭之后执行（after pam_close_ session）.user证书应该被应用程序创建，而不是被pam库或者服务模块创建。 1.8.1参数讲解 参数flags可以被设置为0或者下面的值： PAM_ESTABLISH_CRED 初始化用户证书； PAM_DELETE_CRED 删除用户证书； PAM_REINITIALIZE_CRED 完全重置用户证书； PAM_REFRESH_CRED 延长用户证书的生命周期； 1.8.2返回值讲解 PAM_BUF_ERR 内存缓冲区错误； PAM_CRED_ERR 设置（创建、维持、重置、回复、删除等）用户证书失败 PAM_CRED_EXPIRED 用户证书过期； PAM_CRED_UNAVAIL 回复用户证书失败； PAM_SUCCESS 数据被成功存储； PAM_SYSTEM_ERR 系统错误，例如无效的指针被传入，函数正在被其他模块调用，或者系统错误等等； PAM_USER_UNKNOWN 该用户不能被够服务模块识别； 2. Pam服务模块开发 2.1认证管理pam_sm_authenticate() #define PAM_SM_AUTH #include &lt;security/pam_modules.h&gt; PAM_EXTERN int pam_sm_authenticate( pam_handle_t *pamh, int flags, int argc, const char **argv) pam_sm_authenticate()函数是pam_authenticate()函数在服务模块中的接口，用于执行验证用户令牌的任务。下面将对所有pam_sm_xxx()函数统称为接口函数。 2.1.1参数讲解 服务模块中6个接口函数的参数完全相同，仅在这里做详细说明； 我们永远无法在我们设计的应用程序中直接调用这6个接口函数，也无法在我们自己设计的服务模块中让这6个接口函数相互调用，这6接口函数只能作为回调函数以动态链接库的形式存在，并且只能被PAM库调用。认证请求和认证服务是分离的，应用程序只负责提出认证请求，服务模块负责认证，两者靠PAM库连接起来。 PAM从认证请求函数中获得pamh参数和flags参数，并准备作为参数传递给对应接口函数（分别对应接口函数中pamh和flags）。然后PAM库从配置文件(配置文件的名字在pam_start()函数中指定)中的arguments项中，获取将要传递给接口函数的第四个参数argv，并计算出参数的个数作为接口函数的第三个参数。 整个参数传递的过程大致描述如上。接下来将不对接口函数的参数进行讲解，因为接口函数只能被PAM库调用，传递进来的参数的含义在其他部分已做过说明。 2.1.2返回值讲解 接口函数pam_sm_xxx()的返回值与应用程序中对应函数pam_xxx()的返回值基本一致，不做讲解，若有疑问，请参考Man手册。 2.2账户管理pam_sm_acct_mgmt() #define PAM_SM_ACCOUNT #include PAM_EXTERN int pam_sm_acct_mgmt(pam_handle_t *pamh, int flags, int argc, const char **argv); 2.3会话管理pam_sm_open_session() #define PAM_SM_SESSION #include PAM_EXTERN int pam_sm_open_session(pam_handle_t *pamh, int flags, int argc, const char **argv); 2.4会话管理pam_sm_close_session() #define PAM_SM_SESSION #include PAM_EXTERN int pam_sm_close_session(pam_handle_t *pamh, int flags, int argc, const char **argv); 2.5密码管理pam_sm_chauthtok() #define PAM_SM_PASSWORD #include PAM_EXTERN int pam_sm_chauthtok(pam_handle_t *pamh, int flags, int argc, const char **argv); 2.6认证管理pam_sm_setcred() #define PAM_SM_AUTH #include PAM_EXTERN int pam_sm_setcred(pam_handle_t *pamh, int flags, int argc, const char **argv); 3. Pam配置文件编辑 使用pam_start()函数指定配置文件的文件名之后，就应该在/etc/pam.d/目录下创建对应的配置文件，并按照第一部分“Pam的配置文件”中的讲解编辑配置文件并保存，即可，不需要重启什么服务。 ","link":"https://rexrock.github.io/post/pam2/"},{"title":"PAM介绍","content":"1. Pam概述 PAM（Pluggable Authentication Modules ）是由Sun提出的一种认证机制。它通过提供一些动态链接库和一套统一的API，将系统提供的服务和该服务的认证方式分开，使得系统管理员可以灵活地根据需要给不同的服务配置不同的认证方式而无需更改服务程序，同时也便于向系统中添加新的认证手段。 2. Pam框架 pam框架有以下四部分组成： PAM 应用程序，也称为消费方; PAM 库; PAM 配置文件; PAM 服务模块，也称为提供者; 该框架可为与验证相关的活动提供统一的执行方式。采用该方式，应用程序开发者可使用 PAM 服务，而不必了解策略的语义。算法是集中提供的。可以独立于各个应用程序对算法进行修改。借助 PAM，管理员可以根据特定系统的需要调整验证过程，而不必更改任何应用程序。调整是通过 PAM 配置文件 pam.conf 来执行的。 下图说明了 PAM 体系结构。应用程序通过 PAM 应用编程接口 (application programming interface, API) 与 PAM 库进行通信。PAM 模块通过 PAM 服务提供者接口 (service provider interface, SPI) 与 PAM 库进行通信。通过这种方式，PAM 库可使应用程序和模块相互进行通信。 3. Pam服务模块 PAM 服务模块是一个共享库（动态链接库），用于为系统登录应用程序（如 login、rlogin 和 telnet）提供验证和其他安全服务。四种类型的PAM服务是： 验证服务模块 用于授予用户访问帐户或服务的权限。提供此服务的模块可以验证用户并设置用户凭证。pam_authenticate () | pam_setcred () 帐户管理模块 用于确定当前用户的帐户是否有效。提供此服务的模块可以检查口令或帐户的失效期以及限时访问。pam_acc_mgmt () 会话管理模块 用于设置和终止登录会话。pam_open_session () | pam_close_session () 口令管理模块 用于强制实施口令强度规则并执行验证令牌更新。pam_chauthok () 一个 PAM 模块可以实现其中的一项或多项服务。将简单模块用于明确定义的任务中可以增加配置灵活性。因此，应该在不同的模块中实现PAM服务。然后，可以按照 pam.conf 文件中定义的方式根据需要使用这些服务。 每个使用PAM认证的应用程序都以pam_start开始，pam_end结束。实际做认证工作的API函数有六个（以下将这六个函数简称为认证API）： 应用程序的API还有很多，但是服务模块的API只有6个，如下图： 其中应用程序API的函数原型均为： int pam_xxx( pam_handle_t *pamh, //传递给SPI的参数，对应SPI参数中的pamh int flags //传递给SPI的参数，对应SPI参数中的flags ); 其中服务模块SPI的函数原型均为： PAM_EXTERN int pam_sm_xxx( //本文中红色标记处为SPI参数的来源 pam_handle_t *pamh, int flags, int argc, const char **argv ); 4. Pam库 PAM 库 libpam 是PAM体系结构中的中心元素： libpam 可以导出 API pam。应用程序可以调用此API以执行验证、帐户管理、凭证建立、会话管理以及口令更改。 libpam 可以导入主配置文件 pam.conf。PAM 配置文件可指定每种可用服务的PAM模块要求。pam.conf 由系统管理员进行管理。 libpam 可以导入SPI pam_sm，而导出则由服务模块完成。 5. Pam配置文件（/etc/pam.conf） 该文件是由如下的5项所组成的： 5.1 service-name 应用的名字，比如telnet、login、ftp等，服务名字“OTHER”代表所有没有在该文件中明确配置的其它服务。 5.2 module-type 模块类型有四种：auth、account、session、password，即对应PAM所支持的四种管理方式。同一个服务可以调用多个 PAM模块进行认证，这些模块构成一个stack。 5.3 control-flag 用来告诉PAM库该如何处理与该服务相关的PAM模块的成功或失败情况。它有四种可能的 值：required，requisite，sufficient，optional。 required 表示本模块必须返回成功才能通过认证，但是如果该模块返回失败的话，失败结果也不会立即通知用户，而是要等到同一stack 中的所有模块全部执行完毕再将失败结果返回给应用程序。可以认为是一个必要条件。 requisite 与required类似，该模块必须返回成功才能通过认证，但是一旦该模块返回失败，将不再执行同一stack内的任何模块，而是直 接将控制权返回给应用程序。是一个必要条件。注：这种只有RedHat支持，Solaris不支持。 sufficient 表明本模块返回成功已经足以通过身份认证的要求，不必再执行同一stack内的其它模块，但是如果本模块返回失败的话可以 忽略。可以认为是一个充分条件。 optional表明本模块是可选的，它的成功与否一般不会对身份认证起关键作用，其返回值一般被忽略。 Binding 5.4 module-path 用来指明本模块对应的程序文件的路径名，一般采用绝对路径，如果没有给出绝对路径，默认该文件在目录/usr/lib/security下 面。 5.5 arguments（此处的参数传递给SPI,对应着SPI参数中的 argc 和 argv） 是用来传递给该模块的参数。一般来说每个模块的参数都不相同，可以由该模块的开发者自己定义，但是也有以下几个共同 的参数： debug 该模块应当用syslog( )将调试信息写入到系统日志文件中。 no_warn 表明该模块不应把警告信息发送给应用程序。 use_first_pass 表明该模块不能提示用户输入密码，而应使用前一个模块从用户那里得到的密码。 ry_first_pass 表明该模块首先应当使用前一个模块从用户那里得到的密码，如果该密码验证不通过，再提示用户输入新的密码。 use_mapped_pass 该模块不能提示用户输入密码，而是使用映射过的密码。 expose_account 允许该模块显示用户的帐号名等信息，一般只能在安全的环境下使用，因为泄漏用户名会对安全造成一定程度的威 胁。 如果对于该服务的操作（如验证或帐户管理）/etc/pam.conf 仅包含一个模块，则该模块的结果将确定操作的结果。 如果为服务操作定义了多个模块，那么这些模块就堆叠起来，即，对于该服务存在一个 PAM 堆栈。下图说明如何为每种类型的控制标志记录成败信息 其实在linux系统中，/etc/pam.conf这个配置文件已经起不到什么作用了，因为它的作用完全被/etc/pam.d/中“对应”的配置文件代替了。/etc/pam.d/中“对应”的配置文件就是以/etc/pam.conf中service-name项的“值”命名的。如下图所示： 上图中service-name的值为myyanzheng，那么在/etc/pam.d/中就有这样一个名字为myyanzheng的文件，其内容如下图所示： 可以看到上图中配置文件myyanzheng的内容只比/etc/pam.conf少了service-name一项，之所少这一项，因为该配置文件的名字就是service-name。 ","link":"https://rexrock.github.io/post/pam1/"},{"title":"汇编实现hello world程序","content":"C内嵌汇编实现hello world程序，手动执行ld进行静态链接，不依赖任何库 编译： gcc -c write.c -c : 编译和汇编，不进行链接 链接： ld -static -e main -o write ./write.o -static : 静态链接 -e main : 执行程序入口函数 #define EXITNO 23 const char *str = &quot;Hello world\\n&quot;; #ifdef __x86_64__ void m_print() { asm volatile( &quot;syscall&quot; : :&quot;a&quot;(1), &quot;D&quot;(0), &quot;S&quot;(str), &quot;d&quot;(12) ); } void m_exit() { asm volatile( &quot;syscall&quot; : :&quot;a&quot;(60), &quot;D&quot;(EXITNO) ); } #else void m_print() { asm volatile( &quot;int $0x80&quot; : :&quot;a&quot;(4), &quot;b&quot;(0), &quot;c&quot;(str), &quot;d&quot;(13) ); } void m_exit() { asm volatile( &quot;int $0x80&quot; : :&quot;a&quot;(1), &quot;b&quot;(EXITNO) ); } #endif int main() { m_print(); m_exit(); } ","link":"https://rexrock.github.io/post/ass1/"}]}