{
  "posts": [
    {
      "content": "\n目前 ovs、dpdk、cilium均对 AF_XDP 做了支持，这是否预示在高性能报文转发方面 AF_XDP未来将成为DPDK外又一重要技术分支？加之AF_XDP跟内核更好的配合，随着技术不断程序，AF_XDP是否会全面超越甚至取代DPDK成为高性能报文转发的首选？未来不得而知，但至少从目前看，AF_XDP性能上仍不及DPDK，下面通过一个简单的测试来具体看一下。\n\n> 说明：本次测试，AF_XDP时NATIVE模式，而不是NATIVE_WITH_ZEROCOPY模式，在CX5网卡上 ZEROCOPY开启失败，看来需要网卡的支持，后续再调试解决吧。不开启zerocopy，根据以往测试经验性能可能会有20%-30%的下降。\n\n## 1. 测试拓扑\n![enter description here](https://rexrock.github.io/post-images/1614651821455.png)\n\n## 2. 软件版本\nOVS-2.12 + DPDK-20.11 + KERNEL-5.4.87，具体要求及ovs编译配置参考：\n[Open vSwitch with AF_XDP](https://docs.openvswitch.org/en/latest/intro/install/afxdp/?highlight=native-with-zerocopy#setup-af-xdp-netdev)\n\n## 3. OVS配置\n\n**OVS编译：**\n\n``` \n./configure --prefix=/usr/ --enable-afxdp --with-dpdk --with-debug CFLAGS=\"-O3\"\nmake && make install\n```\n\n**OVS初始化及网络配置：**\n\n``` javascript\n/usr/bin/ovs-vsctl --no-wait set Open_vSwitch . other_config:pmd-cpu-mask=0x550\n\ninit_dpdk() {\novs-vsctl set Open_vSwitch . other_config:dpdk-init=true\n/usr/bin/ovs-vsctl --no-wait set Open_vSwitch . other_config:dpdk-socket-mem=2048,0\n/usr/bin/ovs-vsctl --no-wait set Open_vSwitch . other_config:dpdk-extra=\"-a 0000:5e:00.0,txq_inline=128,txqs_min_inline=4,txq_mpw_en=0 -a 0000:5e:00.0,txq_inline=128,txqs_min_inline=4,txq_mpw_en=0\"\n}\n\nadd_xdp_port () {\n    ovs-vsctl -- add-br br0 \\\n          -- set Bridge br0 datapath_type=netdev\n    ovs-vsctl add-port br0 ens2f0 \\\n          -- set interface ens2f0 type=\"afxdp\" options:xdp-mode=native-with-zerocopy options:n_rxq=4 other_config:pmd-rxq-affinity=\"0:4,1:6,2:8,3:10\"\n    ovs-vsctl add-port br0 ens2f1 \\\n          -- set interface ens2f1 type=\"afxdp\" options:xdp-mode=native-with-zerocopy options:n_rxq=4 other_config:pmd-rxq-affinity=\"0:4,1:6,2:8,3:10\"\n}\n\nadd_dpdk_port() {\n    ovs-vsctl --may-exist add-br br0 \\\n          -- set Bridge br0 datapath_type=netdev \\\n          -- br-set-external-id br0 bridge-id br0 \\\n          -- set bridge br0 fail-mode=secure\n\n    ovs-vsctl --timeout 10 add-port br0 ens2f0 \\\n              -- set Interface ens2f0 type=dpdk options:dpdk-devargs=0000:5e:00.0 options:n_rxq=4 other_config:pmd-rxq-affinity=\"0:4,1:6,2:8,3:10\"\n\n\n    ovs-vsctl --timeout 10 add-port br0 ens2f1 \\\n              -- set Interface ens2f1 type=dpdk options:dpdk-devargs=0000:5e:00.1 options:n_rxq=4 other_config:pmd-rxq-affinity=\"0:4,1:6,2:8,3:10\"\n}\n\n#init_dpdk\n#sleep 1\n#add_dpdk_port\nadd_xdp_port\n```\n\n**OVS流表配置：**\n\n``` javascript\novs-ofctl del-flows br0\n\novs-ofctl add-flow br0 table=0,priority=0,action=normal\n\novs-ofctl add-flow br0 table=1,priority=0,action=normal\n\novs-ofctl add-flow br0 table=0,priority=20,ip,dl_src=b8:59:9f:41:0e:d6,in_port=ens2f0,action=load:0-\\>NXM_OF_IN_PORT[],goto_table:1\n\novs-ofctl add-flow br0 table=0,priority=20,ip,dl_src=b8:59:9f:41:0e:d7,in_port=ens2f1,action=load:0-\\>NXM_OF_IN_PORT[],goto_table:1\n\novs-ofctl add-flow br0 table=1,priority=20,ip,nw_dst=172.10.1.1,action=set_field:b8:59:9f:41:11:8e-\\>eth_src,set_field:b8:59:9f:41:0e:d6-\\>eth_dst,output:ens2f0\n\novs-ofctl add-flow br0 table=1,priority=20,ip,nw_dst=172.10.2.1,action=set_field:b8:59:9f:41:11:8f-\\>eth_src,set_field:b8:59:9f:41:0e:d7-\\>eth_dst,output:ens2f1\n```\n\n## 4. NODE配置\n\n``` javascript\nip netns add net1\nip netns add net2\nsleep 1\nip link set ens2f0 netns net1\nip link set ens2f1 netns net2\nsleep 1\nip netns exec net1 ifconfig ens2f0 172.10.1.1/24 up\nip netns exec net2 ifconfig ens2f1 172.10.2.1/24 up\nip netns exec net1 route add -net 172.10.2.0/24 gw 172.10.1.2 dev ens2f0\nip netns exec net2 route add -net 172.10.1.0/24 gw 172.10.2.2 dev ens2f1\n```\n\n## 5. 测试结果\n\n### 5.1 Throughput\n\n单口25G网卡，带宽AF_XDP和DPDK两种场景下带宽都能够打满：\n\n![Throughput](https://rexrock.github.io/post-images/1614652578603.png)\n\n我们主要看下，相同带宽下，各自的PMD使用率：\n\n![AF_XDP](https://rexrock.github.io/post-images/1614652639030.png)\n\n![DPDK](https://rexrock.github.io/post-images/1614652664832.png)\n\n很明显，DPDK PMD使用率更低，并且hash的更均匀；\n\n### 5.2 PPS\n\n\nAF_XDP：327W pps\nDPDK：1400W pps\n\n并且各自峰值的情况下，PMD使用率DPDK仍是全面占优：\n\n![AF_XDP](https://rexrock.github.io/post-images/1614652872603.png)\n\n![DPDK](https://rexrock.github.io/post-images/1614652884076.png)\n\n### 5.3 Latency\n\n``` javascript\nnetperf -t TCP_RR -H 172.10.2.1 -l 30 -- -r 1B,1B -O  \"MAX_LATENCY,MEAN_LATENCY,P90_LATENCY,P99_LATENCY,P999_LATENCY,P9999_LATENCY,STDDEV_LATENCY,THROUGHPUT,THROUGHPUT_UNITS\"\n```\n\n![TCP_RR](https://rexrock.github.io/post-images/1614653034804.png)\n\n``` \nnetperf -t TCP_CRR -H 172.10.2.1 -l 30 -- -r 1B,1B -O  \"MAX_LATENCY,MEAN_LATENCY,P90_LATENCY,P99_LATENCY,P999_LATENCY,P9999_LATENCY,STDDEV_LATENCY,THROUGHPUT,THROUGHPUT_UNITS\"\n```\n\n![TCP_CRR](https://rexrock.github.io/post-images/1614653168657.png)\n",
      "data": {
        "title": "AF_XDP VS DPDK",
        "date": "2021-03-03 12:32:00",
        "tags": [
          "eBPF",
          "XDP"
        ],
        "published": true,
        "hideInList": false,
        "feature": null,
        "isTop": false
      },
      "isEmpty": false,
      "excerpt": "",
      "abstract": "",
      "fileName": "af_xdp2"
    },
    {
      "content": "\nAF_XDP是一个协议族（例如AF_NET），主要用于高性能报文处理。\n\n前文[XDP技术简介](xdp1.md)中提到过，通过XDP_REDIRECT我们可以将报文重定向到其他设备发送出去或者重定向到其他的CPU继续进行处理。而AF_XDP则利用 bpf_redirect_map()函数，实现将报文重定向到用户态一块指定的内存中，接下来我们看一下这到底是如何做到的。\n\n我们使用普通的 socket() 系统调用创建一个AF_XDP套接字（XSK）。每个XSK都有两个ring：RX RING 和 TX RING。套接字可以在 RX RING 上接收数据包，并且可以在 TX RING 环上发送数据包。这些环分别通过 setockopts() 的 XDP_RX_RING 和 XDP_TX_RING 进行注册和调整大小。每个 socket 必须至少有一个这样的环。RX或TX描述符环指向存储区域（称为UMEM）中的数据缓冲区。RX和TX可以共享同一UMEM，因此不必在RX和TX之间复制数据包。\n\nUMEM也有两个 ring：FILL RING 和 COMPLETION RING。应用程序使用 FILL RING 向内核发送可以承载报文的 addr (该 addr 指向UMEM中某个chunk)，以供内核填充RX数据包数据。每当收到数据包，对这些 chunks 的引用就会出现在RX环中。另一方面，COMPLETION RING包含内核已完全传输的 chunks 地址，可以由用户空间再次用于 TX 或 RX。\n\n![enter description here](https://rexrock.github.io/post-images/1614584458041.png)\n\n> 关于ring，熟悉dpdk的同学应该都不陌生，这里只做简单介绍。ring就是一个固定长度的数组，并且同时拥有一个生产者和一个消费者，生产者向数组中逐个填写数据，消费者从数组中逐个读取生产者填充的数据，生产者和消费者都用数组的下标表示，不断累加，像一个环一样不断重复生产然后消费的动作，因此得名ring。\n> ![enter description here](https://rexrock.github.io/post-images/1614166502357.png)\n\n> 此外需要注意的事，AF_XDP socket不再通过 send()/recv()等函数实现报文收发，而实通过直接操作ring来实现报文收发。\n> \n>  1. FILL RING\n> \n> **fill_ring 的生产者是用户态程序，消费者是内核态中的XDP程序；**\n> \n> 用户态程序通过 fill_ring 将可以用来承载报文的 UMEM frames 传到内核，然后内核消耗 fill_ring 中的元素（后文统一称为 desc），并将报文拷贝到desc中指定地址（该地址即UMEM frame的地址）；\n> \n>  2. COMPLETION RING\n> \n> **completion_ring 的生产者是XDP程序，消费者是用户态程序；**\n> \n> 当内核完成XDP报文的发送，会通过 completion_ring 来通知用户态程序，哪些报文已经成功发送，然后用户态程序消耗 completion_ring 中 desc(只是更新consumer计数相当于确认)；\n> \n>  3. RX RING\n> \n> **rx_ring的生产者是XDP程序，消费者是用户态程序；**\n> \n> XDP程序消耗 fill_ring，获取可以承载报文的 desc并将报文拷贝到desc中指定的地址，然后将desc填充到 rx_ring 中，并通过socket IO机制通知用户态程序从 rx_ring 中接收报文；\n> \n>  4. TX RING\n> \n> **tx_ring的生产者是用户态程序，消费者是XDP程序；**\n> \n> 用户态程序将要发送的报文拷贝 tx_ring 中 desc指定的地址中，然后 XDP程序 消耗 tx_ring 中的desc，将报文发送出去，并通过 completion_ring 将成功发送的报文的desc告诉用户态程序；\n\n## 1. 用户态程序\n\n### 1.1 创建AF_XDP的socket\n\n```\nxsk_fd = socket(AF_XDP, SOCK_RAW, 0);\n```\n这一步没什么好展开的。\n\n### 1.2 为UMEM申请内存\n\n上文提到UMEM是一块包含固定大小chunk的内存，我们可以通过malloc/mmap/hugepages申请。下文大部分代码出自kernel samples。 \n```\n    bufs = mmap(NULL, NUM_FRAMES * opt_xsk_frame_size,\n                         PROT_READ | PROT_WRITE,\n                         MAP_PRIVATE | MAP_ANONYMOUS | opt_mmap_flags, -1, 0);\n    if (bufs == MAP_FAILED) {\n        printf(\"ERROR: mmap failed\\n\");\n        exit(EXIT_FAILURE);\n    }\n```\n\n### 1.3 向AF_XDP socket注册UMEM\n\n```\n        struct xdp_umem_reg mr;\n        memset(&mr, 0, sizeof(mr));\n        mr.addr = (uintptr_t)umem_area; // umem_area即上面通过mmap申请到内存起始地址\n        mr.len = size;\n        mr.chunk_size = umem->config.frame_size;\n        mr.headroom = umem->config.frame_headroom;\n        mr.flags = umem->config.flags;\n\n        err = setsockopt(umem->fd, SOL_XDP, XDP_UMEM_REG, &mr, sizeof(mr));\n        if (err) {\n                err = -errno;\n                goto out_socket;\n        }\n```\n其中xdp_umem_reg结构定义在 usr/include/linux/if_xdp.h中：\n\n```\nstruct xdp_umem_reg {\n        __u64 addr; /* Start of packet data area */\n        __u64 len; /* Length of packet data area */\n        __u32 chunk_size;\n        __u32 headroom;\n        __u32 flags;\n};\n```\n**成员解析：**\n - addr就是UMEM内存的起始地址；\n - len是整个UMEM内存的总长度；\n - chunk_size就是每个chunk的大小；\n - headroom，如果设置了，那么报文数据将不是从每个chunk的起始地址开始存储，而是要预留出headroom大小的内存，再开始存储报文数据，headroom在隧道网络中非常常见，方便封装外层头部；\n - flags, UMEM还有一些更复杂的用法，通过flag设置，后面再进一步展开；\n\n### 1.4 创建FILL RING 和 COMPLETION RING\n\n我们通过 setsockopt() 设置 FILL/COMPLETION/RX/TX ring的大小（在我看来这个过程相当于创建，不设置大小的ring是没有办法使用的）。\n\nFILL RING 和 COMPLETION RING是UMEM必须，RX和TX则是 AF_XDP socket二选一的，例如AF_XDP socket只收包那么只需要设置RX RING的大小即可。\n```\n        err = setsockopt(umem->fd, SOL_XDP, XDP_UMEM_FILL_RING,\n                         &umem->config.fill_size,\n                         sizeof(umem->config.fill_size));\n        if (err) {\n                err = -errno;\n                goto out_socket;\n        }\n        err = setsockopt(umem->fd, SOL_XDP, XDP_UMEM_COMPLETION_RING,\n                         &umem->config.comp_size,\n                         sizeof(umem->config.comp_size));\n        if (err) {\n                err = -errno;\n                goto out_socket;\n        }\n```\n上述操作相当于创建了 FILL RING 和 和 COMPLETION RING，创建ring的过程主要是初始化 producer 和 consumer 的下标，以及创建ring数组。\n\n**问题来了：**\n\n上文提到，用户态程序是 FILL RING 的生产者和 CONPLETION RING 的消费者，上面2个 ring 的创建是在内核中创建了 ring 并初始化了其相关成员。那么用户态程序如何操作这两个位于内核中的 ring 呢？所以接下来我们需要将整个 ring 映射到用户态空间。\n\n### 1.5 将FILL RING 映射到用户态\n\n第一步是获取内核中ring结构各成员的偏移，因为从5.4版本开始后，ring结构中除了 producer、consumer、desc外，又新增了一个flag成员。所以用户态程序需要先获取 ring 结构中各成员的准确便宜，才能在mmap() 之后准确识别内存中各成员位置。 \n```\n        err = xsk_get_mmap_offsets(umem->fd, &off);\n        if (err) {\n                err = -errno;\n                goto out_socket;\n        }\n```\nxsk_get_mmap_offsets() 函数主要是通过getsockopt函数实现这一功能：\n```\n        err = getsockopt(fd, SOL_XDP, XDP_MMAP_OFFSETS, off, &optlen);\n        if (err)\n                return err;\n```\n一切就绪，开始将内核中的 FILL RING 映射到用户态程序中：\n```\n        map = mmap(NULL, off.fr.desc + umem->config.fill_size * sizeof(__u64),\n                   PROT_READ | PROT_WRITE, MAP_SHARED | MAP_POPULATE, umem->fd,\n                   XDP_UMEM_PGOFF_FILL_RING);\n        if (map == MAP_FAILED) {\n                err = -errno;\n                goto out_socket;\n        }\n\n        umem->fill = fill;\n        fill->mask = umem->config.fill_size - 1;\n        fill->size = umem->config.fill_size;\n        fill->producer = map + off.fr.producer;\n        fill->consumer = map + off.fr.consumer;\n        fill->flags = map + off.fr.flags;\n        fill->ring = map + off.fr.desc;\n        fill->cached_cons = umem->config.fill_size;\n```\n上面代码需要关注的一点是 mmap() 函数中指定内存的长度——**off.fr.desc + umem->config.fill_size * sizeof(\\_\\_u64)**，umem->config.fill_size * sizeof(\\_\\_u64)没什么好说的，就是ring数组的长度，而 off.fr.desc 则是ring结构体的长度，我们先看下内核中ring结构的定义：\n```\nstruct xdp_ring_offset {\n        __u64 producer;\n        __u64 consumer;\n        __u64 desc;\n};\n```\n这是没有flag的定义，无伤大雅。这里desc的地址其实就是ring数组的起始地址了。而off.fr.desc是desc相对 ring 结构体起始地址的偏移，相当于结构体长度。我们用一张图来看下ring所在内存的结构分布：\n\n![enter description here](https://rexrock.github.io/post-images/1614236273468.png)\n\n后面一堆赋值代码没什么好讲的，umem->fill 是用户态程序自定义的一个结构体，其成员 producer、consumer、flags、ring都是指针，分别指向实际ring结构中的对应成员，umem->fill中的其他成员主要在后面报文收发时用到，起辅助作用。\n\n### 1.6 将COMPLETION RING 映射到用户态\n跟上面 FILL RING 的映射一样，只贴代码好了：\n\n```\n        map = mmap(NULL, off.cr.desc + umem->config.comp_size * sizeof(__u64),\n                   PROT_READ | PROT_WRITE, MAP_SHARED | MAP_POPULATE, umem->fd,\n                   XDP_UMEM_PGOFF_COMPLETION_RING);\n        if (map == MAP_FAILED) {\n                err = -errno;\n                goto out_mmap;\n        }\n\n        umem->comp = comp;\n        comp->mask = umem->config.comp_size - 1;\n        comp->size = umem->config.comp_size;\n        comp->producer = map + off.cr.producer;\n        comp->consumer = map + off.cr.consumer;\n        comp->flags = map + off.cr.flags;\n        comp->ring = map + off.cr.desc;\n```\n\n### 1.7 创建RX RING和TX RING然后mmap\n\n这里和 FILL RING 以及 COMPLETION RING的做法基本完全一致，只贴代码：\n\n```\n        if (rx) {\n                err = setsockopt(xsk->fd, SOL_XDP, XDP_RX_RING,\n                                 &xsk->config.rx_size,\n                                 sizeof(xsk->config.rx_size));\n                if (err) {\n                        err = -errno;\n                        goto out_socket;\n                }\n        }\n        if (tx) {\n                err = setsockopt(xsk->fd, SOL_XDP, XDP_TX_RING,\n                                 &xsk->config.tx_size,\n                                 sizeof(xsk->config.tx_size));\n                if (err) {\n                        err = -errno;\n                        goto out_socket;\n                }\n        }\n\n        err = xsk_get_mmap_offsets(xsk->fd, &off);\n        if (err) {\n                err = -errno;\n                goto out_socket;\n        }\n\n        if (rx) {\n                rx_map = mmap(NULL, off.rx.desc +\n                              xsk->config.rx_size * sizeof(struct xdp_desc),\n                              PROT_READ | PROT_WRITE, MAP_SHARED | MAP_POPULATE,\n                              xsk->fd, XDP_PGOFF_RX_RING);\n                if (rx_map == MAP_FAILED) {\n                        err = -errno;\n                        goto out_socket;\n                }\n\n                rx->mask = xsk->config.rx_size - 1;\n                rx->size = xsk->config.rx_size;\n                rx->producer = rx_map + off.rx.producer;\n                rx->consumer = rx_map + off.rx.consumer;\n                rx->flags = rx_map + off.rx.flags;\n                rx->ring = rx_map + off.rx.desc;\n        }\n        xsk->rx = rx;\n\n        if (tx) {\n                tx_map = mmap(NULL, off.tx.desc +\n                              xsk->config.tx_size * sizeof(struct xdp_desc),\n                              PROT_READ | PROT_WRITE, MAP_SHARED | MAP_POPULATE,\n                              xsk->fd, XDP_PGOFF_TX_RING);\n                if (tx_map == MAP_FAILED) {\n                        err = -errno;\n                        goto out_mmap_rx;\n                }\n\n                tx->mask = xsk->config.tx_size - 1;\n                tx->size = xsk->config.tx_size;\n                tx->producer = tx_map + off.tx.producer;\n                tx->consumer = tx_map + off.tx.consumer;\n                tx->flags = tx_map + off.tx.flags;\n                tx->ring = tx_map + off.tx.desc;\n                tx->cached_cons = xsk->config.tx_size;\n        }\n        xsk->tx = tx;\n```\n\n### 1.8 调用bind()将AF_XDP socket绑定的指定设备的某一队列\n\n```\n        sxdp.sxdp_family = PF_XDP;\n        sxdp.sxdp_ifindex = xsk->ifindex;\n        sxdp.sxdp_queue_id = xsk->queue_id;\n        sxdp.sxdp_flags = xsk->config.bind_flags;\n\n        err = bind(xsk->fd, (struct sockaddr *)&sxdp, sizeof(sxdp));\n        if (err) {\n                err = -errno;\n                goto out_mmap_tx;\n        }\n```\n\n## 2. 内核态程序\n\n相比用户态程序的一堆操作，内核态XDP程序看起来要简单的多。\n\n在[XDP技术简介](xdp1.md)我们曾介绍过，XDP程序利用 bpf_reditrct() 函数可以将报文重定向到其他设备发送出去或者重定向到其他CPU继续处理，后来又发展出了bpf_redirect_map()函数，可以将重定向的目的地保存在map中。AF_XDP 正是利用了 bpf_redirect_map() 函数以及 BPF_MAP_TYPE_XSKMAP 类型的 map 实现将报文重定向到用户态程序。\n\n### 2.1 创建BPF_MAP_TYPE_XSKMAP类型的map\n该类型map的key是网口设备的queue_id，value则是该queue上绑定的AF_XDP socket fd，所以通常需要为每个网口设备各自创建独立的map，并在用户态将对应的queue_id->xsk_fd存储到map中。\n\n```\nstatic int xsk_create_bpf_maps(struct xsk_socket *xsk)\n{\n        int max_queues;\n        int fd;\n\n        max_queues = xsk_get_max_queues(xsk);\n        if (max_queues < 0)\n                return max_queues;\n\n        fd = bpf_create_map_name(BPF_MAP_TYPE_XSKMAP, \"xsks_map\",\n                                 sizeof(int), sizeof(int), max_queues, 0);\n        if (fd < 0)\n                return fd;\n\n        xsk->xsks_map_fd = fd;\n\n        return 0;\n}\n```\nbpf_create_map_name参数详解：\n\n - BPF_MAP_TYPE_XSKMAP，map类型\n - \"xsks_map\"，map的名字\n - sizeof(int)，分别指定key和vlue的size\n - max_queues，map大小\n - 0, map_flags\n\n### 2.2 XDP程序代码\n\n```\n        /* This is the C-program:\n         * SEC(\"xdp_sock\") int xdp_sock_prog(struct xdp_md *ctx)\n         * {\n         *     int index = ctx->rx_queue_index;\n         *\n         *     // A set entry here means that the correspnding queue_id\n         *     // has an active AF_XDP socket bound to it.\n         *     if (bpf_map_lookup_elem(&xsks_map, &index))\n         *         return bpf_redirect_map(&xsks_map, index, 0);\n         *\n         *     return XDP_PASS;\n         * }\n         */\n```\n是不是非常的简单，真正的redirect操作只有一行代码。\t\t \n\n### 2.3 XDP程序的加载\n\n```\nstatic int xsk_load_xdp_prog(struct xsk_socket *xsk)\n{\n        static const int log_buf_size = 16 * 1024;\n        char log_buf[log_buf_size];\n        int err, prog_fd;\n\n        /* This is the C-program:\n         * SEC(\"xdp_sock\") int xdp_sock_prog(struct xdp_md *ctx)\n         * {\n         *     int index = ctx->rx_queue_index;\n         *\n         *     // A set entry here means that the correspnding queue_id\n         *     // has an active AF_XDP socket bound to it.\n         *     if (bpf_map_lookup_elem(&xsks_map, &index))\n         *         return bpf_redirect_map(&xsks_map, index, 0);\n         *\n         *     return XDP_PASS;\n         * }\n         */\n        struct bpf_insn prog[] = {\n                /* r1 = *(u32 *)(r1 + 16) */\n                BPF_LDX_MEM(BPF_W, BPF_REG_1, BPF_REG_1, 16),\n                /* *(u32 *)(r10 - 4) = r1 */\n                BPF_STX_MEM(BPF_W, BPF_REG_10, BPF_REG_1, -4),\n                BPF_MOV64_REG(BPF_REG_2, BPF_REG_10),\n                BPF_ALU64_IMM(BPF_ADD, BPF_REG_2, -4),\n                BPF_LD_MAP_FD(BPF_REG_1, xsk->xsks_map_fd),\n                BPF_EMIT_CALL(BPF_FUNC_map_lookup_elem),\n                BPF_MOV64_REG(BPF_REG_1, BPF_REG_0),\n                BPF_MOV32_IMM(BPF_REG_0, 2),\n                /* if r1 == 0 goto +5 */\n                BPF_JMP_IMM(BPF_JEQ, BPF_REG_1, 0, 5),\n                /* r2 = *(u32 *)(r10 - 4) */\n\t\t\t\t                BPF_LD_MAP_FD(BPF_REG_1, xsk->xsks_map_fd),\n                BPF_LDX_MEM(BPF_W, BPF_REG_2, BPF_REG_10, -4),\n                BPF_MOV32_IMM(BPF_REG_3, 0),\n                BPF_EMIT_CALL(BPF_FUNC_redirect_map),\n                /* The jumps are to this instruction */\n                BPF_EXIT_INSN(),\n        };\n        size_t insns_cnt = sizeof(prog) / sizeof(struct bpf_insn);\n\n        prog_fd = bpf_load_program(BPF_PROG_TYPE_XDP, prog, insns_cnt,\n                                   \"LGPL-2.1 or BSD-2-Clause\", 0, log_buf,\n                                   log_buf_size);\n        if (prog_fd < 0) {\n                pr_warning(\"BPF log buffer:\\n%s\", log_buf);\n                return prog_fd;\n        }\n\n        err = bpf_set_link_xdp_fd(xsk->ifindex, prog_fd, xsk->config.xdp_flags);\n        if (err) {\n                close(prog_fd);\n                return err;\n        }\n\n        xsk->prog_fd = prog_fd;\n        return 0;\n}\n```\n**XDP程序的load**\n\n调用函数 bpf_load_program() 之前的代码不用关心。通常 eBPF 程序使用 C 语言的一个子集（restricted C）编写，然后通过 LLVM 编译成字节码注入到内核执行。由于本例中XDP程序代码比较简单，功力深厚的作者直接将其编写为 eBPF（JIT）可识别的字节码，然后直接调用 bpf_load_program() 函数将字节码程序加载到内核中。\n\n**XDP程序的attach**\n\nXDP程序加载成功会返回对应的fd（后面统称为prog_fd），但是此时XDP程序还不会被执行（所有的eBPF都需要经过load和attach两步才能被触发执行，load只是将程序加载到内核中，attach将程序添加到hook点后，程序才能真正被触发执行）。我们调用函数 bpf_set_link_xdp_fd() 函数将XDP程序attach到指定网口设备的驱动中的hook点。\n\n> **注意：** AF_XDP socket是跟指定网口设备的队列绑定，而XDP程序则是跟指定的网口设备绑定（attach）。\n\n## 3. 回到用户态，让程序run起来\n\n经过前面两步，AF_XDP socket、UMEM、FILL/COMPLETION/RX/TX RING 都创建设置好了，XSKMAP 和XDP PROG 也都加载好了。但是要想让XDP程序把报文传到用户态程序，我们还得再进行两补操作。\n\n### 3.1 将AF_XDP socket存储到XSKMAP中\n\n前面介绍XSKMAP的时候，大家应该都想到这一步了，所以只贴代码不说话：\n\n```\nstatic int xsk_set_bpf_maps(struct xsk_socket *xsk)\n{\n        return bpf_map_update_elem(xsk->xsks_map_fd, &xsk->queue_id,\n                                   &xsk->fd, 0);\n}\n```\n\n### 3.2 标题先卖个关子\n前面我们介绍过4种ring，分别对应收发包两个场景（收包：FILL/RX ring，发包：TX/COMPLETION RING）,我画个图分别描述一下收发包场景。\n\n#### 3.2.1 先看收包\n\n![收包](https://rexrock.github.io/post-images/1614242674670.png)\n收包过程是由XDP程序触发的，但是XDP程序收包，需要依赖用户态程序填充FILL RING，将可以承载报文的desc告诉XDP程序。所以在用户态程序初始化阶段，我们需要先填充FILL RING，直接看代码：\n\n```\n        ret = xsk_ring_prod__reserve(&xsk->umem->fq,\n                                     XSK_RING_PROD__DEFAULT_NUM_DESCS,\n                                     &idx);\n        if (ret != XSK_RING_PROD__DEFAULT_NUM_DESCS)\n                exit_with_error(-ret);\n        for (i = 0; i < XSK_RING_PROD__DEFAULT_NUM_DESCS; i++)\n                *xsk_ring_prod__fill_addr(&xsk->umem->fq, idx++) =\n                        i * opt_xsk_frame_size;\n        xsk_ring_prod__submit(&xsk->umem->fq,\n                              XSK_RING_PROD__DEFAULT_NUM_DESCS);\n```\n三个经过封装的函数，看起来不明觉厉，咱们一个一个看：\n\n**1. xsk_ring_prod__reserve**\n\n```\nstatic inline size_t xsk_ring_prod__reserve(struct xsk_ring_prod *prod,\n                                            size_t nb, __u32 *idx)\n{\n        if (xsk_prod_nb_free(prod, nb) < nb)\n                return 0;\n\n        *idx = prod->cached_prod;\n        prod->cached_prod += nb;\n\n        return nb;\n}\n```\n这个函数前面先判断一下：我现在想生产nb个数据，ring里有没有足够的地方放啊？没有的话直接退出，等会再试试。\n\n> vhostuser里再这块有个BUG，前端程序想发包发现ring里空间不够了，而后端驱动处理又由于有有问题的判断，导致报文已发的报文一直不被处理，结果造成死锁，以后别的文章中再介绍吧。\n\n如果有足够的空间，那么会将生产者当前下标（cached_prog）赋值给idx，因为退出函数后会根据从这个idx指向的位置开始生产desc，最后cached_prod + nb。\n\n**为什么要有个cached_prog呢？**\n\n因为生产数据这个过程需要分几步完成，所以这个东西应该为了多线程同步吧。\n\n**2. xsk_ring_prod__fill_addr**\n\n```\nstatic inline __u64 *xsk_ring_prod__fill_addr(struct xsk_ring_prod *fill,\n                                              __u32 idx)\n{\n        __u64 *addrs = (__u64 *)fill->ring;\n\n        return &addrs[idx & fill->mask];\n}\n```\n看这段代码前，我们先看下ring中元素xdp_desc的成员结构：\n\n```\nstruct xdp_desc {\n        __u64 addr;\n        __u32 len;\n        __u32 options;\n};\n```\n**成员解析**\n - addr指向UMEM中某个帧的具体位置，并且不是真正的虚拟内存地址，而是相对UMEM内存起始地址的偏移。\n - len则是指报文的具体的长度，当XDP程序向desc填充报文的时候需要设置len，但是用户态程序向FILL RING中填充desc则不用关心len。\n\n\n所以上面xsk_ring_prod__fill_addr的功能就好理解了，返回的ring中下标为idx处的desc中addr的指针；并且在函数返回后对addr进行了赋值，再看下这块代码，可以看到赋值给addr是个偏移量：\n\n```\n        for (i = 0; i < XSK_RING_PROD__DEFAULT_NUM_DESCS; i++)\n                *xsk_ring_prod__fill_addr(&xsk->umem->fq, idx++) =\n                        i * opt_xsk_frame_size;\n```\n\n  3. xsk_ring_prod__submit\n\n```\nstatic inline void xsk_ring_prod__submit(struct xsk_ring_prod *prod, size_t nb)\n{\n        /* Make sure everything has been written to the ring before indicating\n         * this to the kernel by writing the producer pointer.\n         */\n        libbpf_smp_wmb();\n\n        *prod->producer += nb;\n}\n```\n数据填充完毕，更新生产者下标。\n\n> 说明：下标永远指向下一个可填充数据位置。\n\n\n#### 3.2.2 再看发包\n\n![发包](https://rexrock.github.io/post-images/1614243219906.png)\n\n发包真的没啥好说的。初始化的时候不用管，想发包的时候直接就发啦。\n\n## 4. 收包流程解析\n\n![收包](https://rexrock.github.io/post-images/1614242674670.png)\nAF_XDP socket毕竟也是socket，所以select/poll/epoll这些函数都能用的，怎么用这里不介绍了。\n\n我们只看具体从一个AF_XDP socket收包的过程:\n\n```\nstatic void rx_drop(struct xsk_socket_info *xsk, struct pollfd *fds)\n{\n        unsigned int rcvd, i;\n        u32 idx_rx = 0, idx_fq = 0;\n        int ret;\n\n        rcvd = xsk_ring_cons__peek(&xsk->rx, BATCH_SIZE, &idx_rx);\n        if (!rcvd) {\n                if (xsk_ring_prod__needs_wakeup(&xsk->umem->fq))\n                        ret = poll(fds, num_socks, opt_timeout);\n                return;\n        }\n\n        ret = xsk_ring_prod__reserve(&xsk->umem->fq, rcvd, &idx_fq);\n        while (ret != rcvd) {\n                if (ret < 0)\n                        exit_with_error(-ret);\n                if (xsk_ring_prod__needs_wakeup(&xsk->umem->fq))\n                        ret = poll(fds, num_socks, opt_timeout);\n                ret = xsk_ring_prod__reserve(&xsk->umem->fq, rcvd, &idx_fq);\n        }\n\n        for (i = 0; i < rcvd; i++) {\n                u64 addr = xsk_ring_cons__rx_desc(&xsk->rx, idx_rx)->addr;\n                u32 len = xsk_ring_cons__rx_desc(&xsk->rx, idx_rx++)->len;\n                u64 orig = xsk_umem__extract_addr(addr);\n\n                addr = xsk_umem__add_offset_to_addr(addr);\n                char *pkt = xsk_umem__get_data(xsk->umem->buffer, addr);\n\n                hex_dump(pkt, len, addr);\n                *xsk_ring_prod__fill_addr(&xsk->umem->fq, idx_fq++) = orig;\n        }\n\n        xsk_ring_prod__submit(&xsk->umem->fq, rcvd);\n        xsk_ring_cons__release(&xsk->rx, rcvd);\n        xsk->rx_npkts += rcvd;\n}\n```\n该函数并没有对报文做什么复杂处理，只是hex_dump了一下，整个收发包分五个步骤：\n\n**1. xsk_ring_cons__peek()**\n\n开始对RX RING进行消费，返回消费者下标和消费个数，并累加cached_cons；\n\n**2. xsk_ring_prod__reserve**\n\n开始对FILL RING进行生产，返回生产者下标和生产个数，并累加cached_prod;\n\n**3. 报文处理**\n\n处理从RX RING中收到的报文，并回填到FILL RING中；\n```\n        for (i = 0; i < rcvd; i++) {\n                u64 addr = xsk_ring_cons__rx_desc(&xsk->rx, idx_rx)->addr;\n                u32 len = xsk_ring_cons__rx_desc(&xsk->rx, idx_rx++)->len;\n                u64 orig = xsk_umem__extract_addr(addr);\n\n                addr = xsk_umem__add_offset_to_addr(addr);\n                char *pkt = xsk_umem__get_data(xsk->umem->buffer, addr);\n\n                hex_dump(pkt, len, addr);\n                *xsk_ring_prod__fill_addr(&xsk->umem->fq, idx_fq++) = orig;\n        }\n```\n从desc中读取addr，并通过 xsk_umem__get_data() 函数得到报文真正的虚拟地址，然后 hex_dump()下。\n```\nstatic inline void *xsk_umem__get_data(void *umem_area, __u64 addr)\n{\n        return &((char *)umem_area)[addr];\n}\n```\n然后将处理完报文所在的 UMEM 帧回填到FILL RING中：\n\n```\n*xsk_ring_prod__fill_addr(&xsk->umem->fq, idx_fq++) = orig;\n```\n**4. xsk_ring_prod__submit(&xsk->umem->fq, rcvd)**\n\n完成对RX RING的消费，更新消费者下标；\n\n**5. xsk_ring_cons__release(&xsk->rx, rcvd)**\n\n完成对FILL RING的生产，更新生产者下标；\n\n## 5. 结语\n关于AF_XDP的使用及背后原理暂且分析到这，目前AF_XDP已经在ovs、dpdk、cilium中应用，相应的文档下面有链接。如有错误纰漏，欢迎大家拍砖。\n\n**相关代码均出自kernel：**\n```\nsamples/bpf/xdpsock_user.c\ntools/lib/bpf/xsk.c\ntools/lib/bpf/xsk.h\nnet/xdp/xsk.c\nnet/xdp/xsk.h\nusr/include/linux/if_xdp.h\n```\n**相关参考文档如下：**\n\n[Kernel document for AF_XDP](https://www.kernel.org/doc/html/latest/networking/af_xdp.html)\n\n[Man for bpf](https://man7.org/linux/man-pages/man7/bpf-helpers.7.html)\n\n[Openvswitch and XDP](https://docs.openvswitch.org/en/latest/intro/install/afxdp/)\n\n[DPDK and XDP](http://doc.dpdk.org/guides/nics/af_xdp.html)\n\n[性能对比](https://www.dpdk.org/wp-content/uploads/sites/35/2019/07/14-AF_XDP-dpdk-summit-china-2019.pdf)\n\n[编译内核源码中的示例代码](https://cloud.tencent.com/developer/article/1644458)\n\n",
      "data": {
        "title": "AF_XDP技术详解",
        "date": "2021-03-03 12:31:00",
        "tags": [
          "eBPF",
          "XDP"
        ],
        "published": true,
        "hideInList": false,
        "feature": null,
        "isTop": false
      },
      "isEmpty": false,
      "excerpt": "",
      "abstract": "",
      "fileName": "af_xdp1"
    },
    {
      "content": "\n官方文档：\n\n[https://istio.io/latest/zh/docs/setup/getting-started/](https://istio.io/latest/zh/docs/setup/getting-started/)\n\n## 1\\. 下载istio安装包\n\n[https://github.com/istio/istio/releases/tag/1.7.0](https://github.com/istio/istio/releases/tag/1.7.0)\n\n## 2\\. 通过istioctl安装\n\n```\nistioctl install\n```\n\n## 3\\. Sidecar注入\n\n官方文档\n[https://istio.io/latest/docs/setup/additional-setup/sidecar-injection/](https://istio.io/latest/docs/setup/additional-setup/sidecar-injection/)\n\n### 3.1 手动注入\n\n```\nistioctl kube-inject -f samples/sleep/sleep.yaml | kubectl apply -f -\n```\n\n命令istioctl kube-inject会将创建sidecar需要的配置插入到sleep.yaml得配置中\n\n### 3.2 自动注入\n\n```\nkubectl label namespaces <nsName> istio-injection=enabled\n```\n\n这样在namespace nsName 中创建得pod会被自动注入sidecar\n\n### 3.3 自动注入不生效怎么办\n\n**原因一：准入控制器相关问题**\n\n参考：\n\n[https://istio.io/latest/zh/docs/ops/configuration/mesh/webhook/](https://istio.io/latest/zh/docs/ops/configuration/mesh/webhook/)\n\nIstio 使用 ValidatingAdmissionWebhooks 验证 Istio 配置，使用 MutatingAdmissionWebhooks 自动将 Sidecar 代理注入至用户 Pod。\n\n```\n# a) 首先判断当前使用的apiserver默认会开启哪些准入控制器（admission controllers）\nkube-apiserver -h | grep enable-admission-plugins\n# b) 如果没有ValidatingAdmissionWebhooks和MutatingAdmissionWebhooks，则需修改apiserver启动参数开启\n# 编辑/etc/kubernetes/manifests/kube-apiserver.yaml修改如下，然后重建\n# --enable-admission-plugins=NodeRestriction,ValidatingAdmissionWebhooks,MutatingAdmissionWebhooks\n# c) 如果已开启，那么需要确认名为istio-sidecar-injector的webhook是否存在\nkubectl get mutatingwebhookconfigurations.admissionregistration.k8s.io\n# d) 如果不存在，说明istio部署有问题，如果存在还不生效，需要看一下istio-sidecar-injector的配置\nkubectl get mutatingwebhookconfigurations.admissionregistration.k8s.io istio-sidecar-injector -o yaml\n# e) 主要查看matchLabels字段，可以自动注入的labek并不是istio-injection=enabled\n```\n\n## 4\\. 开启both sidecar\n\n为接收端配置containerPort即可开启both sidecar，可以对比下开启both sidecar前后iptables的差别：\n\n```\n--- iptables.1  2020-09-09 17:56:26.452129512 +0800\n+++ iptables.2  2020-09-09 17:56:18.308026728 +0800\n@@ -1,5 +1,6 @@\n Chain PREROUTING (policy ACCEPT)\n target     prot opt source               destination\n# 首先在PREROUTING链这里将所有的流量导向ISTIO_INBOUND\n+ISTIO_INBOUND  tcp  --  0.0.0.0/0            0.0.0.0/0\n\n Chain INPUT (policy ACCEPT)\n target     prot opt source               destination\n@@ -11,7 +12,11 @@\n Chain POSTROUTING (policy ACCEPT)\n target     prot opt source               destination\n\n-Chain ISTIO_IN_REDIRECT (1 references)\n+Chain ISTIO_INBOUND (1 references)\n+target     prot opt source               destination\n# 然后在ISTIO_INBOUND中将访问containerPort端口的流量导向ISTIO_IN_REDIRECT\n# ISTIO_IN_REDIRECT和ISTIO_REDIRECT规则一样，都是将流量重定向到本地15001端口\n+ISTIO_IN_REDIRECT  tcp  --  0.0.0.0/0            0.0.0.0/0            tcp dpt:8088\n+\n+Chain ISTIO_IN_REDIRECT (2 references)\n target     prot opt source               destination\n REDIRECT   tcp  --  0.0.0.0/0            0.0.0.0/0            redir ports 15001\n```\n",
      "data": {
        "title": "使用istio + servicemesh搭建服务网格",
        "date": "2021-03-03 11:33:00",
        "tags": [
          "K8S"
        ],
        "published": true,
        "hideInList": false,
        "feature": null,
        "isTop": false
      },
      "isEmpty": false,
      "excerpt": "",
      "abstract": "",
      "fileName": "k8s3"
    },
    {
      "content": "## 1. 安装kubebuilder\r\n\r\n建议源码安装更容易些：\r\n\r\n```\r\ngit clone https://github.com/kubernetes-sigs/kubebuilder.git\r\ncd kubebuilder/\r\nmake install\r\n./bin/kubebuilder version\r\n```\r\n\r\n将./bin/kubebuilde拷贝到可执行路径\r\n\r\n## 2. 使用kubebuilder创建CRD及Controller\r\n\r\n官方参考文档：\r\n\r\n[https://book-v1.book.kubebuilder.io/quick\\_start.html](https://book-v1.book.kubebuilder.io/quick_start.html)\r\n\r\n其他参考文档：\r\n\r\n[https://www.cnblogs.com/alisystemsoftware/p/11580202.html](https://www.cnblogs.com/alisystemsoftware/p/11580202.html)\r\n\r\n[https://blog.csdn.net/qianggezhishen/article/details/106995181](https://blog.csdn.net/qianggezhishen/article/details/106995181)\r\n\r\n### 2.1 准备工作\r\n\r\n```\r\nmkdir -p testProject/src/testController\r\ncd testProject/\r\nexport PATH=$PATH:/root/go/bin/\r\nexport GOPATH=$PWD\r\n# 强制启用go module\r\nexport GO111MODULE=on\r\n# 配置默认从goproxy.io拉去go mod的依赖包，goproxy.io是国内七牛云维护的一个golang包代理库\r\nexport GOPROXY=https://goproxy.io\r\ncd src/testController\r\n```\r\n\r\n### 2.2 创建PROJECT\r\n\r\n~~~\r\nkubebuilder init --domain test1.test2 --license apache2 --owner \"The Kubernetes Authors\"\r\ncat PROJECT\r\n~~~\r\n\r\n![enter description here](https://rexrock.github.io/post-images/1614304636185.png)\r\n\r\n### 2.3 创建API\r\n\r\n~~~\r\nkubebuilder create api --group apps --version v1 --kind Test\r\ncat config/samples/apps_v1_test.yaml\r\n~~~\r\n\r\n![enter description here](https://rexrock.github.io/post-images/1614304670523.png)\r\n\r\n### 2.4 Install the CRDs into the cluster\r\n\r\n```\r\nmake install\r\n```\r\n\r\n### 2.5 在本地运行Controller\r\n\r\n```\r\nmake run\r\n```\r\n\r\n![enter description here](https://rexrock.github.io/post-images/1614304707772.png)\r\n\r\n### 2.6 创建Resource of CRD\r\n\r\n```\r\nkubectl create -f config/samples/apps_v1_test.yaml\r\n```\r\n\r\n可以看到controller的日志打印如下\r\n\r\n![enter description here](https://rexrock.github.io/post-images/1614304738769.png)\r\n\r\n### 2.7 编译打包\r\n\r\n首先修改Dockerfile：\r\n*1）在RUN go mod download前插入一行ENV GOPROXY=https://goproxy.cn,direct*\r\n*2）将FROM *[*gcr.io/distroless/static:nonroot改为FROM*](http://gcr.io/distroless/static:nonroot%E6%94%B9%E4%B8%BAFROM)* golang:1.13*\r\n*3）删除USER nonroot:nonroot*\r\n然后运行：\r\n\r\n```\r\ndocker build -t test-controller .\r\n```\r\n\r\n成功会看到\r\n\r\n![enter description here](https://rexrock.github.io/post-images/1614304772724.png)\r\n\r\n可以通过docker images命令查看镜像，或者通过docker save和docker load导出和导入。\r\n\r\n## 3. 创建Core Resource的controller\r\n\r\n参考：\r\n\r\n[https://book-v1.book.kubebuilder.io/beyond\\_basics/controllers\\_for\\_core\\_resources.html](https://book-v1.book.kubebuilder.io/beyond_basics/controllers_for_core_resources.html)\r\n\r\n1. 创建后需要修改controllers/deployment\\_controller.go文件：\r\n    \r\n       import添加：corev1 \"[k8s.io/api/core/v1](http://k8s.io/api/core/v1)\"\r\n\t   \r\n       import删除：appsv1 \"testController/api/v1\"\r\n\r\n2. 修改函数SetupWithManager：\r\n         \r\n\t   将For(&appsv1.Test{})改为For(&corev1.Pod{})\r\n\t   \r\n       然后重新make run，则会收到pod创建、删除的event消息\r\n",
      "data": {
        "title": "使用kubebuilder创建CRD及Controller",
        "date": "2021-03-03 11:32:00",
        "tags": [
          "K8S"
        ],
        "published": true,
        "hideInList": false,
        "feature": null,
        "isTop": false
      },
      "isEmpty": false,
      "excerpt": "",
      "abstract": "",
      "fileName": "k8s2"
    },
    {
      "content": "官方文档：\r\n\r\n[Installing kubeadm](https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/)\r\n\r\n[Creating a cluster with kubeadm](https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/)\r\n\r\n参考：\r\n\r\n[使用kubeadm安装Kubernetes 1.11](https://zhuanlan.zhihu.com/p/40931670)\r\n\r\n[Kubernetes kubectl run 命令详解](http://docs.kubernetes.org.cn/468.html)\r\n\r\n[k8s的API手册](https://godoc.org/k8s.io/api/core/v1#PodStatus)\r\n\r\n## 1\\. 安装docker\r\n\r\n```\r\ncurl -sSL https://get.docker.com | sh\r\ncat > /etc/docker/daemon.json <\r\n{\r\n\"registry-mirrors\": [\"https://dic5s40p.mirror.aliyuncs.com\"]\r\n}\r\nEOF\r\n```\r\n\r\n然后在/etc/default/docker中添加:\r\n\r\n```\r\nDOCKER\\_OPTS=\"--registry-mirror=https://dic5s40p.mirror.aliyuncs.com\"\r\n```\r\n\r\n然后执行：\r\n\r\n```\r\nsystemctl restart docker\r\n```\r\n\r\n## 2\\. 安装kubeadm、kubelet、kubectl\r\n\r\n[https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/](https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/)\r\n\r\n```\r\n#!/bin/bash\r\nset -e\r\napt-get -y install apt-transport-https ca-certificates curl software-properties-common\r\ncurl -s https://mirrors.aliyun.com/kubernetes/apt/doc/apt-key.gpg | apt-key add -\r\nadd-apt-repository \\\r\n\"deb http://mirrors.ustc.edu.cn/kubernetes/apt \\\r\nkubernetes-xenial \\\r\nmain\"\r\napt-get update\r\napt-get install -y kubelet kubeadm kubectl\r\nsystemctl enable kubelet && systemctl start kubelet\r\n```\r\n\r\n## 3\\. 初始化master节点\r\n\r\n```\r\nkubeadm init --pod-network-cidr=10.17.0.0/16 --service-cidr=10.18.200.0/24 --kubernetes-version=v1.18.5 --image-repository registry.cn-hangzhou.aliyuncs.com/google\\_containers\r\n```\r\n\r\n成功会显示\r\n\r\n```\r\nkubeadm join 192.168.100.12:6443 --token yskexa.twu83wmh7n64oczk \\\r\n    --discovery-token-ca-cert-hash sha256:d6dcfecc04d8452875155de28dc229eb4f7842eb55e8f998cade89cc625a679e\r\n```\r\n\r\n## 4\\. 为了可以执行kubectl\r\n\r\n```\r\nrm -rf $HOME/.kube\r\nmkdir -p $HOME/.kube\r\ncp -i /etc/kubernetes/admin.conf $HOME/.kube/config\r\nchown $(id -u):$(id -g) $HOME/.kube/config\r\n```\r\n\r\n## 5\\. 安装pod network\r\n\r\n```\r\nwget https://docs.projectcalico.org/v3.14/manifests/calico.yaml\r\n# 什么都不要改，会自动检测出pod ip的范围\r\nkubectl create -f calico.yaml\r\n```\r\n\r\n## 6\\. 添加worker节点\r\n\r\n在worker节点上执行kubeadm init成功后返回的命令，即\r\n\r\n```\r\nkubeadm join 192.168.100.12:6443 --token 7u1jah.da6w4tilh0j5097w \\\r\n    --discovery-token-ca-cert-hash sha256:bcd0ce4354f2e8b794b830d7a14389b6a06e46e225486ece8218424a1744583f\r\n```\r\n\r\n注意：token的有效期只有24小时，我们可以用如下命令查看可用的token\r\n\r\n```\r\nkubeadm token list\r\n```\r\n\r\n如果为空，我们可以通过如下命令创建token\r\n\r\n```\r\nkubeadm token create\r\n```\r\n\r\n如果你连cert-hash也忘了，那么可以通过如下命令查看\r\n\r\n```\r\nopenssl x509 -pubkey -in /etc/kubernetes/pki/ca.crt | openssl rsa -pubin -outform der 2>/dev/null | \\\r\n   openssl dgst -sha256 -hex | sed 's/^.* //'\r\n```\r\n\r\n## 7\\. 删除worker节点\r\n\r\n```\r\nkubectl drain <node name> --delete-local-data --force --ignore-daemonsets\r\n# 下面三条命令在worker节点上运行\r\nkubeadm reset\r\niptables -F && iptables -t nat -F && iptables -t mangle -F && iptables -X\r\nipvsadm -C # 如果使用了ipvs\r\nkubectl delete node <node name>\r\n```\r\n\r\n## 8\\. 删除master节点\r\n\r\n```\r\n# 在master节点上运行\r\nkubeadm reset\r\n```\r\n\r\n## 9\\. 开启关闭dns\r\n\r\n```\r\nkubectl -n kube-system scale --replicas=0 deployment/coredns\r\nkubectl -n kube-system scale --replicas=1 deployment/coredns\r\n```\r\n\r\n## 10\\. 让master节点也可以调度pod\r\n\r\n~~~\r\nkubectl taint nodes --all node-role.kubernetes.io/master-\r\n~~~\r\n",
      "data": {
        "title": "使用kubeadm搭建一个k8s集群",
        "date": "2021-03-03 11:31:00",
        "tags": [
          "K8S"
        ],
        "published": true,
        "hideInList": false,
        "feature": null,
        "isTop": false
      },
      "isEmpty": false,
      "excerpt": "",
      "abstract": "",
      "fileName": "k8s1"
    },
    {
      "content": "\n## 1. XDP程序的运行位置\nXDP（eXpress Data Path）提供了一个内核态、高性能、可编程 BPF 包处理框架。这个框架在软件中最早可以处理包的位置（即网卡驱动收到包的 时刻）运行 BPF 程序。如下图所示：\n\n![XDP程序运行的位置](https://rexrock.github.io/post-images/1613889918890.png)\n\nNAPI poll 机制不断调用驱动实现的 poll 方法，后者处理 RX 队列内的包，并最终将包送到正确的程序，也就是我们所说的 XDP 程序。所以很明显这需要网卡驱动的支持，如果驱动支持 XDP ，那 XDP 程序将在 poll 机制内执行。如果不支持，那 XDP 程序将只能在更后面的位置被执行，即上图中的receive_skb中。这其中经历了哪些步骤呢？\n\n 1. 创建skb，如果不支持XDP，poll机制会将报文送给 clean_rx()，该函数会创建一个skb，并skb进行一些硬件校验何检查，然后较给 gro_receive() 函数；\n 2. 分片重组，GRO可以理解为LRO的软件实现，相比LRO只针对TCP报文，GRO可以处理更多其他类型的报文，总之在 gro_receive() 函数中，如果是分片报文则进行分片重组然后交给 receive_skb() 函数，如果不是分片报文，则直接交给 receive_skn() 函数进行处理；\n\n## 2. XDP的三种工作模式\n\n上面提到XDP程序可以运行在不同位置，每个位置即对应XDP的一种工作模式：\n\n - Native XDP，即运行在网卡驱动实现的的 poll() 函数中，需要网卡驱动的支持；\n - Generic XDP，即上面提到的如果网卡驱动不支持XDP，则可以运行在 receive_skb() 函数中；\n - Offloaded XDP，这种模式是指将XDP程序offload到网卡中，这需要网卡硬件的支持，JIT编译器将BPF代码翻译成网卡原生指令并在网卡上运行。\n\n## 3. XDP程序的返回码\nXDP程序执行结束后会返回一个结果，告诉调用者接下来如何处理这个包：\n\n - XDP_DROP，丢弃这个包，主要用于报文过滤的安全场景；\n - XDP_PASS，将这个包“交给/还给”内核，继续走正常的内核处理流程；\n - XDP_TX，从收到包的网卡上再将这个包发出去（即hairpin模式），主要用于负载均衡场景；\n - XDP_REDIRECT，何XDP_TX类似，但是是通过另外一个网卡将包发出去。除此之外还可以实现将报文重定向到其他的CPU处理，类似于XDP_PASS继续走内核处理流程，但是由其他的CPU处理，当前CPU继续处理后续的报文接收；\n - XDP_ABORTED，表示程序产生异常，行为类似XDP_DROP，但是会通过一个tracepoint打印日志义工追踪；\n\n下面是 Mellanox mlx5 驱动中关于XDP的处理，如果该函数返回 true，则说明报文被XDP处理了，不用再走内核协议栈了，如果返回 false 则创建SKB然后继续走内核协议栈：\n\n```\n/* returns true if packet was consumed by xdp */\nbool mlx5e_xdp_handle(struct mlx5e_rq *rq, struct mlx5e_dma_info *di,\n                      u32 *len, struct xdp_buff *xdp)\n{\n        struct bpf_prog *prog = rcu_dereference(rq->xdp_prog);\n        u32 act;\n        int err;\n\n        if (!prog)\n                return false;\n\n        act = bpf_prog_run_xdp(prog, xdp);\n        switch (act) {\n        case XDP_PASS:\n                *len = xdp->data_end - xdp->data;\n                return false;\n        case XDP_TX:\n                if (unlikely(!mlx5e_xmit_xdp_buff(rq->xdpsq, rq, di, xdp)))\n                        goto xdp_abort;\n                __set_bit(MLX5E_RQ_FLAG_XDP_XMIT, rq->flags); /* non-atomic */\n                return true;\n        case XDP_REDIRECT:\n                if (xdp->rxq->mem.type != MEM_TYPE_XSK_BUFF_POOL) {\n                        page_ref_sub(di->page, di->refcnt_bias);\n                        di->refcnt_bias = 0;\n                }\n                /* When XDP enabled then page-refcnt==1 here */\n                err = xdp_do_redirect(rq->netdev, xdp, prog);\n                if (unlikely(err))\n                        goto xdp_abort;\n                __set_bit(MLX5E_RQ_FLAG_XDP_XMIT, rq->flags);\n                __set_bit(MLX5E_RQ_FLAG_XDP_REDIRECT, rq->flags);\n                if (xdp->rxq->mem.type != MEM_TYPE_XSK_BUFF_POOL)\n                        mlx5e_page_dma_unmap(rq, di);\n                rq->stats->xdp_redirect++;\n                return true;\n        default:\n                bpf_warn_invalid_xdp_action(act);\n                fallthrough;\n        case XDP_ABORTED:\nxdp_abort:\n                trace_xdp_exception(rq->netdev, prog, act);\n                fallthrough;\n        case XDP_DROP:\n                rq->stats->xdp_drop++;\n                return true;\n        }\n}\n\n```\n\n**疑问？**\n如果我们相对报文执行 redirect，那么我们在BPF程序中需要执行 bpf_redirect() / bpf_redirect_map()，但是从上面的代码中看，从我们的BPF程序返回后，驱动程序也调用了一个叫做 xdp_do_redirect() 的函数。那么问题来了，报文的 redirect 到底是在什么时候执行的呢？答案后面揭晓。\n\n**接着分析：**\n\n``` drivers/net/ethernet/mellanox/mlx5/core/en/xdp.c:mlx5e_xdp_handle\n        case XDP_REDIRECT:\n                /* When XDP enabled then page-refcnt==1 here */\n                err = xdp_do_redirect(rq->netdev, &xdp, prog);\n                if (unlikely(err))\n                        goto xdp_abort;\n                __set_bit(MLX5E_RQ_FLAG_XDP_XMIT, rq->flags);\n                __set_bit(MLX5E_RQ_FLAG_XDP_REDIRECT, rq->flags);\n                if (!xsk)\n                        mlx5e_page_dma_unmap(rq, di);\n                rq->stats->xdp_redirect++;\n                return true;\n```\nXDP程序返回后，驱动会根据XDP程序的返回码去真正执行 action。我们以 XDP_REDIRECT 为例，继续跟踪 xdp_do_redirect() 函数：\n``` javascript\n// >> net/core/filter.c\nxdp_do_redirect(netdev, xdp_buff, xdp_prog) =>\nxdp_do_redirect_map(netdev, xdp_buff, xdp_prog, bpf_map, bpf_redirect_info) =>\n__bpf_tx_xdp_map(netdev, fwd, bpf_map, xdp_buff, index) =>\n// fwd即xdp_sock；\n\n// >> kernel/bpf/xskmap.c\n__xsk_map_redirect(bpf_map, xdp_buff, xdp_sock) =>\n\n// >> net/xdp/xsk.c\nxsk_rcv(xdp_sock, xdp_buff)\n__xsk_rcv(xdp_sock, xdp_buff, len)\n```\n\n我们主要看下 xsk_rck() 和 __xsk_rcv() 两个函数：\n``` xl\nint xsk_rcv(struct xdp_sock *xs, struct xdp_buff *xdp)\n{\n        u32 len;\n\n        if (!xsk_is_bound(xs))\n                return -EINVAL;\n        // AF_XDP技术详解中曾介绍过，AF_XDP socket是跟具体的网卡RX队列绑定的，这里再真正执行\n\t\t// 收包前做了依次判断(虽然XDP程序中也有判断，但毕竟不是强制的)\n        if (xs->dev != xdp->rxq->dev || xs->queue_id != xdp->rxq->queue_index)\n                return -EINVAL;\n\n        len = xdp->data_end - xdp->data;\n\n        return (xdp->rxq->mem.type == MEM_TYPE_ZERO_COPY) ?\n                __xsk_rcv_zc(xs, xdp, len) : __xsk_rcv(xs, xdp, len);\n}\n```\n\n``` javascript\nstatic int __xsk_rcv(struct xdp_sock *xs, struct xdp_buff *xdp, u32 len)\n{\n        u64 offset = xs->umem->headroom;\n        u64 addr, memcpy_addr;\n        void *from_buf;\n        u32 metalen;\n        int err;\n\n        // 从 FILL RING中获取可以承载报文数据的desc\n        if (!xskq_peek_addr(xs->umem->fq, &addr, xs->umem) ||\n            len > xs->umem->chunk_size_nohr - XDP_PACKET_HEADROOM) {\n                xs->rx_dropped++;\n                return -ENOSPC;\n        }\n\n        if (unlikely(xdp_data_meta_unsupported(xdp))) {\n                from_buf = xdp->data;\n                metalen = 0;\n        } else {\n                from_buf = xdp->data_meta;\n                metalen = xdp->data - xdp->data_meta;\n        }\n        // 执行报文数据的copy，该函数时非zero copy模式下的执行函数\n        memcpy_addr = xsk_umem_adjust_offset(xs->umem, addr, offset);\n        __xsk_rcv_memcpy(xs->umem, memcpy_addr, from_buf, len, metalen);\n\n        offset += metalen;\n        addr = xsk_umem_adjust_offset(xs->umem, addr, offset);\n\t\t// 插入到 RX RING中\n        err = xskq_produce_batch_desc(xs->rx, addr, len);\n        if (!err) {\n                xskq_discard_addr(xs->umem->fq);\n                xdp_return_buff(xdp);\n                return 0;\n        }\n\n        xs->rx_dropped++;\n        return err;\n}\n```\n\n**结论：**\nbpf_redirect() 和 bpf_redirect_map() 应该只是填充bpf_redirect_info结构（即redirect的target相关的数据），真正的redirect操作仍由驱动在 XDP程序返回后执行。\n\n``` javascript\n// >> include/linux/filter.h\nstruct bpf_redirect_info {\n        u32 flags;\n        u32 tgt_index;\n        void *tgt_value;\n        struct bpf_map *map;\n        struct bpf_map *map_to_flush;\n        u32 kern_flags;\n};\n// >> net/core/filter.c:\nint xdp_do_redirect(struct net_device *dev, struct xdp_buff *xdp,\n                    struct bpf_prog *xdp_prog)\n{\n        struct bpf_redirect_info *ri = this_cpu_ptr(&bpf_redirect_info);\n        struct bpf_map *map = READ_ONCE(ri->map);\n\n        if (likely(map))\n                return xdp_do_redirect_map(dev, xdp, xdp_prog, map, ri);\n\n        return xdp_do_redirect_slow(dev, xdp, xdp_prog, ri);\n}\n```\n\n分析的没错，bpf_redirect_map()函数定义如下：\n\n``` javascript\n// >> net/core/filter.c\nBPF_CALL_3(bpf_xdp_redirect_map, struct bpf_map *, map, u32, ifindex,\n           u64, flags)\n{\n        struct bpf_redirect_info *ri = this_cpu_ptr(&bpf_redirect_info);\n\n        /* Lower bits of the flags are used as return code on lookup failure */\n        if (unlikely(flags > XDP_TX))\n                return XDP_ABORTED;\n\n        ri->tgt_value = __xdp_map_lookup_elem(map, ifindex);\n        if (unlikely(!ri->tgt_value)) {\n                /* If the lookup fails we want to clear out the state in the\n                 * redirect_info struct completely, so that if an eBPF program\n                 * performs multiple lookups, the last one always takes\n                 * precedence.\n                 */\n                WRITE_ONCE(ri->map, NULL);\n                return flags;\n        }\n\n        ri->flags = flags;\n        ri->tgt_index = ifindex;\n        WRITE_ONCE(ri->map, map);\n\n        return XDP_REDIRECT;\n}\n```\n",
      "data": {
        "title": "XDP技术简介",
        "date": "2021-03-03 10:51:00",
        "tags": [
          "eBPF",
          "XDP"
        ],
        "published": true,
        "hideInList": false,
        "feature": null,
        "isTop": false
      },
      "isEmpty": false,
      "excerpt": "",
      "abstract": "",
      "fileName": "xdp1"
    },
    {
      "content": "## 1\\. TC的几个概念\n\n* 队列——qdisc(queueing discipline)，分为不可分类队列（classless qdisc）和可分类队列（classful qdisc）,一个qdisc会被分配一个主序列号，叫做句柄(handle)，然后把从序列号作为类的命名空间。句柄采用象10:一样的表达方式。习惯上，需要为有子类的QDisc显式地分配一个句柄。队列真正的实现QoS功能；\n* 类别——class，通过分类器将流量划分为不同的class，一个class对应一个qos对象（即一个具体的可以配置qos策略的子队列），添加class的时候需要指定该class对应的qos策略（就是给多少带宽这种）；\n* 分类器——filter，用于将流量划分为不同的class；\n\n**用一张图表示三者的关系，如下图所示：**\n\n![img](https://rexrock.github.io/post-images/tc_filter_class.jpg)\n\n## 2\\. 举个简单的TC例子\n\n### 2.1 创建队列\n\n有关队列的TC命令的一般形式为:\n\n```\ntc qdisc [add|change|replace|link] dev DEV [parent qdisk-id|root][handle qdisc-id] qdisc[qdisc specific parameters]\n```\n\n首先，需要为网卡eth0配置一个HTB队列，使用下列命令:\n\n```\ntc qdisc add dev eth0 root handle 1:htb default 11\n```\n\n**参数说明：**\n\n* add 表示要添加\n* dev eth0 表示要操作的网卡为eth0\n* root 表示为网卡eth0添加的是一个根队列\n* handle 1: 表示队列的句柄为1:\n* htb 表示要添加的队列为HTB队列\n* 命令最后的”default 11 是htb特有的队列参数，意思是所有未分类的流量都将分配给类别1:11\n\n### 2.2 创建类别\n\n有关类别的TC 命令的一般形式为:\n\n```\ntc class [add|change|replace] dev DEV parent qdisc-id [classid class-id] qdisc [qdisc specific parameters]\n```\n\n可以利用下面这三个命令为根队列1创建三个类别，分别是1:11、1:12和1:13，它们分别占用40、40和20mb\\[t的带宽。\n\n```\ntc class add dev eth0 parent 1: classid 1:11 htb rate 40mbit ceil 40mbit\ntc class add dev eth0 parent 1: classid 1:12 htb rate 40mbit ceil 40mbit\ntc class add dev eth0 parent 1: cllassid 1:13 htb rate 20mbit ceil 20mbit\n```\n\n**参数说明：**\n\n* parent 1: 表示类别的父亲为根队列1:\n* classid1:11 表示创建一个标识为1:11的类别\n* rate 40mbit 表示系统将为该类别确保带宽40mbit\n* ceil 40mbit 表示该类别的最高可占用带宽为40mbit\n\n**注意， 在TC 中使用下列的缩写表示相应的带宽：**\n\n* Kbps kiIobytes per second， 即”千字节每秒\n* Mbps megabytes per second， 即”兆字节每秒\n* Kbit kilobits per second，即”千比特每秒\n* Mbit megabits per second， 即”兆比特每秒\n\n### 2.3 创建分类器\n\n有关过滤器的TC 命令的一般形式为:\n\n```\ntc filter [add|change|replace] dev DEV [parent qdisc-id|root] protocol protocol prio priority filtertype [filtertype specific parameters] flowid flow-id\n```\n\n由于需要将WWW、E-mail、Telnet三种流量分配到三个类别，即上述1:11、1:12和1:13，因此，需要创建三个过滤器，如下面的三个命令:\n\n```\ntc filter add dev eth0 protocol ip parent 1:0 prio 1 u32 match ip dport 80 0xffff flowid 1:11\ntc filter add dev eth0 prtocol ip parent 1:0 prio 1 u32 match ip dport 25 0xffff flowid 1:12\ntc filter add dev eth0 protocol ip parent 1:0 prio 1 u32 match ip dport 23 oxffff flowid 1:13\n```\n\n**参数说明：**\n\n* protocol ip 表示该过滤器应该检查报文分组的协议字段\n* prio 1 表示它们对报文处理的优先级是相同的，对于不同优先级的过滤器， 系统将按照从小到大的优先级顺序来执行过滤器，对于相同的优先级，系统将按照命令的先后顺序执行。\n\n这几个过滤器还用到了u32选择器(命令中u32后面的部分)来匹配不同的数据流。以第一个命令为例，判断的是dport字段，如果该字段与Oxffff进行与操作的结果是80，则“flowid 1:11” 表示将把该数据流分配给类别1:11\n\n### 2.4 ingress qdisc\n\n* ingress qdisc没有任何参数，我们可以像下面这样添加一个ingress qdisc:\n\n~~~\ntc qdisc add dev eth0 ingress\n~~~\n\n* ingress qdisc不占用根队列，创建ingress qdisc后我们还能继续创建其他的tx的qdisc；\n* ingress qdisc不支持任何子类别，所以我们无法为ingress qdisc创建class，但是我们可以直接为ingress qdisc创建分类器；\n\n~~~\ntc qdisc add dev eth0 handle ffff: ingress \ntc filter add dev eth0 parent ffff: protocol all prio 49 basic police rate 10mbit burst 1mb mtu 65535 drop\n~~~\n\npolice参考：[https://man7.org/linux/man-pages/man8/tc-police.8.html](https://man7.org/linux/man-pages/man8/tc-police.8.html)\n\n**参数说明：**\n\n* rate 限制的最大流量？后面的drop动作是指超过限速的流量还是命中的流量？？？\n* burst 每个计时器的流量峰值，应该同HTB的burst\n* mtu 匹配的mtu\n* drop ？\n\n## 3\\. 如何使用tc的ebpf功能\n\n从内核4.1版本起，tc引入了一个特殊的qdisc，叫做clsact，它为TC提供了一个可以加载BPF程序的入口，使TC和XDP一样，成为一个可以加载BPF程序的网络钩子。\n\n![img](https://rexrock.github.io/post-images/ebpf_in_tc.jpg)\n\n**TC vs XDP**\n\n这两个钩子都可以用于相同的应用场景，如DDoS缓解、隧道、处理链路层信息等。但是，由于XDP在任何套接字缓冲区（SKB）分配之前运行，所以它可以达到比TC上的程序更高的吞吐量值。然而，后者可以从通过 struct \\_\\_sk\\_buff 提供的额外的解析数据中受益，并且可以执行 BPF 程序，对入站流量和出站流量都可以执行 BPF 程序，是 TX 链路上的能被操控的最后一个点。\n\n**无需网卡驱动的支持**\n\ntc BPF 程序不需要驱动做任何改动，因为它们运行在网络栈通用层中的 hook 点。因此，它们可以 attach 到任何类型的网络设备上。\n\n**Ingress**\n\n这提供了很好的灵活性，但跟运行在原生 XDP 层的程序相比，性能要差一些。然而，tc BPF 程序仍然是内核的通用 data path 做完 GRO 之后、且处理任何协议之前 最早的 处理点。传统的 iptables 防火墙也是在这里处理的，例如 iptables PREROUTING 或 nftables ingress hook 或其他数据包包处理过程。\n\n**Egress**\n\n类似的，对于 egress，tc BPF 程序在将包交给驱动之前的最晚的地方（latest point）执 行，这个地方在传统 iptables 防火墙 hook 之后（例如 iptables POSTROUTING）， 但在内核 GSO 引擎之前。\n\n\\*\\*详细参考：\\*\\*\n\n[http://arthurchiao.art/blog/cilium-bpf-xdp-reference-guide-zh/#prog\\_type\\_tc](http://arthurchiao.art/blog/cilium-bpf-xdp-reference-guide-zh/#prog_type_tc)\n\n## 4\\. 最佳实践\n\n参考：[https://github.com/rexrock/tc-xdp-drop-tcp](https://github.com/rexrock/tc-xdp-drop-tcp)\n\n注意：需使用4.20及以上版本的内核，才能使veth支持XDP\n",
      "data": {
        "title": "Run ebpf with tc",
        "date": "2021-03-03 10:32:00",
        "tags": [
          "eBPF",
          "TC"
        ],
        "published": true,
        "hideInList": false,
        "feature": null,
        "isTop": false
      },
      "isEmpty": false,
      "excerpt": "",
      "abstract": "",
      "fileName": "ebpf2"
    },
    {
      "content": "## 1. 不能使用循环语句\r\n\r\neBPF程序中不能使用循环语句，如果非要使用循环，则必须通过编译选项“`#pragma clang loop unroll(full)`”让编译器在编译过程中将循环展开。\r\n\r\n此外必须要注意的一点是，循环中的语句必须是单一且独立的块，如下：\r\n\r\n~~~\r\nstatic __always_inline int search_service_ip(int i, __u32 ip) {\r\n\t...\r\n\treturn 0;\r\n}\r\n\r\nstatic __always_inline int is_service_ip(__u32 ip) {\r\n#pragma clang loop unroll(full)\r\n\tfor(int i = 0; i < 32; i++) {\r\n\t\tswitch(search_service_ip(i, ip)) {\r\n\t\t\tcase 0:\r\n\t\t\t\tcontinue;\r\n\t\t\tcase 1:\r\n\t\t\t\treturn 1;\r\n\t\t\tdefault:\r\n\t\t\t\tbreak;\r\n\t\t}\r\n\t}\r\n\treturn 0;\r\n}\r\n~~~\r\n\r\n## 2. map操作的原子性\r\n\r\n* ebpf提供函数map\\_update\\_elem()对ebpf map表中的数据进行更新，该函数在对BPF\\_HASH表进行操作时是原子操作，对BPF\\_ARRAY操作时是非原子的。\r\n* 即使map\\_update\\_elem(）全部是原子操作，我们执行累加的流程是“ lookup elem -> elem++ -> update elem”，这一串操作也没办法保证原子性。\r\n\r\n**对ebpf map进行原子更新我们分内核态和用户态两种场景：**\r\n\r\n* 在内核态运行的ebpf程序可直接对ebpf map中的数据进行操作，加上bpf\\_map\\_lookup\\_elem()返回的是map中数据的指针。我们可以借助编译器原语（\\_\\_sync\\_fetch\\_and\\_add）在LLVM生成eBPF指令时，以原子方式对bpf\\_map\\_lookup\\_elem()返回的数据指针直接进行加减以此实现原子操作。（bcc的llvm编译器是内置的，所以已将该原语封装成lock\\_xadd()函数）\r\n* 目前用户态程序对ebpf map进行累加修改，还没有办法保证原子性，所以在程序设计阶段，务必保证不要让用户态程序和内核态程序同时对ebpf map进行类似累加修改的操作。\r\n",
      "data": {
        "title": "eBPF的使用限制",
        "date": "2021-03-03 10:31:00",
        "tags": [
          "eBPF"
        ],
        "published": true,
        "hideInList": false,
        "feature": null,
        "isTop": false
      },
      "isEmpty": false,
      "excerpt": "",
      "abstract": "",
      "fileName": "ebpf1"
    },
    {
      "content": "sriov-network-device-plugin需基于multus/danm和srioc-cni，所以我们依次安装multus、sriov-cni、sriov-network-device-plugin。\n\n# 1\\. 安装Multus\n\nMultus项目地址：\\[[https://github.com/intel/multus-cni.git](https://github.com/intel/multus-cni.git)\\]([https://github.com/intel/multus-cni.git](https://github.com/intel/multus-cni.git)\n\n```\ncd multus-cni-master\nkubectl create -f file://C:/Users/liyang07/Documents/Gridea/post-images/multus-daemonset.yml\n```\n\n**部署完成后：**\n\n* 每个node上都会运行一个multus的守护进程；\n* 获取当前“主cni”配置，并创建一个新的multus cni配置/etc/cni/net.d/00-multus.conf，以劫持cni\n\n**配置入口：**\n\n* 创建/etc/cni/net.d/multus.d，用来存储multus访问API server的验证文件；\n    \n**验证安装是否成功：**\n\n```\n# kubectl get pods --all-namespaces | grep -i multus\nkube-system   kube-multus-ds-amd64-4ncw6                     1/1     Running   0          17h\nkube-system   kube-multus-ds-amd64-jgzp4                     1/1     Running   2          24h\n```\n\nOK，让我们用macvlan口来验证一下Multus是否可以正常工作。\n\n```\napiVersion: \"k8s.cni.cncf.io/v1\"\nkind: NetworkAttachmentDefinition\nmetadata:\n  name: macvlan-conf\nspec:\n  config: '{\n      \"cniVersion\": \"0.3.0\",\n      \"type\": \"macvlan\",\n      \"master\": \"bond2.100\",\n      \"mode\": \"bridge\",\n      \"ipam\": {\n        \"type\": \"host-local\",\n        \"subnet\": \"192.168.1.0/24\",\n        \"rangeStart\": \"192.168.1.200\",\n        \"rangeEnd\": \"192.168.1.216\",\n        \"routes\": [\n          { \"dst\": \"0.0.0.0/0\" }\n        ],\n        \"gateway\": \"192.168.1.1\"\n      }\n    }'\n```\n\nMultus在部署的时候，顺便创建了一个CRD，用来让用户定义想要添加什么样的“副CNI”。上面的配置定义了我们想要给指定的pod添加基于macvlan的网口。下面让我来创建一个需要添加macvlan接口的pod：\n\n~~~\napiVersion: v1\nkind: Pod\nmetadata:\n  name: samplepod\n  annotations:\n    k8s.v1.cni.cncf.io/networks: macvlan-conf\nspec:\n  containers:\n  - name: samplepod\n    command: [\"/bin/ash\", \"-c\", \"trap : TERM INT; sleep infinity & wait\"]\n    image: alpine\n~~~\n\n**注意：**\n并不是所有的pod创建都会自动添加副接口，我们需要通过annotations指定，我们想要给pod添加“哪些”副接口。pod成功Running后，我们查看pod里面的网卡配置，可以看到名为net1的我们创建的macvlan接口：\n\n![enter description here](https://rexrock.github.io/post-images/1614305778007.png)\n\nOK，如果我们想要添加更多的“副接口”呢，配置如下：\n\n```\napiVersion: v1\nkind: Pod\nmetadata:\n  name: samplepod\n  annotations:\n    k8s.v1.cni.cncf.io/networks: macvlan-conf, sriov-net1 # 新增sriov-net1类型的接口\nspec:\n  containers:\n  - name: samplepod\n    command: [\"/bin/ash\", \"-c\", \"trap : TERM INT; sleep infinity & wait\"]\n    image: alpine\n    resources: # 这是sriov接口特有的配置，这里先忽略\n      requests:\n        intel.com/mlnx_sriov: '1'\n      limits:\n        intel.com/mlnx_sriov: '1'\n```\n\n应用后，我们可以分别看到名为net1的vxlan接口和名为net2的sriov接口：\n\n![enter description here](https://rexrock.github.io/post-images/1614305806391.png)\n\n说明：上述配置是我在以完成sriov-network-device-plugin安装的情况下才可以配置sriov接口。\n\n# 2\\. 安装sriov-cni\n\n项目地址：[https://github.com/k8snetworkplumbingwg/sriov-cni.git](https://github.com/k8snetworkplumbingwg/sriov-cni.git)\n\n没有太多好讲的，把二进制编译出来放到/opt/cni/bin/目录下即可：\n\n```\n# git clone https://github.com/k8snetworkplumbingwg/sriov-cni.git\n# cd sriov-cni\n# make\n# cp build/sriov /opt/cni/bin\n```\n\n一般cni的安装需要cni二进制（/opt/cni/bin/）+cni配置文件（/etc/cni/net.d/），因为这里所有的cni配置已经被multus劫持，所以只需要安装二进制文件即可，而具体的每个副cni配置则通过multus的CRD NetworkAttachmentDefinition来定义。\n\n# 3\\. 安装sriov-network-device-plugin\n\n项目地址：[https://github.com/k8snetworkplumbingwg/sriov-network-device-plugin.git](https://github.com/k8snetworkplumbingwg/sriov-network-device-plugin.git)\n\n```\n# git clone https://github.com/k8snetworkplumbingwg/sriov-network-device-plugin.git\n# cd sriov-network-device-plugin/\n```\n\n这里需要编辑下configmap文件，这个configmap使用来定义sriov资源的，我用的mellanox网卡，配置如下：\n\n![enter description here](https://rexrock.github.io/post-images/1614305844183.png)\n\n**参数解析：**\n\n - vendors通过lspci -v -s pci\\_addr可以查看；\n - devices，我的网卡pf时1017，vf是1018，包括驱动，dpdk-devbind.py都可以查看。\n\n**开始部署：**\n```\n# kubectl create -f configMap.yaml\n# kubectl create -f k8s-v1.16/sriovdp-daemonset.yaml\n# kubectl get pod --all-namespaces | grep sriov\nkube-system   kube-sriov-device-plugin-amd64-5vsnr           1/1     Running   0          18h\nkube-system   kube-sriov-device-plugin-amd64-lr8wh           1/1     Running   0          19h\n```\n\nsriov-device-plugin成功部署，我们可以看到vf被添加到相应的资源池：\n\n```\n# kubectl get node dell740.it.163.org -o json | jq '.status.allocatable'\n{\n  \"cpu\": \"48\",\n  \"ephemeral-storage\": \"260988928388\",\n  \"hugepages-1Gi\": \"0\",\n  \"hugepages-2Mi\": \"0\",\n  \"intel.com/intel_sriov_dpdk\": \"0\",\n  \"intel.com/intel_sriov_netdevice\": \"0\",\n  \"intel.com/mlnx_sriov\": \"24\",\n  \"memory\": \"394747480Ki\",\n  \"pods\": \"110\"\n}\n```\n\nOK，sriov-device-plugin到现在算是部署成功了，接下来我们可以创建基于sriov的“副CNI”了：\n\n```\napiVersion: \"k8s.cni.cncf.io/v1\"\nkind: NetworkAttachmentDefinition\nmetadata:\n  name: sriov-net1\n  annotations:\n    k8s.v1.cni.cncf.io/resourceName: intel.com/mlnx_sriov\nspec:\n  config: '{\n  \"type\": \"sriov\",\n  \"cniVersion\": \"0.3.1\",\n  \"name\": \"sriov-network\",\n  \"ipam\": {\n    \"type\": \"host-local\",\n    \"subnet\": \"172.10.1.0/24\",\n    \"routes\": [{\n      \"dst\": \"0.0.0.0/0\"\n    }]\n  }\n}'\n```\n\n注意：annotations里指定的resourceName，必须跟前面configmap定义的资源名称“完全一致”。\n\nOK，创建基于sriov的pod的配置前面已经贴过了。置于configmap以及pod yaml中的配置参数，可以参考项目中的文档。\n\n\n",
      "data": {
        "title": "玩转sriov-network-device-plugin",
        "date": "2021-03-03 10:25:00",
        "tags": [
          "K8S-NET",
          "SRIOV"
        ],
        "published": true,
        "hideInList": false,
        "feature": null,
        "isTop": false
      },
      "isEmpty": false,
      "excerpt": "",
      "abstract": "",
      "fileName": "k8s-net-sriov"
    },
    {
      "content": "## 1. Cilium datapath的组成\n\n### 1.1 Cilium中的流量劫持点\n\n![cilium datapath](https://rexrock.github.io/post-images/1614297388691.png)\n\n**所以流量劫持从prog类型上可以分为：**\n\n* 内核层面基于sock的流量劫持，主要用于lb（k8s-proxy）;\n* 基于端口流量劫持，实现整个datapath的替换；\n\n**置于cilium_host和cilium_net:**\n\n![cilium host](https://rexrock.github.io/post-images/1614297440408.png)\n\n### 1.2 Cilium中ebpf map的构成\n\n![the maps of cilium](https://rexrock.github.io/post-images/1614297479000.png)\n\n## 2. Cilium datapath加载流程\n\n### 2.1 公共ebpf map的初始化\n\ncilium有很多公用的ebpf map，这些map在ebpf prog加载前被创建：\n\n```\nrunDaemon() =>NewDaemon() =>Daemon.initMaps()\n```\n\n* **cilium\\_call\\_policy**，PROG\\_ARRAY，用来装“to-contaner”\n* **cilium\\_ct4\\_global**，CT表，for tcp\n* **cilium\\_ct\\_any4\\_global**，CT表，for non-tcp\n* cilium\\_events，\n* **cilium\\_ipcache**，ip+mask -> sec\\_label + VETP,如果是本地，则VETP为0\n* cilium\\_ipv4\\_frag\\_datagrams\n* cilium\\_lb4\\_affinity\n* cilium\\_lb4\\_backends\n* cilium\\_lb4\\_reverse\\_nat\n* cilium\\_lb4\\_reverse\\_sk\n* cilium\\_lb4\\_services\\_v2\n* cilium\\_lb\\_affinity\\_match\n* **cilium\\_lxc**，本地endpoint对应的netdev，ip -> NETDEV-INFO\n* cilium\\_metrics\n* cilium\\_nodeport\\_neigh4\n* cilium\\_signals\n* cilium\\_snat\\_v4\\_external\n* **cilium\\_tunnel\\_map**，ip -> VETP，只记录非本地的ip\n\n### 2.2 基础网络构建(init.sh)\n\n#### 2.2.1 初始化参数\n\n* LIB=/var/lib/cilium/bpf，bpf源码所在目录\n* RUNDIR=/var/run/cilium/state，工作目录\n* IP4\\_HOST=10.17.0.7，cilium\\_host的ipv4地址\n* IP6\\_HOST=nil\n* MODE=vxlan，网络模式\n* **NATIVE\\_DEVS**\\=eth0，出口网卡，可以手动指定，没指定的话就看默认路由走那个口\n* XDP\\_DEV=nil\n* XDP\\_MODE=nil\n* MTU=1500\n* IPSEC=false\n* ENCRYPT\\_DEV=nil\n* HOSTLB=true\n* HOSTLB\\_UDP=true\n* HOSTLB\\_PEER=false\n* CGROUP\\_ROOT=/var/run/cilium/cgroupv2\n* BPFFS\\_ROOT=/sys/fs/bpf\n* NODE\\_PORT=true\n* NODE\\_PORT\\_BIND=true\n* MCPU=v2\n* NODE\\_PORT\\_IPV4\\_ADDRS=eth0=0xc64a8c0\n* NODE\\_PORT\\_IPV6\\_ADDRS=nil\n* NR\\_CPUS=64\n\n#### 2.2.2 具体工作\n\n1）创建了cilium\\_host和cilium\\_net；\n\n2）如果是vxlan模式，添加并设置vxlan口cilium\\_vxlan；\n\n3）编译并加载cilium\\_vxlan相关的prog和map；\n\n> **2个map：**\n> \n> * cilium\\_calls\\_overlay\\_2，每个endpoint都有自己独立的tail call map，2是init.sh脚本固定写死的ID\\_WORLD；\n> * cilium\\_encrypt\\_state\n> \n> **6个prog：**\n> \n> * from-container：bpf\\_overlay.c\n> * to-container：bpf\\_overlay.c\n> * cilium\\_calls\\_overlay\\_2【1】 = \\_\\_send\\_drop\\_notify：lib/drop.h\n> * cilium\\_calls\\_overlay\\_2【7】 = tail\\_handle\\_ipv4：bpf\\_overlay.c\n> * cilium\\_calls\\_overlay\\_2【15】= tail\\_nodeport\\_nat\\_ipv4：lib/nodeport.h\n> * cilium\\_calls\\_overlay\\_2【17】= tail\\_rev\\_nodeport\\_lb4：lib/nodeport.\n\n4）删除出口网卡已经挂载的ebpf程序（from-netdev和to-netdev）\n\n5）加载LB相关ebpf和map；\n\n``` load prog and map\ntc exec bpf pin /sys/fs/bpf/tc/globals/cilium\\_cgroups\\_connect6 obj bpf\\_sock.o type sockaddr attach\\_type connect6 sec connect6\ntc exec bpf pin /sys/fs/bpf/tc/globals/cilium\\_cgroups\\_post\\_bind6 obj bpf\\_sock.o type sock attach\\_type post\\_bind6 sec post\\_bind6\ntc exec bpf pin /sys/fs/bpf/tc/globals/cilium\\_cgroups\\_sendmsg6 obj bpf\\_sock.o type sockaddr attach\\_type sendmsg6 sec sendmsg6\ntc exec bpf pin /sys/fs/bpf/tc/globals/cilium\\_cgroups\\_recvmsg6 obj bpf\\_sock.o type sockaddr attach\\_type recvmsg6 sec recvmsg6\ntc exec bpf pin /sys/fs/bpf/tc/globals/cilium\\_cgroups\\_connect4 obj bpf\\_sock.o type sockaddr attach\\_type connect4 sec connect4\ntc exec bpf pin /sys/fs/bpf/tc/globals/cilium\\_cgroups\\_post\\_bind4 obj bpf\\_sock.o type sock attach\\_type post\\_bind4 sec post\\_bind4\ntc exec bpf pin /sys/fs/bpf/tc/globals/cilium\\_cgroups\\_sendmsg4 obj bpf\\_sock.o type sockaddr attach\\_type sendmsg4 sec sendmsg4\ntc exec bpf pin /sys/fs/bpf/tc/globals/cilium\\_cgroups\\_recvmsg4 obj bpf\\_sock.o type sockaddr attach\\_type recvmsg4 sec recvmsg4\n```\n\n6）XDP、FLANNEL、IPSEC相关初始化暂未研究\n\n### 2.3 剩余的初始化工作\n\n1）cilium\\_host的datapath\n\n```\ntc[filter replace dev cilium_host ingress prio 1 handle 1 bpf da obj 554_next/bpf_host.o sec to-host]\ntc[filter replace dev cilium_host egress prio 1 handle 1 bpf da obj 554_next/bpf_host.o sec from-host]\n```\n\n> **说明**：加载了2 + 5 个prog，1个PROG\\_ARRAY map，1个cilium\\_policy\\_00554 map\n> \n>  - PROG：\n>  from-host、to-host\n>  - PROG_ARRAY_MAP：\n>  cilium\\_calls\\_hostns\\_00554（554是epid）\n>  - PROG IN PROG_ARRAY_MAP：\n>  cilium\\_calls\\_hostns\\_00554【1】= \\_\\_send\\_drop\\_notify\n>  cilium\\_calls\\_hostns\\_00554【7】=  tail\\_handle\\_ipv4\\_from\\_netdev => tail\\_handle\\_ipv4(ctx,false)\n>  cilium\\_calls\\_hostns\\_00554【15】= tail\\_nodeport\\_nat\\_ipv4\n>  cilium\\_calls\\_hostns\\_00554【17】= tail\\_rev\\_nodeport\\_lb4\n>  cilium\\_calls\\_hostns\\_00554【22】= tail\\_handle\\_ipv4\\_from\\_host => tail\\_handle\\_ipv4(ctx, true)\n\n2）cilium\\_net的datapath\n\n```\ntc[filter replace dev cilium_net ingress prio 1 handle 1 bpf da obj 554_next/bpf_host_cilium_net.o sec to-host]\n```\n\n>  - **说明**：加载了1 + 5个prog，1个PROG\\_ARRAY map\n>  - PROG：\n>  to-host\n>  - PROG_ARRAY_MAP：\n>  cilium\\_calls\\_netdev\\_00004（4是ifindex，ip link命令可以查看）\n>  - PROG IN PROG_ARRAY_MAP：\n>  cilium\\_calls\\_netdev\\_00004【1】= \\_\\_send\\_drop\\_notify\n>  cilium\\_calls\\_netdev\\_00004【7】=  tail\\_handle\\_ipv4\\_from\\_netdev => tail\\_handle\\_ipv4(ctx,false)\n>  cilium\\_calls\\_netdev\\_00004【15】= tail\\_nodeport\\_nat\\_ipv4\n>  cilium\\_calls\\_netdev\\_00004【17】= tail\\_rev\\_nodeport\\_lb4\n>  cilium\\_calls\\_netdev\\_00004【22】= tail\\_handle\\_ipv4\\_from\\_host => tail\\_handle\\_ipv4(ctx, true)\n\n3）eth0的datapath\n\n```\ntc[filter replace dev eth0 ingress prio 1 handle 1 bpf da obj 554_next/bpf_netdev_eth0.o sec from-netdev]\ntc[filter replace dev eth0 egress prio 1 handle 1 bpf da obj 554_next/bpf_netdev_eth0.o sec to-netdev]\n```\n\n> \\*\\*说明：\\*\\*加载了2+5个prog，1个PROG\\_ARRAY map\n> \n>  - PROG：\n>  from-netdev、to-netdev\n>  - PROG_ARRAY_MAP：\n>  cilium\\_calls\\_netdev\\_00002（4是ifindex，ip link命令可以查看）\n>  - PROG IN PROG_ARRAY_MAP：\n>  cilium\\_calls\\_netdev\\_00002【1】= \\_\\_send\\_drop\\_notify\n>  cilium\\_calls\\_netdev\\_00002【7】=  tail\\_handle\\_ipv4\\_from\\_netdev => tail\\_handle\\_ipv4(ctx,false)\n>  cilium\\_calls\\_netdev\\_00002【15】= tail\\_nodeport\\_nat\\_ipv4\n>  cilium\\_calls\\_netdev\\_00002【17】= tail\\_rev\\_nodeport\\_lb4\n>  cilium\\_calls\\_netdev\\_00002【22】= tail\\_handle\\_ipv4\\_from\\_host => tail\\_handle\\_ipv4(ctx, true)\n\n4）lxc\\_health的datapath，**跟增加一个pod的datapath是完全一样的**\n\n```\ntc[filter replace dev lxc_health ingress prio 1 handle 1 bpf da obj 908_next/bpf_lxc.o sec from-container]\n```\n\n> **说明**：加载了1+4+1个prog，1个PROG\\_ARRAY map，1个cilium\\_policy\\_00908 map\n> \n>  - PROG：\n>  from-container\n>  - PROG IN PROG_ARRAY_MAP：\n>  cilium\\_calls\\_00908【1】=  \\_\\_send\\_drop\\_notify\n>  cilium\\_calls\\_00908【6】= tail\\_handle\\_arp\n>  cilium\\_calls\\_00908【15】= tail\\_nodeport\\_nat\\_ipv4\n>  cilium\\_calls\\_00908【17】= tail\\_rev\\_nodeport\\_lb4\n>  cilium\\_call\\_policy\\[908\\] = handle\\_policy(to-container好像已经废弃了)",
      "data": {
        "title": "Cilium datapath梳理",
        "date": "2021-03-03 10:23:00",
        "tags": [
          "eBPF",
          "Cilium"
        ],
        "published": true,
        "hideInList": false,
        "feature": null,
        "isTop": false
      },
      "isEmpty": false,
      "excerpt": "",
      "abstract": "",
      "fileName": "cilium2"
    },
    {
      "content": "## 1. Cilium工作模式\n\n官网：\n[https://docs.cilium.io/en/v1.8/concepts/networking/routing/](https://docs.cilium.io/en/v1.8/concepts/networking/routing/)\n\nCilium也分为overlay和underlay两种工作模式：\n\n - overlay，目前支持vxlan和geneve两种虚拟化网络协议；\n - underlay，该模式下cilium需要能够对分配到其他node上的ip段进行路由，但遗憾得是，cilium既不能像flannel那样通过自身的守护进程下发路由配置，也不能像calico那样直接集成bird以提供BGP功能；所以要实现cilium路由模式的部署，我们需要自己提供BGP功能，有两种方式：\n\n        方式一：节点本身知道如何路由所有POD IP，但是网络中存在一个路由器，该路由器知道如何到达每个POD IP，每个节点需配置一条默认路由指向该路由器，该方式主要常见于云提供商的网络集成场景。\n        方式二：每个节点都知道所有的POD IP，并将路由插入到本地内核的路由表中。和flannel和calico一样，这需要所有节点二层互通。这需要我们自己部署BGP功能（可以通过kube-router来部署BGP功能）。\n\n**说明：**\n\nGeneve（Generic Network Virtualization Encapsulation-统用网络虚拟化封装），参考：[https://zhuanlan.zhihu.com/p/35790366](https://zhuanlan.zhihu.com/p/35790366)\n\n## 2. 路由模式\n\n### 2.1 Cilium部署\n\n```\ngit clone https://github.com/cilium/cilium.git\n#kubectl create -f cilium/install/kubernetes/quick-install.yaml\n```\n\n**注意：**\n\ncilium默认是采用vxlan方式部署的，所以我们需要先修改quick-install.yaml：\n\n```\n--- cilium-vxlan.yaml   2020-10-26 14:36:13.449026862 +0800\n+++ cilium-route.yaml   2020-10-26 14:28:08.458113851 +0800\n@@ -95,7 +95,9 @@\n   #   - disabled\n   #   - vxlan (default)\n   #   - geneve\n-  tunnel: vxlan\n+  tunnel: disabled\n+  native-routing-cidr: 10.17.0.0/16\n+  #auto-direct-node-routes: true\n\n   # Name of the cluster. Only relevant when building a mesh of clusters.\n   cluster-name: default\n```\n\n官方文档中说，如果各节点二层互通，那么直接通过参数auto-direct-node-routes: true\n即可实现各节点路由配置的同步和下发，环境所限并未验证。\n\n![enter description here](https://rexrock.github.io/post-images/1614300041226.png)\n\n我们看到，只部署cilium，那么该节点上是没有其他节点POD IP的路由的。这时候如果默认路由指向的网关可以提供其他节点POD IP的路由（对应underlay方式一的部署），那么整个集群POD已经可以直接互通。\n\n### 2.2 kube-router部署\n\n```\nwget https://raw.githubusercontent.com/cloudnativelabs/kube-router/v0.4.0/daemonset/generic-kuberouter-only-advertise-routes.yaml\n```\n\n按照文档说明，需要先修改一些内容：\n\n```\nroot@k8s-99-12:~# diff -rNua kube-router.orig.yaml kube-router.yaml\n--- kube-router.orig.yaml       2020-10-26 14:45:03.716777078 +0800\n+++ kube-router.yaml    2020-10-26 11:08:40.467744916 +0800\n@@ -29,11 +29,9 @@\n         - \"--run-firewall=false\"\n         - \"--run-service-proxy=false\"\n         - \"--enable-cni=false\"\n-        - \"--enable-ibgp=false\"\n-        - \"--enable-overlay=false\"\n-        - \"--peer-router-ips=<CHANGE ME>\"\n-        - \"--peer-router-asns=<CHANGE ME>\"\n-        - \"--cluster-asn=<CHANGE ME>\"\n+        - \"--enable-pod-egress=false\"\n+        - \"--enable-ibgp=true\"\n+        - \"--enable-overlay=true\"\n         - \"--advertise-cluster-ip=true\"\n         - \"--advertise-external-ip=true\"\n         - \"--advertise-loadbalancer-ip=true\"\n```\n\n测试环境使用ibgp即可，无需配置ebpf，所以peer-router-ips、peer-router-asns、cluster-asn无需配置。\n\n![enter description here](https://rexrock.github.io/post-images/1614300092938.png)\n\nkube-router启动后，可以看到其他节点的POD IP已经被加入本地路由。\n\n## 3. VXLAN模式\n\nCilium默认就是以vxlan方式部署，但是cilium并不会自动读取以配置的pod-cidr，需要我们通过参数native-routing-cidr: 10.17.0.0/16自己指定。\n\n![enter description here](https://rexrock.github.io/post-images/1614300139319.png)\n\n可以看到vxlan部署后，跨界点访问都导向了cilium\\_host。由于cilium大量使用ebpf功能，穿透内核协议栈部分功能，所以目前没办法画出完整的流量走向图，后续持续更新。",
      "data": {
        "title": "Cilium简介",
        "date": "2021-03-03 10:22:00",
        "tags": [
          "K8S-NET",
          "Cilium"
        ],
        "published": true,
        "hideInList": false,
        "feature": null,
        "isTop": false
      },
      "isEmpty": false,
      "excerpt": "",
      "abstract": "",
      "fileName": "cilium1"
    },
    {
      "content": "## 1. Flannel\n\nFlannel是CoreOS维护的一个网络组件，Flannel为每个Pod提供全局唯一的IP，Flannel使用ETCD来存储Pod子网与Node IP之间的关系。flanneld守护进程在每台主机上运行，并负责维护ETCD信息和路由数据包。\n\n```\nhttps://github.com/coreos/flannel\n```\n\n### 1.1 Flannel部署\n\n```\nwget https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml\n# 修改net-conf.json配置，选择网络模式“ vxlan / host-gw \"，并配置pod-cidr(注意需要跟kubeadm创建集群时指定\n# 的pod-cidr一致，这里不够只能，calico可以自动读取的)\nkubectl apply -f kube-flannel.yml\n```\n\n部署完成，可以在每个node上看到属于这个node子网\n\n```\nroot@k8s-99-12:~# cat /var/run/flannel/subnet.env\nFLANNEL_NETWORK=10.17.0.0/16\nFLANNEL_SUBNET=10.17.0.1/24\nFLANNEL_MTU=1450\nFLANNEL_IPMASQ=true\n```\n\n### 1.2 hos-gw工作模式\n\n如何配置工作模式，上面已有介绍。这里主要看下flannel的host-gw模式时如何工作的\n\n![enter description here](file://C:/Users/liyang07/Documents/Gridea/post-images/1614303683871.png)\n\n**说明：**\n\nflannel的host-gw模式，就是将每个节点都当成一个网关，部署中指定pod-cidr后，flannel会为每个node分配一个子网，并将这些node子网信息都存储到etcd中，然后每个节点上flannel守护进程会根据这些信息将其他所有节点都加到本地路由中以实现跨界点访问。下面我们分别看一下node节点上以及pod中的路由是什么样的。\n\n**pod路由**\n\n![enter description here](file://C:/Users/liyang07/Documents/Gridea/post-images/1614303723395.png)\n\n![enter description here](file://C:/Users/liyang07/Documents/Gridea/post-images/1614303751754.png)\n\n**宿主机路由**\n\n![enter description here](file://C:/Users/liyang07/Documents/Gridea/post-images/1614303802333.png)\n\n![enter description here](file://C:/Users/liyang07/Documents/Gridea/post-images/1614303831855.png)\n\n> **限制：**\n> \n> 1\\. Node需要二层互通，否则下一条转发不出去（以我们的云主机为例，云主机之间转发靠流表，因此在云主机上搭建的k8s集群，如果采用flannel的host-gw模式，跨节点访问不通的）。\n\n### 1.3 vxlan模式\n\nvxlan模式的转发路径如下图所示，flannel.1即linux的vxlan port：\n\n![enter description here](file://C:/Users/liyang07/Documents/Gridea/post-images/1614303854716.png)\n\nvxlan的原理这里不做展开，关于linux vxlan的配置及工作原理可参考：\n[http://just4coding.com/2020/04/20/vxlan-fdb/](http://just4coding.com/2020/04/20/vxlan-fdb/)\n\n分别看一下12节点和16节点的转发表：\n\n![enter description here](file://C:/Users/liyang07/Documents/Gridea/post-images/1614303886618.png)\n\n![enter description here](file://C:/Users/liyang07/Documents/Gridea/post-images/1614303902545.png)\n\n可以看到每个节点被当成一个网关，只不过底层传输走了vxlan。\n\n## 2. Calico\n\n### 2.1 Calico部署\n\n**部署前确保宿主机的iptables为legacy模式：**\n\n```\niptables --version\nupdate-alternatives --set iptables /usr/sbin/iptables-legacy\nupdate-alternatives --set ip6tables /usr/sbin/ip6tables-legacy\nupdate-alternatives --set arptables /usr/sbin/arptables-legacy\nupdate-alternatives --set ebtables /usr/sbin/ebtables-legacy\n```\n\n**参考：**[https://docs.projectcalico.org/getting-started/kubernetes/self-managed-onprem/onpremises#install-calico-with-kubernetes-api-datastore-50-nodes-or-less](https://docs.projectcalico.org/getting-started/kubernetes/self-managed-onprem/onpremises#install-calico-with-kubernetes-api-datastore-50-nodes-or-less)\n\n**参考：**[https://blog.51cto.com/14143894/2463392](https://blog.51cto.com/14143894/2463392)\n\n**说明：**\n\n1\\. Calico可以自己读取集群的pod-cidr配置，无需像flannel一样去手动修改配置；\n\n2\\. Calico网络模式的选择：\n\n![enter description here](file://C:/Users/liyang07/Documents/Gridea/post-images/1614303947059.png)\n\n - **BGP**：CALICO\\_IPV4POOL\\_IPIP=\"Never\" 且 CALICO\\_IPV4POOL\\_VXLAN=”Never“\n - **IP Tunnel:** CALICO\\_IPV4POOL\\_IPIP=\"Always\" 且 CALICO\\_IPV4POOL\\_VXLAN=”Never“\n - **VXLAN**: CALICO\\_IPV4POOL\\_IPIP=\"Never\" 且 CALICO\\_IPV4POOL\\_VXLAN=”Always“\n\n### 2.2 BGP模式\n\n![enter description here](file://C:/Users/liyang07/Documents/Gridea/post-images/1614303997323.png)\n\n**类似flannel的host-gw模式，但是有两点不同：**\n\n1\\. 不在使用bridge，所有pod通信全部走路由，例如pod1和pod2的通信的路由如下：\n\n![enter description here](file://C:/Users/liyang07/Documents/Gridea/post-images/1614304023667.png)\n\n2\\. 其他节点路由信息的添加由BGP负责，flannel则是通过自己的守护进程实现，可以看到calico-node的pod里跑了bird：\n\n![enter description here](file://C:/Users/liyang07/Documents/Gridea/post-images/1614304040483.png)\n\n**限制：**\n\n和flannel一样，BGP模式要求节点在二层互通；\n\n**最后看一下两个节点上的路由信息：**\n\n![enter description here](file://C:/Users/liyang07/Documents/Gridea/post-images/1614304078596.png)\n\n![enter description here](file://C:/Users/liyang07/Documents/Gridea/post-images/1614304108422.png)\n\n### 2.3 IP Tunnel模式\n\n![enter description here](file://C:/Users/liyang07/Documents/Gridea/post-images/1614304129190.png)\n\nIP Tunnel方案相比BGP方案相比，唯一的区别时跨节点通信由原来的路由转发，改为IPIP隧道模式。好处时对节点网络没有二层互通的要求，只要节点三层可达，即可实现通信。\n\n![enter description here](file://C:/Users/liyang07/Documents/Gridea/post-images/1614304149124.png)\n\n### 2.4 Vxlan模式\n\n![enter description here](file://C:/Users/liyang07/Documents/Gridea/post-images/1614304168262.png)\n\n现在Calico也支持vxlan模式了，相比IP Tunnel模式，唯一的区别就是隧道类型变了，毕竟vxlan已经成为网络虚拟化的主流方案，和flannel一样都是使用的linux内核提供的vxlan功能。\n\n直接看下12节点的路由和转发表好了：\n\n![enter description here](file://C:/Users/liyang07/Documents/Gridea/post-images/1614304182545.png)\n\n![enter description here](file://C:/Users/liyang07/Documents/Gridea/post-images/1614304208695.png)",
      "data": {
        "title": "Flannel和Calico简介",
        "date": "2021-03-03 10:21:00",
        "tags": [
          "K8S-NET"
        ],
        "published": true,
        "hideInList": false,
        "feature": null,
        "isTop": false
      },
      "isEmpty": false,
      "excerpt": "",
      "abstract": "",
      "fileName": "k8s-net1"
    },
    {
      "content": "\n## 1. virtio的ring结构\nVirtio设备是支持多队列，每个队列由结构vring_virtqueue定义（可以是收包队列也可以是发包队列），而每个vring_virtqueue中都定义了一个vring结构，负责具体的数据传输。\n\n``` code\n// include/uapi/linux/virtio_ring.h\nstruct vring {\n        unsigned int num;\n\n        struct vring_desc *desc;\n\n        struct vring_avail *avail;\n\n        struct vring_used *used;\n};\n```\n可见，ving不是一个ring环，而是包含了三个ring环，利用着三个ring环实现报文的收发。我们通过一张图来描述三个ring环的作用及关系：\n![enter description here](./post-images/1615334196092.png)\n\n**1. vring_desc**\nStruct vring_desc并没有定义一个ring环，而是定义了ring环中每个元素的结构。上图中已经对vring_desc各成员做了注解。Desc ring没有消费者和生产者，我们可以把它看作一块用来交互数据的共享内存。\n\n> 说明：vring_desc结构中的addr成员，在Guest向外发包的场景中，指向的是一块承载了发包数据的内存，而在Guest从外面收包的场景中，指向的是一块预分配好的空内存，Host会将收到的包存放到这块空内存中。\n\n**2. vring_avail**\nStruct vring_avail是定义了一个ring环的（即成员ring[]），这个ring环的生产者是Guest中的virtio-net，消费者是Host中vhostuser/vhostnet。Avail ring环中每个元素即指向desc ring的下标。\n\n> 说明：Avail ring和desc ring的长度都是一样的，但是avail ring并不会指向desc ring的每一个desc。例如有些skb是由多个分片组成的（scattergather），那么这个skb实际会被转换成多个desc，并且通过vring_desc中的next将多个desc链接在一起，最后一个desc通过flag标记结束。那么这种情况下，Avail ring只会存储第一个desc的下标，同时vring_avail的idx也只累加1。\n\n**3. vring_used**\n Struct vring_used跟vring_avail类似，不过used ring的生产者是vhostnet/vhostuser，消费者是virtio-net。\n \n> 说明：used ring中的每个元素包含两个成员id和len，id指向desc ring中的下标，而len则指向desc中所存储数据的长度(通常len成员只在Guest从外面收包的场景中才有效，这个时候desc中len指的是内存中可以最大存储的数据的长度，而user ring中的len指的则是内存中实际存储的数据的长度）。\n\n那么这三个ring在内存中是怎么分布的呢？我们通过一张图描述下：\n![enter description here](./post-images/1615334206753.png)\n\n如图，三个ring是分布在一块连续的内存中的（物理/虚拟地址都是连续的）。最前面是desc ring，接下来是avail ring，最后是used ring。\n\n## 2. 将vring映射到vhostuser\nVirtio队列中的vring是由Guest中的virtio-net驱动申请的，那么vhostuser如何操作这些vring呢？答案是virtio-net在申请好vring后需要将vring的地址告诉vhostuser。我们通过一张图，看一下虚拟机启动时所涉及到的内存注册过程：\n![enter description here](./post-images/1615335007124.png)\n\n如上图所示，整个内存注册过程分为三个步骤：\n**第一步：**\nQEMU未虚拟机申请内存，并将虚拟机的整个内存注册到vhostuser。你没看错，确实是需要将虚拟机的整个内存都注册到vhostuser驱动中。\n\n> 说明：Vhostuser和QEMU通过unix socket建立了通信连接，两者通过该连接进行协商。\n\n**第二步：**\nGuest中的virtio-net驱动申请队列（即virtqueue），并将队列中的vring地址同步给QEMU。\n``` code\n// 追踪从virtio-net开始初始化到创建virtqueue，函数位置：linux-kernel-src/drivers/virtio/\n|virtio_pci_probe\n| |virtio_pci_legacy_probe / virtio_pci_modern_probe\n| |\t|setup_vq\n| |\t| |vring_create_virtqueue\n| | | | |vring_create_virtqueue_split\n| | | | | |void *queue = vring_alloc_queue // 申请vring的地址\n| | | | | |vring_init(struct vring *, queue)\n| | | | | |__vring_create_virtqueue\n| | | |iowrite32(VIRTIO_PCI_QUEUE_PFN) // 将vring_addr注册到QEMU\n```\n> 说明：Virtio-net和QEMU之间的通信不是通过什么scoket，而是由virtio-net向一段特定的io空间写数据实现的。不单单QEMU是这样做的，包括VMWARE也是这么做的（XEN不熟悉）。同理，QEMU向GUEST发起的数据请求也都是都通过IO实现的。\n\n**第三步：**\nQEMU在enable每个virtqueu的时候，会将virtqueue中三个vring的长度及地址注册到vhostuser。并且初始化三个vring中消费者/生产者的位置。\n\n``` code\n// vhostuser中相关协商处理函数\nstatic vhost_message_handler_t vhost_message_handlers[VHOST_USER_MAX] = {\n\t......\n\t[VHOST_USER_SET_VRING_NUM] = vhost_user_set_vring_num,\n\t[VHOST_USER_SET_VRING_ADDR] = vhost_user_set_vring_addr,\n\t[VHOST_USER_SET_VRING_BASE] = vhost_user_set_vring_base,\n\t......\n};\n```\n\n## 3. Guest向外发包\n\n``` code\n// 函数位置：linux-kernel-src/drivers/net/virtio-net.c\n|start_xmit\n| |free_old_xmit_skbs // 每次发包前，先清理上一次已成功发送的包\n| |xmit_skb\n| | |virtqueue_add_outbuf\n| | | |virtqueue_add\n| | | | |virtqueue_add_split\n```\n这里面virtqueue_add()是一个通用的函数，不管收包还是发包，都是通过调用virtqueue_add()函数实现：\n\n``` code\nstatic inline int virtqueue_add(struct virtqueue *_vq,\n                                struct scatterlist *sgs[],\n                                unsigned int total_sg,\n                                unsigned int out_sgs,\n                                unsigned int in_sgs,\n                                void *data,\n                                void *ctx,\n                                gfp_t gfp)；\n```\n**参数解析：**\n\n - _vq，没什么好解释的，virtqueue被包含在vring_virtqueue中，几乎跟vring传输相关的所有内容都定义在vring_virtqueue中；\n - sgs，元素为scatterlist的列表；这里需要额外注意，每个scatterlist本身也是一个列表；举个例子，一个skb可以由多个分片构成，多个分片内存上是不连续的，在没有scatter-gather之前或者禁用scatter-gather的情况下，驱动需要将所有分片拷贝到一块连续的内存上，而开启scatter-gather后，我们不必再重新拷贝报文分片，直接通过scattherlist将报文的多个分片串联起来，供网卡驱动使用。可以说scatterlist是skb在网卡驱动中的表示；\n - total_sg，所有scatterlist中分片加起来的总数，每个分片都占用一个独立的desc，所以total_sg表明接下来要消耗的desc总数；\n - out_sgs，sgs中有多少是out_sg；\n==说明： #F44336==scatterlist是分为out_sg（只读）和in_sg（可读可写）两种类型的。当Guest发送报文的时候，使用out_sg，当Guest打算收包，需要先将可承载报文数据的内存通过desc ring传递到vhost的时候，就使用in_sg。此外需要注意，我们发包的时候，只会传递out_sg给virtqueue_add()，收包的时候只传递in_sg给virtqueue_add()，还有一种通过virtqueue进行前后端协商和管理的virtqueue，会同时传递out_sg和in_sg给virtqueue_add（）。\n - int_sgs，sgs中有多少是in_sg；\n - data，要传输的内存起始地址；\n==说明： #F44336==在发包场景中，就是要发送的skb的地址，注意是虚拟地址，而我们赋值给desc->addr是物理地址，那么这个data有啥用呢？用处就是这个报文被vhost成功处理发送后，virtio-net会通过used ring再次获取到已经被成功发送的报文，这个时候virtio-net需要释放报文，那么直接引用这个data指向的虚拟地址释放就可以了。\n==说明: #F44336==在收包场景中类似，virtio-net填充预申请的空白内存给vhostuser收包，收到的报文会通过used ring再送回到virtio-net中，这个时候直接引用data即可对内存中的报文数据进行操作了。\n==说明： #F44336==那么data存储再哪呢？下面代码解析里有介绍。\n - ctx，跟indirect相关，暂时不管；\n - gfp，跟indirect相关，暂时不管；\n\n**virtqueue_add_split函数源码分析：**\n==说明： #F44336==packed queus是virtio 1.1引入的新特性，我们暂时不管，先分析老的split模式。\n``` code\n\nstatic inline int virtqueue_add_split(struct virtqueue *_vq,\n                                      struct scatterlist *sgs[],\n                                      unsigned int total_sg,\n                                      unsigned int out_sgs,\n                                      unsigned int in_sgs,\n                                      void *data,\n                                      void *ctx,\n                                      gfp_t gfp)\n{\n\t\t......\n\t\t} else {\n\t\t// 非indirect模式\n                indirect = false;\n                desc = vq->split.vring.desc;\n                i = head;\n                descs_used = total_sg;\n        }\n\t\t......\n\t\t// 如果desc ring没有空间了，赶紧通知vhost处理报文好腾地方\n        if (vq->vq.num_free < descs_used) {\n                pr_debug(\"Can't add buf len %i - avail = %i\\n\",\n                         descs_used, vq->vq.num_free);\n                /* FIXME: for historical reasons, we force a notify here if\n                 * there are outgoing parts to the buffer.  Presumably the\n                 * host should service the ring ASAP. */\n                if (out_sgs)\n                        vq->notify(&vq->vq);\n                if (indirect)\n                        kfree(desc);\n                END_USE(vq);\n                return -ENOSPC;\n        }\n\t\t......\n\t\t// *************************************************************************\n\t\t// 第一步，填充desc ring\n\t\t// 本函数最核心的代码了，out_sg和in_sg的存放位置也是有讲究的，当同时又两种scatterlist时，\n\t\t// out_sg总是被放在前面，in_sg被存储在out_sg后面；\n\t\tfor (n = 0; n < out_sgs; n++) {\n                for (sg = sgs[n]; sg; sg = sg_next(sg)) {\n\t\t\t\t\t\t// 这里需要注意的是，通过desc->addr传递给vhost的是Guest的物理地址\n                        dma_addr_t addr = vring_map_one_sg(vq, sg, DMA_TO_DEVICE);\n                        if (vring_mapping_error(vq, addr))\n                                goto unmap_release;\n\n                        desc[i].flags = cpu_to_virtio16(_vq->vdev, VRING_DESC_F_NEXT);\n                        desc[i].addr = cpu_to_virtio64(_vq->vdev, addr);\n                        desc[i].len = cpu_to_virtio32(_vq->vdev, sg->length);\n                        prev = i;\n                        i = virtio16_to_cpu(_vq->vdev, desc[i].next);\n                }\n        }\n        for (; n < (out_sgs + in_sgs); n++) {\n                for (sg = sgs[n]; sg; sg = sg_next(sg)) {\n                        dma_addr_t addr = vring_map_one_sg(vq, sg, DMA_FROM_DEVICE);\n                        if (vring_mapping_error(vq, addr))\n                                goto unmap_release;\n\n                        desc[i].flags = cpu_to_virtio16(_vq->vdev, VRING_DESC_F_NEXT | VRING_DESC_F_WRITE);\n                        desc[i].addr = cpu_to_virtio64(_vq->vdev, addr);\n                        desc[i].len = cpu_to_virtio32(_vq->vdev, sg->length);\n                        prev = i;\n                        i = virtio16_to_cpu(_vq->vdev, desc[i].next);\n                }\n        }\n        /* Last one doesn't continue. */\n\t\t// OK，对于发包场景，上面所有desc都是一个SKB的，现在这个SKB填充完毕，需要通过flag标记\n\t\t// desc的结束，前面介绍desc ring的时候介绍过，所有desc通过next成员链在一起，并且通过flag\n\t\t// 标记一个报文存储的结束。\n        desc[prev].flags &= cpu_to_virtio16(_vq->vdev, ~VRING_DESC_F_NEXT);\n\t\t\n\t\t/* We're using some buffers from the free list. */\n\t\t// 用了多少，得从num_free中减掉\n        vq->vq.num_free -= descs_used;\n\n        /* Update free pointer */\n        if (indirect)\n\t\t\t\t......\n        else\n\t\t\t\t// 更新下一次开始填充的desc下标\n                vq->free_head = i;\n\t\t......\n\t\t// vring_virtqueue又自己维护了一个跟desc ring长度相同的数组，专门用来存储对应desc中内存\n\t\t// 对应的虚拟地址\n\t\tvq->split.desc_state[head].data = data;\n\t\t......\n\t\t/* Put entry in available array (but don't update avail->idx until they\n         * do sync). */\n        // *************************************************************************\n\t\t// 第二步，填充avail ring\n\t\t// 上面是desc ring的填充，下main开始填充avail ring了，可以看到只需要将第一个desc\n\t\t// 填充到avail ring即可\n        avail = vq->split.avail_idx_shadow & (vq->split.vring.num - 1);\n        vq->split.vring.avail->ring[avail] = cpu_to_virtio16(_vq->vdev, head);\n\t\t\n        /* Descriptors and available array need to be set before we expose the\n         * new available array entries. */\n        // 累加avail ring的生产者计数\n        virtio_wmb(vq->weak_barriers);\n        vq->split.avail_idx_shadow++;\n        vq->split.vring.avail->idx = cpu_to_virtio16(_vq->vdev,\n                                                vq->split.avail_idx_shadow);\n\t\t// *************************************************************************\n\t\t// num_added主要跟通知机制有关，下面章节详细介绍\n        vq->num_added++;\n\n        pr_debug(\"Added buffer head %i to %p\\n\", head, vq);\n        END_USE(vq);\n\n        /* This is very unlikely, but theoretically possible.  Kick\n         * just in case. */\n        if (unlikely(vq->num_added == (1 << 16) - 1))\n                virtqueue_kick(_vq);\n\t\t......\n```\n\n## 4. Guest从外面收包\n\n``` code\n|virtnet_poll()\n| |virtnet_receive()\n| | |virtqueue_get_buf()\n| | | |detach_buf()\n| | |receive_buf()\n| | |try_fill_recv()\n| | | |add_recebuf_xxx()\n| | | | |virtqueue_add_xxx()\n| | | | | |virtqueue_add()\t\n| | | |virqueue_kick()\n```\n我们从virtqueue_get_buf()函数开始看。该函数执行的是收包函数的第一步，还是以split模式为例，该函数会根据模式选择最终调用到virtqueue_get_buf_ctx_split()函数：\n\n``` code\nstatic void *virtqueue_get_buf_ctx_split(struct virtqueue *_vq,\n                                         unsigned int *len,\n                                         void **ctx)\n{\n\t\t// 注意：该函数每次只收一个包\n\t\t......\n\t\t// 这一步先判断下used ring里有没有未处理的成员。贴一下more_used_split（）的代码：\n\t\t// return vq->last_used_idx != \n\t\t//                    virtio16_to_cpu(vq->vq.vdev, vq->split.vring.used->idx);\n\t\t// ***************************************************************************\n\t\t// 这里需要说明的是，vring_virtqueue中定义了一个成员叫last_used_idx，last_used_idx是\n\t\t// virtio-net消费used ring的下标+1，也就是这一次将从last_used_idx这个位置开始消费used \n\t\t// ring。而vring_used中的idx则是由生产者（也就是vhost）填充的，表示下一次将要填充的used \n\t\t// ring的下标。\n\t\t// ***************************************************************************\n\t\t// 说明：Vring_avail和vring_used中的idx都是生产者填充的，而消费者都会在各自的virtqueue的\n\t\t// 结构中定义一个last_xxx_idx，表示上次消费的截至位置，以及下一次开始消费的位置。\n        if (!more_used_split(vq)) {\n                pr_debug(\"No more buffers in queue\\n\");\n                END_USE(vq);\n                return NULL;\n        }\n\t\t\n        /* Only get used array entries after they have been exposed by host. */\n        virtio_rmb(vq->weak_barriers);\n\n\t\t// 获取要消费的used ring的下标\n        last_used = (vq->last_used_idx & (vq->split.vring.num - 1));\n        // 从used成员中获取指向的desc ring中的下标\n\t\ti = virtio32_to_cpu(_vq->vdev,\n                        vq->split.vring.used->ring[last_used].id);\n\t\t// 获取这个报文的实际长度\n\t\t// 注意：这个报文可能是由多个desc构成的，下面的len是指所有desc中报文的总长度，并且报文的存\n\t\t// 储总是前面desc满了之后，再向下一个desc中存储数据。\n        *len = virtio32_to_cpu(_vq->vdev,\n                        vq->split.vring.used->ring[last_used].len);\n\n\t\t// 如果这个desc ring的下标超过数组长度，则发生错误。\n\t\t// ***************************************************************************\n\t\t// 特别注意：\n\t\t// 细心的同学可能已经发现，avail ring和used ring的生产者/消费者下标是不断累加的，然后使用\n\t\t// 的时候做一下“idx&(vring_num-1)”的操作来保证访问不越界。但是我们使用desc ring的下标并不\n\t\t// 是不断累加的，而是每次通过desc的next成员获取到的（观察上面virtqueue_add函数得分析）。所\n\t\t// 以我们从avail ring和used ring中获取得desc下标是直接得下标，不存在越界。\n        if (unlikely(i >= vq->split.vring.num)) {\n                BAD_RING(vq, \"id %u out of range\\n\", i);\n                return NULL;\n        }\n\t\t// ***************************************************************************\n\t\t// 这个data钱买你介绍过了\n        if (unlikely(!vq->split.desc_state[i].data)) {\n                BAD_RING(vq, \"id %u is not a head!\\n\", i);\n                return NULL;\n        }\n\n        /* detach_buf_split clears data, so grab it now. */\n        ret = vq->split.desc_state[i].data;\n\t\t// OK，报文已成功提取，时方掉这个desc，如果占用了多个desc，会在detach_buf_split中一起\n\t\t// 释放（通过flag标记结束）。\n        detach_buf_split(vq, i, ctx);\n\t\t// 累加消费者下标\n        vq->last_used_idx++;\n        /* If we expect an interrupt for the next entry, tell host\n         * by writing event index and flush out the write before\n         * the read in the next get_buf call. */\n        if (!(vq->split.avail_flags_shadow & VRING_AVAIL_F_NO_INTERRUPT))\n                virtio_store_mb(vq->weak_barriers,\n                                &vring_used_event(&vq->split.vring),\n                                cpu_to_virtio16(_vq->vdev, vq->last_used_idx));\n\n        LAST_ADD_TIME_INVALID(vq);\n\n        END_USE(vq);\n\t\t// 返回指向报文的虚拟机地址\n        return ret;\n```\n\n## 5. Vhost从Guest收包\n\n## 6. Vhost向Guest发包\n\n## 7. Virtio的前后端通知机制\n\n \n",
      "data": {
        "title": "深入浅出vhostuser传输模",
        "date": "2020-03-03 12:31:00",
        "tags": [
          "eBPF",
          "XDP"
        ],
        "published": true,
        "hideInList": false,
        "feature": null,
        "isTop": false
      },
      "isEmpty": false,
      "excerpt": "",
      "abstract": "",
      "fileName": "vhu1"
    }
  ],
  "tags": [
    {
      "index": -1,
      "name": "K8S",
      "slug": "SNvVFY1lM",
      "used": true
    },
    {
      "index": -1,
      "name": "Cilium",
      "slug": "Xcfbupjum",
      "used": true
    },
    {
      "index": -1,
      "name": "SRIOV",
      "slug": "w1Rvw2p6z",
      "used": true
    },
    {
      "index": -1,
      "name": "TC",
      "slug": "kwqg6YbWf",
      "used": true
    },
    {
      "index": -1,
      "name": "XDP",
      "slug": "37FuJFu1i",
      "used": true
    },
    {
      "name": "K8S-NET",
      "slug": "IAbZBeTqj",
      "used": true
    },
    {
      "name": "eBPF",
      "slug": "1emxgPk46",
      "used": true
    }
  ],
  "menus": [
    {
      "link": "/",
      "name": "首页",
      "openType": "Internal"
    },
    {
      "link": "/archives",
      "name": "归档",
      "openType": "Internal"
    },
    {
      "link": "/tags",
      "name": "标签",
      "openType": "Internal"
    },
    {
      "link": "/post/about",
      "name": "关于",
      "openType": "Internal"
    }
  ]
}